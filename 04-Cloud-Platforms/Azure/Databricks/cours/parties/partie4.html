<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Partie 4 : Apache Spark et traitement de donnÃ©es</title>
    <link rel="stylesheet" href="../assets/styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ”¥ Partie 4 : Apache Spark et traitement de donnÃ©es</h1>
            <p class="subtitle">Exploitez la puissance du traitement distribuÃ©</p>
            <span class="duration">â±ï¸ DurÃ©e : 50 minutes</span>
        </header>

        <nav class="nav-menu">
            <ul>
                <li><a href="../index.html">ğŸ  Accueil</a></li>
                <li><a href="partie3.html">â† Partie 3</a></li>
                <li><a href="partie4.html" class="active">Partie 4</a></li>
                <li><a href="partie5.html">Partie 5 â†’</a></li>
            </ul>
        </nav>

        <div class="content">
            <section class="objectives">
                <h2>ğŸ¯ Objectifs d'apprentissage</h2>
                <ul>
                    <li>Comprendre l'architecture et les concepts d'Apache Spark</li>
                    <li>MaÃ®triser les DataFrames et Datasets</li>
                    <li>Effectuer des transformations et actions sur les donnÃ©es</li>
                    <li>Utiliser Spark SQL pour requÃªter les donnÃ©es</li>
                    <li>Optimiser les performances des requÃªtes</li>
                </ul>
            </section>

            <section class="section">
                <h2>1. Architecture Apache Spark</h2>

                <h3>Concepts fondamentaux</h3>
                <div class="workflow-diagram">
                    <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ARCHITECTURE SPARK                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚   Driver     â”‚   (Master Node)                            â”‚
â”‚  â”‚   Program    â”‚   â€¢ SparkContext                           â”‚
â”‚  â”‚              â”‚   â€¢ Planification des tÃ¢ches               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â€¢ Distribution du code                   â”‚
â”‚         â”‚                                                     â”‚
â”‚         â”‚                                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    â”‚      Cluster Manager                 â”‚                  â”‚
â”‚    â”‚  (YARN / Mesos / Kubernetes / Local) â”‚                  â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚              â”‚                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚ Worker  â”‚    â”‚ Worker  â”‚    â”‚ Worker  â”‚                â”‚
â”‚    â”‚ Node 1  â”‚    â”‚ Node 2  â”‚    â”‚ Node 3  â”‚                â”‚
â”‚    â”‚         â”‚    â”‚         â”‚    â”‚         â”‚                â”‚
â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚                â”‚
â”‚    â”‚ â”‚Task â”‚ â”‚    â”‚ â”‚Task â”‚ â”‚    â”‚ â”‚Task â”‚ â”‚                â”‚
â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚                â”‚
â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚                â”‚
â”‚    â”‚ â”‚Task â”‚ â”‚    â”‚ â”‚Task â”‚ â”‚    â”‚ â”‚Task â”‚ â”‚                â”‚
â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                </div>

                <table>
                    <thead>
                        <tr>
                            <th>Composant</th>
                            <th>RÃ´le</th>
                            <th>ResponsabilitÃ©s</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Driver</strong></td>
                            <td>NÅ“ud maÃ®tre</td>
                            <td>
                                â€¢ ExÃ©cute le code utilisateur<br>
                                â€¢ CrÃ©e le SparkContext<br>
                                â€¢ Planifie les tÃ¢ches
                            </td>
                        </tr>
                        <tr>
                            <td><strong>Cluster Manager</strong></td>
                            <td>Gestionnaire de ressources</td>
                            <td>
                                â€¢ Alloue les ressources<br>
                                â€¢ GÃ¨re les workers<br>
                                â€¢ Monitore la santÃ© du cluster
                            </td>
                        </tr>
                        <tr>
                            <td><strong>Workers</strong></td>
                            <td>NÅ“uds de calcul</td>
                            <td>
                                â€¢ ExÃ©cutent les tÃ¢ches<br>
                                â€¢ Stockent les donnÃ©es en cache<br>
                                â€¢ Renvoient les rÃ©sultats
                            </td>
                        </tr>
                        <tr>
                            <td><strong>Executors</strong></td>
                            <td>Processus sur workers</td>
                            <td>
                                â€¢ ExÃ©cutent le code<br>
                                â€¢ GÃ¨rent le cache<br>
                                â€¢ Communiquent avec driver
                            </td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section class="section">
                <h2>2. DataFrames et Datasets</h2>

                <h3>Qu'est-ce qu'un DataFrame ?</h3>
                <p>Un DataFrame est une collection distribuÃ©e de donnÃ©es organisÃ©e en colonnes nommÃ©es, similaire Ã  une table SQL ou un DataFrame pandas, mais optimisÃ© pour le traitement distribuÃ©.</p>

                <h3>CrÃ©ation de DataFrames</h3>
                <pre><code class="language-python"># 1. Depuis une collection Python
data = [
    ("Alice", 34, "Engineering"),
    ("Bob", 45, "Sales"),
    ("Charlie", 29, "Marketing")
]
df = spark.createDataFrame(data, ["name", "age", "department"])
df.show()

# 2. Depuis un fichier CSV
df_csv = spark.read.csv(
    "/path/to/data.csv",
    header=True,
    inferSchema=True
)

# 3. Depuis un fichier Parquet
df_parquet = spark.read.parquet("/path/to/data.parquet")

# 4. Depuis une table Delta
df_delta = spark.read.format("delta").load("/path/to/delta-table")

# 5. Depuis JSON
df_json = spark.read.json("/path/to/data.json")

# 6. Depuis une requÃªte SQL
df_sql = spark.sql("SELECT * FROM my_table WHERE age > 30")
</code></pre>

                <h3>SchÃ©ma des DataFrames</h3>
                <pre><code class="language-python">from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# DÃ©finir un schÃ©ma explicitement
schema = StructType([
    StructField("name", StringType(), nullable=False),
    StructField("age", IntegerType(), nullable=True),
    StructField("department", StringType(), nullable=True)
])

# CrÃ©er DataFrame avec schÃ©ma
df = spark.createDataFrame(data, schema=schema)

# Afficher le schÃ©ma
df.printSchema()
# root
#  |-- name: string (nullable = false)
#  |-- age: integer (nullable = true)
#  |-- department: string (nullable = true)

# Obtenir le schÃ©ma sous forme de DDL
print(df.schema.simpleString())
</code></pre>
            </section>

            <section class="section">
                <h2>3. Transformations et Actions</h2>

                <p>Spark utilise deux types d'opÃ©rations :</p>

                <div class="grid">
                    <div class="grid-item">
                        <h4>Transformations (Lazy)</h4>
                        <p>CrÃ©ent un nouveau DataFrame sans exÃ©cution immÃ©diate</p>
                        <ul>
                            <li>select(), filter(), groupBy()</li>
                            <li>join(), orderBy(), distinct()</li>
                            <li>withColumn(), drop()</li>
                        </ul>
                    </div>
                    <div class="grid-item">
                        <h4>Actions (Eager)</h4>
                        <p>DÃ©clenchent l'exÃ©cution et retournent un rÃ©sultat</p>
                        <ul>
                            <li>show(), count(), collect()</li>
                            <li>take(), first(), head()</li>
                            <li>write.save(), foreach()</li>
                        </ul>
                    </div>
                </div>

                <h3>Transformations courantes</h3>
                <pre><code class="language-python">from pyspark.sql.functions import col, lit, when, avg, sum, count

# SELECT : SÃ©lectionner des colonnes
df_select = df.select("name", "age")
df_select = df.select(col("name"), col("age") + 1)

# FILTER / WHERE : Filtrer les lignes
df_filtered = df.filter(col("age") > 30)
df_filtered = df.where("age > 30 AND department = 'Engineering'")

# WITH COLUMN : Ajouter ou modifier une colonne
df_with_senior = df.withColumn(
    "is_senior",
    when(col("age") >= 40, lit("Yes")).otherwise(lit("No"))
)

df_with_salary = df.withColumn("estimated_salary", col("age") * 1000)

# DROP : Supprimer des colonnes
df_dropped = df.drop("department")

# DISTINCT : Valeurs uniques
df_unique = df.select("department").distinct()

# ORDER BY : Trier
df_sorted = df.orderBy(col("age").desc())

# LIMIT : Limiter le nombre de lignes
df_limited = df.limit(10)
</code></pre>

                <h3>AgrÃ©gations</h3>
                <pre><code class="language-python">from pyspark.sql.functions import avg, sum, count, min, max, stddev

# AgrÃ©gations simples
df.select(
    avg("age").alias("avg_age"),
    min("age").alias("min_age"),
    max("age").alias("max_age")
).show()

# GROUP BY
df_grouped = df.groupBy("department").agg(
    count("*").alias("employee_count"),
    avg("age").alias("avg_age"),
    min("age").alias("youngest"),
    max("age").alias("oldest")
)
df_grouped.show()

# Multiple groupBy
df.groupBy("department", "is_senior").agg(
    count("*").alias("count")
).show()
</code></pre>

                <h3>Jointures</h3>
                <pre><code class="language-python"># CrÃ©er deux DataFrames
employees = spark.createDataFrame([
    (1, "Alice", "Engineering"),
    (2, "Bob", "Sales"),
    (3, "Charlie", "Engineering")
], ["emp_id", "name", "dept"])

salaries = spark.createDataFrame([
    (1, 95000),
    (2, 85000),
    (3, 78000)
], ["emp_id", "salary"])

# INNER JOIN
df_inner = employees.join(salaries, on="emp_id", how="inner")
df_inner.show()

# LEFT JOIN
df_left = employees.join(salaries, on="emp_id", how="left")

# RIGHT JOIN
df_right = employees.join(salaries, on="emp_id", how="right")

# FULL OUTER JOIN
df_full = employees.join(salaries, on="emp_id", how="outer")

# JOIN avec conditions multiples
departments = spark.createDataFrame([
    ("Engineering", "Building A"),
    ("Sales", "Building B")
], ["dept_name", "location"])

df_complex_join = employees.join(
    departments,
    employees.dept == departments.dept_name,
    how="left"
).drop("dept_name")
df_complex_join.show()
</code></pre>
            </section>

            <section class="section">
                <h2>4. Spark SQL</h2>

                <h3>CrÃ©er des vues temporaires</h3>
                <pre><code class="language-python"># CrÃ©er une vue temporaire globale
df.createOrReplaceGlobalTempView("employees_global")

# CrÃ©er une vue temporaire locale (session)
df.createOrReplaceTempView("employees")

# RequÃªter avec SQL
result = spark.sql("""
    SELECT
        department,
        COUNT(*) as employee_count,
        AVG(age) as avg_age,
        MIN(age) as youngest,
        MAX(age) as oldest
    FROM employees
    GROUP BY department
    ORDER BY employee_count DESC
""")
result.show()
</code></pre>

                <h3>RequÃªtes SQL avancÃ©es</h3>
                <pre><code class="language-sql">%sql
-- Window functions
SELECT
  name,
  age,
  department,
  AVG(age) OVER (PARTITION BY department) as dept_avg_age,
  RANK() OVER (PARTITION BY department ORDER BY age DESC) as age_rank
FROM employees;

-- CTEs (Common Table Expressions)
WITH dept_stats AS (
  SELECT
    department,
    AVG(age) as avg_age,
    COUNT(*) as emp_count
  FROM employees
  GROUP BY department
)
SELECT
  e.name,
  e.age,
  e.department,
  ds.avg_age,
  ds.emp_count
FROM employees e
JOIN dept_stats ds ON e.department = ds.department
WHERE e.age > ds.avg_age;

-- Sous-requÃªtes
SELECT *
FROM employees
WHERE age > (SELECT AVG(age) FROM employees);
</code></pre>

                <h3>Fonctions SQL utiles</h3>
                <pre><code class="language-python">from pyspark.sql.functions import *

df_advanced = df.select(
    # Fonctions de chaÃ®ne
    upper(col("name")).alias("name_upper"),
    lower(col("name")).alias("name_lower"),
    length(col("name")).alias("name_length"),
    concat(col("name"), lit(" - "), col("department")).alias("full_desc"),

    # Fonctions de date
    current_date().alias("today"),
    current_timestamp().alias("now"),
    date_add(current_date(), 7).alias("next_week"),

    # Fonctions conditionnelles
    when(col("age") >= 40, "Senior")
    .when(col("age") >= 30, "Mid")
    .otherwise("Junior").alias("seniority"),

    # Fonctions mathÃ©matiques
    round(col("age") / 10, 2).alias("age_decades"),
    abs(col("age") - 35).alias("distance_from_35")
)
df_advanced.show(truncate=False)
</code></pre>
            </section>

            <section class="section">
                <h2>5. Optimisation des performances</h2>

                <h3>Catalyst Optimizer</h3>
                <p>Spark utilise Catalyst, un optimiseur de requÃªtes qui rÃ©organise automatiquement vos opÃ©rations pour de meilleures performances.</p>

                <pre><code class="language-python"># Voir le plan d'exÃ©cution logique
df.explain(mode="simple")

# Plan d'exÃ©cution dÃ©taillÃ©
df.explain(mode="extended")

# Plan d'exÃ©cution formatÃ©
df.explain(mode="formatted")

# CoÃ»t estimÃ©
df.explain(mode="cost")
</code></pre>

                <h3>Partitionnement</h3>
                <pre><code class="language-python"># VÃ©rifier le nombre de partitions
print(f"Nombre de partitions : {df.rdd.getNumPartitions()}")

# Repartitionner (shuffle)
df_repartitioned = df.repartition(10)
df_by_dept = df.repartition(10, "department")

# Coalesce (pas de shuffle, rÃ©duction uniquement)
df_coalesced = df.coalesce(5)

# Optimal : partition par colonne frÃ©quemment filtrÃ©e
df_optimized = df.repartition("department")
</code></pre>

                <h3>Mise en cache</h3>
                <pre><code class="language-python"># Cache en mÃ©moire
df.cache()  # ou df.persist()

# Utiliser le DataFrame mis en cache
df.count()  # Premier accÃ¨s : calcul et mise en cache
df.filter(col("age") > 30).count()  # RÃ©utilise le cache

# Retirer du cache
df.unpersist()

# Cache avec niveau de stockage personnalisÃ©
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_AND_DISK)
</code></pre>

                <h3>Bonnes pratiques d'optimisation</h3>
                <div class="grid">
                    <div class="grid-item">
                        <h4>Filtrage prÃ©coce</h4>
                        <p>Filtrez les donnÃ©es le plus tÃ´t possible pour rÃ©duire le volume traitÃ©</p>
                        <pre><code class="language-python"># Bon
df.filter(col("age") > 30) \
  .select("name", "dept") \
  .groupBy("dept").count()

# Moins bon
df.groupBy("dept").count() \
  .filter(col("count") > 10)</code></pre>
                    </div>
                    <div class="grid-item">
                        <h4>Ã‰viter collect()</h4>
                        <p>collect() ramÃ¨ne toutes les donnÃ©es au driver, risque d'OutOfMemory</p>
                        <pre><code class="language-python"># Dangereux sur big data
# all_data = df.collect()

# PrÃ©fÃ©rer
df.show(20)
df.take(10)
df.limit(100).toPandas()</code></pre>
                    </div>
                    <div class="grid-item">
                        <h4>Broadcast Join</h4>
                        <p>Pour joindre une petite table avec une grande</p>
                        <pre><code class="language-python">from pyspark.sql.functions import broadcast

# Broadcast automatique si < 10MB
df_large.join(
    broadcast(df_small),
    on="key"
)</code></pre>
                    </div>
                    <div class="grid-item">
                        <h4>Adaptive Query Execution</h4>
                        <p>Active par dÃ©faut dans Spark 3.x, optimise dynamiquement</p>
                        <pre><code class="language-python">spark.conf.set(
  "spark.sql.adaptive.enabled",
  "true"
)</code></pre>
                    </div>
                </div>

                <div class="alert alert-info">
                    <h4>FonctionnalitÃ©s spÃ©cifiques Databricks</h4>
                    <ul>
                        <li><strong>Photon Engine :</strong> Moteur vectorisÃ© C++ jusqu'Ã  4x plus rapide (Premium tier)</li>
                        <li><strong>Auto Optimize :</strong> Optimisation automatique des tables Delta</li>
                        <li><strong>Adaptive Query Execution :</strong> ActivÃ© par dÃ©faut</li>
                        <li><strong>Dynamic File Pruning :</strong> RÃ©duit les fichiers lus lors des jointures</li>
                    </ul>
                </div>
            </section>

            <section class="section">
                <h2>6. Exemple pratique complet</h2>

                <pre><code class="language-python"># ScÃ©nario : Analyse de ventes e-commerce

# 1. Charger les donnÃ©es
orders = spark.read.parquet("/mnt/data/orders/")
customers = spark.read.parquet("/mnt/data/customers/")
products = spark.read.parquet("/mnt/data/products/")

# 2. CrÃ©er des vues pour SQL
orders.createOrReplaceTempView("orders")
customers.createOrReplaceTempView("customers")
products.createOrReplaceTempView("products")

# 3. Analyse avec PySpark
from pyspark.sql.functions import *

# Jointure enrichie
enriched_orders = orders \
    .join(customers, on="customer_id", how="left") \
    .join(products, on="product_id", how="left") \
    .withColumn("order_date", to_date(col("timestamp"))) \
    .withColumn("revenue", col("quantity") * col("unit_price"))

# AgrÃ©gation par produit et mois
monthly_sales = enriched_orders \
    .withColumn("month", date_trunc("month", col("order_date"))) \
    .groupBy("month", "product_name", "category") \
    .agg(
        sum("revenue").alias("total_revenue"),
        sum("quantity").alias("total_quantity"),
        count("order_id").alias("order_count"),
        countDistinct("customer_id").alias("unique_customers")
    ) \
    .orderBy(col("month").desc(), col("total_revenue").desc())

# 4. Ou avec SQL
result = spark.sql("""
    SELECT
        DATE_TRUNC('month', o.order_date) as month,
        p.product_name,
        p.category,
        SUM(o.quantity * o.unit_price) as total_revenue,
        SUM(o.quantity) as total_quantity,
        COUNT(o.order_id) as order_count,
        COUNT(DISTINCT o.customer_id) as unique_customers
    FROM orders o
    LEFT JOIN products p ON o.product_id = p.product_id
    GROUP BY month, p.product_name, p.category
    ORDER BY month DESC, total_revenue DESC
""")

# 5. Sauvegarder les rÃ©sultats
result.write \
    .format("delta") \
    .mode("overwrite") \
    .partitionBy("month") \
    .save("/mnt/analytics/monthly_sales")

# 6. Afficher les insights
display(result.limit(20))
</code></pre>
            </section>

            <section class="section">
                <div class="key-points">
                    <h3>ğŸ“Œ Points clÃ©s Ã  retenir</h3>
                    <ul>
                        <li>Spark distribue le calcul sur plusieurs nÅ“uds (Driver + Workers)</li>
                        <li>DataFrames sont des collections distribuÃ©es avec schÃ©ma</li>
                        <li>Transformations (lazy) vs Actions (eager) - comprenez la diffÃ©rence</li>
                        <li>Spark SQL permet de requÃªter avec syntaxe SQL standard</li>
                        <li>Catalyst optimizer optimise automatiquement vos requÃªtes</li>
                        <li>Cache les DataFrames rÃ©utilisÃ©s, partitionnez intelligemment</li>
                        <li>Ã‰vitez collect() sur de gros volumes, privilÃ©giez show()/take()</li>
                    </ul>
                </div>
            </section>

            <div class="alert alert-warning">
                <h4>Prochaine Ã©tape</h4>
                <p>Vous maÃ®trisez Spark ! Dans la <strong>Partie 5</strong>, dÃ©couvrez Delta Lake pour des donnÃ©es fiables et performantes.</p>
            </div>
        </div>

        <nav class="nav-menu">
            <ul>
                <li><a href="../index.html">ğŸ  Accueil</a></li>
                <li><a href="partie3.html">â† Partie 3</a></li>
                <li><a href="partie4.html" class="active">Partie 4</a></li>
                <li><a href="partie5.html">Partie 5 : Delta Lake â†’</a></li>
            </ul>
        </nav>

        <footer>
            <p>&copy; 2024 Formation Simplon - Azure Databricks</p>
        </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
</body>
</html>
