{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "# Exercice 04 : Delta Lake Avance",
        "",
        "## Objectifs pedagogiques",
        "",
        "A la fin de cet exercice, vous serez capable de :",
        "- Utiliser le Time Travel pour consulter les anciennes versions",
        "- Effectuer des MERGE (upserts) pour mettre a jour les donnees",
        "- Implementer le Change Data Capture (CDC)",
        "- Optimiser les tables avec OPTIMIZE et Z-ORDER",
        "- Nettoyer les anciennes versions avec VACUUM",
        "- Gerer les schemas et l'evolution des donnees",
        "",
        "## Duree estimee : 90 minutes",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 1 : Concepts theoriques",
        "",
        "### Qu'est-ce que Delta Lake ?",
        "",
        "Delta Lake est une couche de stockage qui apporte :",
        "- **ACID transactions** : Atomicite, Coherence, Isolation, Durabilite",
        "- **Time Travel** : Acces aux versions precedentes",
        "- **Schema enforcement** : Validation du schema",
        "- **Schema evolution** : Evolution controlee du schema",
        "- **Audit trail** : Historique complet des modifications",
        "",
        "### Architecture Delta",
        "",
        "Une table Delta = Fichiers Parquet + Transaction Log (_delta_log/)",
        "",
        "Le transaction log enregistre :",
        "- Chaque modification (INSERT, UPDATE, DELETE, MERGE)",
        "- Les metadonnees (schema, partitions)",
        "- Les statistiques (min, max, count par fichier)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 2 : Historique et versions",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.1 : Consulter l'historique d'une table\")\nprint(\"=\" * 70)\n\n# Voir l'historique complet de la table\ndf_history = spark.sql(\"DESCRIBE HISTORY gares_silver\")\n\nprint(\"Historique de la table gares_silver :\")\ndf_history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(10, truncate=False)\n\n# Compter les versions\nnb_versions = df_history.count()\nprint(f\"\\nNombre de versions : {nb_versions}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 3 : Time Travel (voyage dans le temps)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.2 : Lire une version specifique\")\nprint(\"=\" * 70)\n\n# Methode 1 : Lire par numero de version\ndf_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"gares_silver\")\nprint(f\"Version 0 : {df_version_0.count()} lignes\")\n\n# Methode 2 : Lire par timestamp\nfrom datetime import datetime, timedelta\n\n# Lire la version d'il y a 1 heure (si elle existe)\ntimestamp_1h_ago = (datetime.now() - timedelta(hours=1)).strftime(\"%Y-%m-%d %H:%M:%S\")\ntry:\n    df_timestamp = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_1h_ago).table(\"gares_silver\")\n    print(f\"Version d'il y a 1h : {df_timestamp.count()} lignes\")\nexcept:\n    print(\"Pas de version disponible il y a 1 heure\")\n\n# Methode 3 : Syntaxe SQL\nspark.sql(\"SELECT COUNT(*) FROM gares_silver VERSION AS OF 0\").show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 4 : Modifications des donnees",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.3 : INSERT de nouvelles donnees\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import lit\n\n# Creer des nouvelles gares fictives pour l'exemple\nnouvelles_gares = [\n    (\"Gare Test 1\", \"TST1\", \"A\", \"48.8566, 2.3522\", \"75056\", \"87001001\", 48.8566, 2.3522, \"75\", \"A\", \"Gare Principale\"),\n    (\"Gare Test 2\", \"TST2\", \"B\", \"45.7640, 4.8357\", \"69123\", \"87002002\", 45.7640, 4.8357, \"69\", \"B\", \"Gare Importante\")\n]\n\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\nschema_gares = StructType([\n    StructField(\"nom\", StringType(), True),\n    StructField(\"trigramme\", StringType(), True),\n    StructField(\"segment_drg\", StringType(), True),\n    StructField(\"position_geo\", StringType(), True),\n    StructField(\"code_commune\", StringType(), True),\n    StructField(\"code_uic\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True),\n    StructField(\"code_departement\", StringType(), True),\n    StructField(\"segment_clean\", StringType(), True),\n    StructField(\"categorie_gare\", StringType(), True)\n])\n\ndf_nouvelles = spark.createDataFrame(nouvelles_gares, schema_gares)\n\n# Creer une table temporaire pour les tests\ndf_gares_original = spark.table(\"gares_silver\")\ndf_gares_original.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gares_test\")\n\nprint(f\"Table de test creee : {spark.table('gares_test').count()} lignes\")\n\n# Inserer les nouvelles gares\ndf_nouvelles.write.format(\"delta\").mode(\"append\").saveAsTable(\"gares_test\")\n\nprint(f\"Apres insertion : {spark.table('gares_test').count()} lignes\")\n\n# Verifier l'historique\nspark.sql(\"DESCRIBE HISTORY gares_test\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 5 : UPDATE et DELETE",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.4 : UPDATE avec SQL\")\nprint(\"=\" * 70)\n\n# UPDATE : Modifier les gares de test\nspark.sql(\"\"\"\n    UPDATE gares_test\n    SET segment_clean = 'X', categorie_gare = 'Gare Test'\n    WHERE trigramme LIKE 'TST%'\n\"\"\")\n\nprint(\"Gares modifiees :\")\nspark.sql(\"SELECT * FROM gares_test WHERE trigramme LIKE 'TST%'\").show()\n\n# Verifier l'historique\nspark.sql(\"DESCRIBE HISTORY gares_test\").select(\"version\", \"operation\", \"operationMetrics\").limit(3).show(truncate=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.5 : DELETE avec SQL\")\nprint(\"=\" * 70)\n\n# DELETE : Supprimer les gares de test\nspark.sql(\"\"\"\n    DELETE FROM gares_test\n    WHERE trigramme LIKE 'TST%'\n\"\"\")\n\nprint(f\"Apres suppression : {spark.table('gares_test').count()} lignes\")\n\n# Verifier qu'elles ont bien ete supprimees\nspark.sql(\"SELECT * FROM gares_test WHERE trigramme LIKE 'TST%'\").show()\n\n# Verifier l'historique\nprint(\"\\nHistorique des operations :\")\nspark.sql(\"DESCRIBE HISTORY gares_test\").select(\"version\", \"operation\", \"operationMetrics\").limit(5).show(truncate=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 4.6 : Restaurer une version precedente",
        "",
        "Les gares de test ont ete supprimees. Utilisez Time Travel pour les recuperer !",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\n# 1. Trouvez la version AVANT la suppression\nprint(\"Versions disponibles :\")\nspark.sql(\"DESCRIBE HISTORY gares_test\").select(\"version\", \"operation\").show(10)\n\n# 2. Restaurez cette version (utilisez RESTORE)\nspark.sql(\"RESTORE TABLE gares_test TO VERSION AS OF 2\")\n\n# 3. Verifiez que les gares sont revenues\nprint(\"\\nVerification :\")\nspark.sql(\"SELECT * FROM gares_test WHERE trigramme LIKE 'TST%'\").show()\n\nprint(f\"\\nNombre total de lignes : {spark.table('gares_test').count()}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 6 : MERGE (Upsert)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.7 : MERGE - INSERT + UPDATE en une operation\")\nprint(\"=\" * 70)\n\n# Creer des donnees pour le MERGE\n# - Certaines gares existent deja (UPDATE)\n# - D'autres sont nouvelles (INSERT)\ndonnees_merge = [\n    (\"Gare Test 1\", \"TST1\", \"A\", \"75\", \"Gare Modifiee\"),  # Existe -> UPDATE\n    (\"Gare Test 3\", \"TST3\", \"C\", \"13\", \"Gare Nouvelle\")   # N'existe pas -> INSERT\n]\n\nschema_merge = StructType([\n    StructField(\"nom\", StringType(), True),\n    StructField(\"trigramme\", StringType(), True),\n    StructField(\"segment_clean\", StringType(), True),\n    StructField(\"code_departement\", StringType(), True),\n    StructField(\"categorie_gare\", StringType(), True)\n])\n\ndf_updates = spark.createDataFrame(donnees_merge, schema_merge)\n\n# Enregistrer comme table temporaire\ndf_updates.createOrReplaceTempView(\"updates_gares\")\n\n# MERGE : UPDATE si existe, INSERT sinon\nspark.sql(\"\"\"\n    MERGE INTO gares_test AS target\n    USING updates_gares AS source\n    ON target.trigramme = source.trigramme\n    WHEN MATCHED THEN\n        UPDATE SET\n            target.categorie_gare = source.categorie_gare,\n            target.segment_clean = source.segment_clean\n    WHEN NOT MATCHED THEN\n        INSERT (nom, trigramme, segment_clean, code_departement, categorie_gare)\n        VALUES (source.nom, source.trigramme, source.segment_clean, source.code_departement, source.categorie_gare)\n\"\"\")\n\nprint(\"Resultat du MERGE :\")\nspark.sql(\"SELECT * FROM gares_test WHERE trigramme LIKE 'TST%' ORDER BY trigramme\").show()\n\n# Historique\nprint(\"\\nHistorique :\")\nspark.sql(\"DESCRIBE HISTORY gares_test\").select(\"version\", \"operation\", \"operationMetrics\").limit(3).show(truncate=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 4.8 : MERGE avec DELETE",
        "",
        "Implementez un MERGE qui :",
        "1. UPDATE si la gare existe et segment_clean = 'A'",
        "2. DELETE si la gare existe mais segment_clean != 'A'",
        "3. INSERT si la gare n'existe pas",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\n# Creer des donnees de test\ndonnees_merge_delete = [\n    (\"Gare Test 1\", \"TST1\", \"A\", \"75\", \"Principale Mise a Jour\"),  # UPDATE\n    (\"Gare Test 2\", \"TST2\", \"C\", \"69\", \"A Supprimer\"),             # DELETE\n    (\"Gare Test 4\", \"TST4\", \"B\", \"33\", \"Nouvelle Gare\")            # INSERT\n]\n\ndf_merge_delete = spark.createDataFrame(donnees_merge_delete, schema_merge)\ndf_merge_delete.createOrReplaceTempView(\"merge_updates\")\n\n# MERGE avec DELETE\nspark.sql(\"\"\"\n    MERGE INTO gares_test AS target\n    USING merge_updates AS source\n    ON target.trigramme = source.trigramme\n    WHEN MATCHED AND source.segment_clean = 'A' THEN\n        UPDATE SET target.categorie_gare = source.categorie_gare\n    WHEN MATCHED AND source.segment_clean != 'A' THEN\n        DELETE\n    WHEN NOT MATCHED THEN\n        INSERT (nom, trigramme, segment_clean, code_departement, categorie_gare)\n        VALUES (source.nom, source.trigramme, source.segment_clean, source.code_departement, source.categorie_gare)\n\"\"\")\n\nprint(\"Resultat du MERGE avec DELETE :\")\nspark.sql(\"SELECT * FROM gares_test WHERE trigramme LIKE 'TST%' ORDER BY trigramme\").show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 7 : Change Data Capture (CDC)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.9 : Implementer CDC avec CDF (Change Data Feed)\")\nprint(\"=\" * 70)\n\n# Activer CDF sur la table\nspark.sql(\"ALTER TABLE gares_test SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n\nprint(\"CDF active sur gares_test\")\n\n# Faire quelques modifications\nspark.sql(\"UPDATE gares_test SET segment_clean = 'Z' WHERE trigramme = 'TST1'\")\nspark.sql(\"DELETE FROM gares_test WHERE trigramme = 'TST3'\")\n\n# Lire les changements entre deux versions\ndf_changes = spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 6) \\\n    .table(\"gares_test\")\n\nprint(\"\\nChangements captures par CDF :\")\ndf_changes.select(\"nom\", \"trigramme\", \"_change_type\", \"_commit_version\").show()\n\nprint(\"\"\"\nTypes de changements :\n- insert : Nouvelle ligne\n- update_preimage : Ancienne valeur\n- update_postimage : Nouvelle valeur\n- delete : Ligne supprimee\n\"\"\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 8 : OPTIMIZE et Z-ORDER",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.10 : Optimiser les performances avec OPTIMIZE\")\nprint(\"=\" * 70)\n\n# Voir les statistiques avant optimisation\nspark.sql(\"DESCRIBE DETAIL gares_silver\").select(\"numFiles\", \"sizeInBytes\").show()\n\n# OPTIMIZE : Compacter les petits fichiers\nspark.sql(\"OPTIMIZE gares_silver\")\n\nprint(\"\\nApres OPTIMIZE :\")\nspark.sql(\"DESCRIBE DETAIL gares_silver\").select(\"numFiles\", \"sizeInBytes\").show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.11 : Z-ORDERING pour accelerer les filtres\")\nprint(\"=\" * 70)\n\n# Z-ORDER sur les colonnes frequemment filtrees\nspark.sql(\"OPTIMIZE gares_silver ZORDER BY (code_departement, segment_clean)\")\n\nprint(\"Z-ORDERING applique sur code_departement et segment_clean\")\nprint(\"Les requetes filtrant sur ces colonnes seront beaucoup plus rapides !\")\n\n# Verifier l'historique\nspark.sql(\"DESCRIBE HISTORY gares_silver\").select(\"version\", \"operation\").limit(5).show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 9 : VACUUM (nettoyage)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.12 : Nettoyer les anciennes versions avec VACUUM\")\nprint(\"=\" * 70)\n\n# Voir la taille actuelle\ndetail_avant = spark.sql(\"DESCRIBE DETAIL gares_test\")\nprint(\"Avant VACUUM :\")\ndetail_avant.select(\"location\", \"numFiles\", \"sizeInBytes\").show(truncate=False)\n\n# VACUUM : Supprimer les fichiers de plus de 168 heures (7 jours)\n# ATTENTION : Cela supprime les anciennes versions (Time Travel limite)\n\n# Pour tester, on peut reduire la retention (EN DEV UNIQUEMENT !)\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\nspark.sql(\"VACUUM gares_test RETAIN 0 HOURS\")\n\nprint(\"\\nApres VACUUM :\")\nspark.sql(\"DESCRIBE DETAIL gares_test\").select(\"location\", \"numFiles\", \"sizeInBytes\").show(truncate=False)\n\nprint(\"\"\"\nATTENTION :\n- VACUUM supprime les anciennes versions\n- Time Travel ne fonctionnera plus pour les versions supprimees\n- En production, garder au moins 7 jours (RETAIN 168 HOURS)\n\"\"\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 10 : Schema Evolution",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.13 : Evolution du schema\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import lit\n\n# Lire la table actuelle\ndf_gares = spark.table(\"gares_test\")\n\n# Ajouter une nouvelle colonne\ndf_avec_nouvelle_colonne = df_gares.withColumn(\"pays\", lit(\"France\"))\n\n# Essayer d'inserer SANS schema evolution (va echouer)\ntry:\n    df_avec_nouvelle_colonne.write.format(\"delta\").mode(\"append\").saveAsTable(\"gares_test\")\n    print(\"Insertion reussie\")\nexcept Exception as e:\n    print(f\"Erreur attendue : {str(e)[:100]}...\")\n\n# Activer l'evolution du schema\ndf_avec_nouvelle_colonne.write \\\n    .format(\"delta\") \\\n    .mode(\"append\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .saveAsTable(\"gares_test\")\n\nprint(\"\\nSchema evolue avec succes !\")\n\n# Verifier le nouveau schema\nprint(\"\\nNouveau schema :\")\nspark.table(\"gares_test\").printSchema()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 11 : Constraints (contraintes)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 4.14 : Ajouter des contraintes\")\nprint(\"=\" * 70)\n\n# Ajouter une contrainte CHECK\nspark.sql(\"\"\"\n    ALTER TABLE gares_test\n    ADD CONSTRAINT valid_segment CHECK (segment_clean IN ('A', 'B', 'C', 'X', 'Z'))\n\"\"\")\n\nprint(\"Contrainte ajoutee : segment_clean doit etre A, B, C, X ou Z\")\n\n# Essayer d'inserer une valeur invalide\ntry:\n    spark.sql(\"INSERT INTO gares_test (nom, trigramme, segment_clean) VALUES ('Test Invalid', 'INV', 'INVALID')\")\nexcept Exception as e:\n    print(f\"\\nErreur attendue : {str(e)[:150]}...\")\n\n# Verifier les contraintes\nspark.sql(\"SHOW TBLPROPERTIES gares_test\").filter(\"key LIKE '%constraint%'\").show(truncate=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE FINAL 4.15 : Pipeline CDC complete",
        "",
        "Implementez une pipeline CDC complete qui :",
        "",
        "1. Cree une table source avec CDF active",
        "2. Simule des modifications (INSERT, UPDATE, DELETE)",
        "3. Lit les changements avec CDF",
        "4. Applique les changements a une table cible avec MERGE",
        "5. Optimise la table cible",
        "6. Documente toutes les versions",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER - SOLUTION PROPOSEE\nprint(\"PIPELINE CDC COMPLETE\")\nprint(\"=\" * 70)\n\n# 1. Creer une table source avec CDF\ndf_source = spark.table(\"gares_silver\").limit(100)\ndf_source.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gares_source_cdc\")\nspark.sql(\"ALTER TABLE gares_source_cdc SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n\n# 2. Creer une table cible\ndf_source.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gares_cible_cdc\")\n\nprint(\"Tables source et cible creees\")\n\n# 3. Simuler des modifications sur la source\nspark.sql(\"UPDATE gares_source_cdc SET segment_clean = 'A' WHERE segment_clean = 'B' LIMIT 5\")\nspark.sql(\"DELETE FROM gares_source_cdc WHERE segment_clean = 'C' LIMIT 3\")\n\n# Nouvelles insertions\nnouvelles = spark.table(\"gares_silver\").limit(5).withColumn(\"trigramme\", lit(\"NEW\"))\nnouvelles.write.format(\"delta\").mode(\"append\").saveAsTable(\"gares_source_cdc\")\n\nprint(\"\\nModifications appliquees a la source\")\n\n# 4. Lire les changements\nversion_actuelle = spark.sql(\"DESCRIBE HISTORY gares_cible_cdc\").select(\"version\").first()[0]\n\ndf_changes = spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 1) \\\n    .table(\"gares_source_cdc\")\n\nprint(f\"\\nChangements detectes : {df_changes.count()} lignes\")\ndf_changes.groupBy(\"_change_type\").count().show()\n\n# 5. Appliquer les changements a la cible avec MERGE\n# (Simplifie ici - en production, traiter chaque type de changement)\ndf_etat_actuel = spark.table(\"gares_source_cdc\")\ndf_etat_actuel.createOrReplaceTempView(\"source_actuelle\")\n\nspark.sql(\"\"\"\n    MERGE INTO gares_cible_cdc AS target\n    USING source_actuelle AS source\n    ON target.trigramme = source.trigramme AND target.code_uic = source.code_uic\n    WHEN MATCHED THEN UPDATE SET *\n    WHEN NOT MATCHED THEN INSERT *\n\"\"\")\n\nprint(\"\\nChangements appliques a la cible\")\n\n# 6. Optimiser\nspark.sql(\"OPTIMIZE gares_cible_cdc\")\nprint(\"Table cible optimisee\")\n\n# 7. Documenter\nprint(\"\\nHistorique complet :\")\nspark.sql(\"DESCRIBE HISTORY gares_cible_cdc\").select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## RESUME DES CONCEPTS APPRIS",
        "",
        "Dans cet exercice, vous avez appris :",
        "",
        "1. **Time Travel** : `VERSION AS OF`, `TIMESTAMP AS OF`, `RESTORE`",
        "2. **DML Operations** : `INSERT`, `UPDATE`, `DELETE`",
        "3. **MERGE** : Upserts (INSERT + UPDATE en une operation)",
        "4. **Change Data Feed** : Capturer les modifications",
        "5. **OPTIMIZE** : Compacter les fichiers",
        "6. **Z-ORDER** : Optimiser les performances des filtres",
        "7. **VACUUM** : Nettoyer les anciennes versions",
        "8. **Schema Evolution** : `mergeSchema`",
        "9. **Constraints** : Validation des donnees",
        "10. **Historique** : `DESCRIBE HISTORY`",
        "",
        "## Bonnes pratiques Delta Lake",
        "",
        "- Activer CDF sur les tables importantes",
        "- OPTIMIZE regulierement (ex: quotidien)",
        "- Z-ORDER sur les colonnes de filtre",
        "- VACUUM avec retention >= 7 jours",
        "- Utiliser MERGE pour les upserts",
        "- Documenter les changements de schema",
        "",
        "## Prochaine etape",
        "",
        "Passez a l'**Exercice 05 : Streaming en temps reel** pour apprendre :",
        "- Structured Streaming",
        "- Ingestion en continu",
        "- Traitement en micro-batches",
        "- Integration avec Delta Lake",
        ""
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Exercice_04_Delta_Lake_Avance",
      "dashboards": [],
      "language": "python",
      "widgets": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}