{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "# Exercice 01 : Manipulation de DataFrames Spark",
        "",
        "## Objectifs pedagogiques",
        "",
        "A la fin de cet exercice, vous serez capable de :",
        "- Charger des donnees depuis une table Delta",
        "- Selectionner des colonnes specifiques",
        "- Filtrer des lignes selon des conditions",
        "- Trier des donnees",
        "- Renommer des colonnes",
        "- Creer de nouvelles colonnes calculees",
        "",
        "## Duree estimee : 45 minutes",
        "",
        "## Dataset : Gares SNCF",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 1 : Concepts theoriques",
        "",
        "### Qu'est-ce qu'un DataFrame Spark ?",
        "",
        "Un DataFrame est une collection distribuee de donnees organisees en colonnes nommees.",
        "C'est equivalent a une table SQL ou un dataframe Pandas, mais optimise pour le traitement distribue.",
        "",
        "### Caracteristiques principales :",
        "- **Immutable** : Chaque transformation cree un nouveau DataFrame",
        "- **Lazy Evaluation** : Les transformations ne sont executees que quand une action est demandee",
        "- **Distribue** : Les donnees sont reparties sur plusieurs machines",
        "- **Optimise** : Le moteur Catalyst optimise automatiquement les requetes",
        "",
        "### Transformations vs Actions :",
        "",
        "**Transformations** (lazy - ne s'executent pas immediatement) :",
        "- `select()`, `filter()`, `withColumn()`, `orderBy()`, etc.",
        "",
        "**Actions** (executent les transformations) :",
        "- `show()`, `count()`, `collect()`, `write()`, etc.",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 2 : Chargement des donnees",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 1.1 : Charger la table des gares\")\nprint(\"=\" * 70)\n\n# Charger la table Silver des gares\ndf_gares = spark.table(\"gares_silver\")\n\n# Afficher le nombre de lignes et de colonnes\nprint(f\"Nombre de lignes : {df_gares.count()}\")\nprint(f\"Nombre de colonnes : {len(df_gares.columns)}\")\n\n# Afficher les 5 premieres lignes\nprint(\"\\nApercu des donnees :\")\ndf_gares.show(5)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 3 : Selection de colonnes",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 1.2 : Selection de colonnes\")\nprint(\"=\" * 70)\n\n# Methode 1 : Selectionner des colonnes specifiques\ndf_simple = df_gares.select(\"nom\", \"trigramme\", \"segment_clean\", \"code_departement\")\n\nprint(\"Selection de 4 colonnes :\")\ndf_simple.show(10)\n\n# Methode 2 : Selectionner avec alias\nfrom pyspark.sql.functions import col\n\ndf_alias = df_gares.select(\n    col(\"nom\").alias(\"nom_gare\"),\n    col(\"trigramme\").alias(\"code\"),\n    col(\"categorie_gare\").alias(\"categorie\")\n)\n\nprint(\"\\nSelection avec alias :\")\ndf_alias.show(10)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 1.3",
        "",
        "A VOUS : Selectionnez les colonnes suivantes avec des alias :",
        "- nom vers gare",
        "- code_departement vers dept",
        "- latitude vers lat",
        "- longitude vers lon",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\nfrom pyspark.sql.functions import col\n\ndf_exercice = df_gares.select(\n    # Completez ici\n    col(\"nom\").alias(\"gare\"),\n    col(\"code_departement\").alias(\"dept\"),\n    col(\"latitude\").alias(\"lat\"),\n    col(\"longitude\").alias(\"lon\")\n)\n\ndf_exercice.show(10)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 4 : Filtrage de donnees",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 1.4 : Filtrage de lignes\")\nprint(\"=\" * 70)\n\n# Methode 1 : Filtre simple\ndf_paris = df_gares.filter(col(\"code_departement\") == \"75\")\nprint(f\"Gares a Paris (75) : {df_paris.count()}\")\ndf_paris.show(10)\n\n# Methode 2 : Filtres multiples avec AND\ndf_principales_paris = df_gares.filter(\n    (col(\"code_departement\") == \"75\") &\n    (col(\"segment_clean\") == \"A\")\n)\nprint(f\"\\nGares principales a Paris : {df_principales_paris.count()}\")\ndf_principales_paris.show()\n\n# Methode 3 : Filtres avec OR\ndf_grandes_villes = df_gares.filter(\n    (col(\"code_departement\") == \"75\") |\n    (col(\"code_departement\") == \"69\") |\n    (col(\"code_departement\") == \"13\")\n)\nprint(f\"\\nGares a Paris, Lyon ou Marseille : {df_grandes_villes.count()}\")\n\n# Methode 4 : Utiliser isin() pour plusieurs valeurs\ndf_grandes_villes_2 = df_gares.filter(col(\"code_departement\").isin(\"75\", \"69\", \"13\"))\nprint(f\"Meme resultat avec isin() : {df_grandes_villes_2.count()}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 1.5",
        "",
        "A VOUS : Trouvez toutes les gares qui :",
        "- Sont de segment A (principales)",
        "- Ont \"Paris\" dans leur nom",
        "- Afficher uniquement : nom, trigramme, categorie_gare",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\nfrom pyspark.sql.functions import col\n\ndf_exercice = df_gares.filter(\n    # Completez les conditions ici\n    (col(\"segment_clean\") == \"A\") &\n    (col(\"nom\").like(\"%Paris%\"))\n).select(\"nom\", \"trigramme\", \"categorie_gare\")\n\nprint(f\"Nombre de gares trouvees : {df_exercice.count()}\")\ndf_exercice.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 5 : Tri des donnees",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 1.6 : Tri des donnees\")\nprint(\"=\" * 70)\n\n# Tri croissant\ndf_tri_asc = df_gares.orderBy(\"nom\")\nprint(\"Tri croissant par nom :\")\ndf_tri_asc.select(\"nom\", \"trigramme\").show(10)\n\n# Tri decroissant\ndf_tri_desc = df_gares.orderBy(col(\"nom\").desc())\nprint(\"\\nTri decroissant par nom :\")\ndf_tri_desc.select(\"nom\", \"trigramme\").show(10)\n\n# Tri sur plusieurs colonnes\ndf_tri_multi = df_gares.orderBy(\n    col(\"code_departement\").asc(),\n    col(\"nom\").asc()\n)\nprint(\"\\nTri par departement puis par nom :\")\ndf_tri_multi.select(\"code_departement\", \"nom\", \"trigramme\").show(15)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 6 : Creation de nouvelles colonnes",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 1.7 : Creer de nouvelles colonnes\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import col, when, concat, lit, upper, length\n\n# Methode 1 : Colonne simple calculee\ndf_avec_region = df_gares.withColumn(\n    \"est_ile_de_france\",\n    when(col(\"code_departement\").isin(\"75\", \"77\", \"78\", \"91\", \"92\", \"93\", \"94\", \"95\"), \"Oui\")\n    .otherwise(\"Non\")\n)\n\nprint(\"Ajout d'une colonne est_ile_de_france :\")\ndf_avec_region.select(\"nom\", \"code_departement\", \"est_ile_de_france\").show(10)\n\n# Methode 2 : Concatenation de colonnes\ndf_avec_libelle = df_gares.withColumn(\n    \"libelle_complet\",\n    concat(col(\"nom\"), lit(\" (\"), col(\"trigramme\"), lit(\")\"))\n)\n\nprint(\"\\nConcatenation de colonnes :\")\ndf_avec_libelle.select(\"libelle_complet\", \"segment_clean\").show(10)\n\n# Methode 3 : Transformations de texte\ndf_avec_maj = df_gares.withColumn(\n    \"nom_majuscule\",\n    upper(col(\"nom\"))\n).withColumn(\n    \"longueur_nom\",\n    length(col(\"nom\"))\n)\n\nprint(\"\\nTransformations de texte :\")\ndf_avec_maj.select(\"nom\", \"nom_majuscule\", \"longueur_nom\").show(10)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 1.8",
        "",
        "A VOUS : Creer les colonnes suivantes :",
        "",
        "1. **type_gare** :",
        "- \"Majeure\" si segment A ou B",
        "- \"Locale\" si segment C",
        "",
        "2. **identifiant** :",
        "- Concatener le code departement et le trigramme avec un tiret",
        "- Exemple : \"75-MON\" pour Montparnasse",
        "",
        "3. **nom_court** :",
        "- Si le nom fait plus de 20 caracteres, True, sinon False",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\nfrom pyspark.sql.functions import col, when, concat, lit, length\n\ndf_exercice = df_gares \\\n    .withColumn(\n        \"type_gare\",\n        # Completez ici\n        when(col(\"segment_clean\").isin(\"A\", \"B\"), \"Majeure\")\n        .otherwise(\"Locale\")\n    ) \\\n    .withColumn(\n        \"identifiant\",\n        # Completez ici\n        concat(col(\"code_departement\"), lit(\"-\"), col(\"trigramme\"))\n    ) \\\n    .withColumn(\n        \"nom_court\",\n        # Completez ici\n        when(length(col(\"nom\")) > 20, True).otherwise(False)\n    )\n\n# Afficher le resultat\ndf_exercice.select(\"nom\", \"type_gare\", \"identifiant\", \"nom_court\").show(15)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 7 : Combinaison de toutes les operations",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 1.9 : Pipeline complete\")\nprint(\"=\" * 70)\n\n# Exemple de pipeline complete combinant toutes les operations\ndf_final = df_gares \\\n    .filter(col(\"segment_clean\").isNotNull()) \\\n    .withColumn(\n        \"region_approx\",\n        when(col(\"code_departement\").isin(\"75\", \"77\", \"78\", \"91\", \"92\", \"93\", \"94\", \"95\"), \"Ile-de-France\")\n        .when(col(\"code_departement\").isin(\"69\", \"01\", \"42\", \"38\", \"73\", \"74\"), \"Auvergne-Rhone-Alpes\")\n        .when(col(\"code_departement\").isin(\"13\", \"83\", \"84\", \"04\", \"05\", \"06\"), \"PACA\")\n        .otherwise(\"Autre\")\n    ) \\\n    .withColumn(\n        \"importance\",\n        when(col(\"segment_clean\") == \"A\", 3)\n        .when(col(\"segment_clean\") == \"B\", 2)\n        .otherwise(1)\n    ) \\\n    .select(\n        \"nom\",\n        \"trigramme\",\n        \"code_departement\",\n        \"region_approx\",\n        \"categorie_gare\",\n        \"importance\"\n    ) \\\n    .orderBy(col(\"importance\").desc(), col(\"nom\").asc())\n\nprint(\"Pipeline complete executee :\")\ndf_final.show(20)\n\nprint(f\"\\nNombre total de gares traitees : {df_final.count()}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE FINAL 1.10 : Projet complet",
        "",
        "Creez une pipeline complete qui :",
        "",
        "1. Charge la table gares_silver",
        "2. Filtre uniquement les gares des segments A et B",
        "3. Ajoute une colonne \"zone_geo\" :",
        "- \"Nord\" si code dept commence par 0, 5, 59, 62, 80",
        "- \"Sud\" si code dept commence par 1, 2, 3, 4, 6, 83, 84",
        "- \"Autre\" sinon",
        "4. Ajoute une colonne \"a_coordonnees\" : True si latitude ET longitude non nulles",
        "5. Selectionne : nom, trigramme, zone_geo, a_coordonnees, categorie_gare",
        "6. Trie par zone_geo puis nom",
        "7. Affiche les 30 premieres lignes",
        "8. Compte combien de gares par zone_geo",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER - SOLUTION PROPOSEE\nfrom pyspark.sql.functions import col, when\n\n# Pipeline complete\ndf_projet = df_gares \\\n    .filter(col(\"segment_clean\").isin(\"A\", \"B\")) \\\n    .withColumn(\n        \"zone_geo\",\n        when(col(\"code_departement\").isin(\"02\", \"08\", \"51\", \"59\", \"62\", \"80\"), \"Nord\")\n        .when(col(\"code_departement\").isin(\"13\", \"30\", \"34\", \"66\", \"83\", \"84\"), \"Sud\")\n        .otherwise(\"Autre\")\n    ) \\\n    .withColumn(\n        \"a_coordonnees\",\n        when((col(\"latitude\").isNotNull()) & (col(\"longitude\").isNotNull()), True)\n        .otherwise(False)\n    ) \\\n    .select(\"nom\", \"trigramme\", \"zone_geo\", \"a_coordonnees\", \"categorie_gare\") \\\n    .orderBy(\"zone_geo\", \"nom\")\n\nprint(\"Resultat de la pipeline :\")\ndf_projet.show(30)\n\nprint(\"\\nRepartition par zone geographique :\")\ndf_projet.groupBy(\"zone_geo\").count().orderBy(\"count\", ascending=False).show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 8 : Sauvegarde du resultat",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 1.11 : Sauvegarder le resultat\")\nprint(\"=\" * 70)\n\n# Sauvegarder le DataFrame final en table Delta\ndf_projet.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gares_exercice_01\")\n\nprint(\"Table 'gares_exercice_01' creee avec succes !\")\n\n# Verifier la table\ndf_verification = spark.table(\"gares_exercice_01\")\nprint(f\"\\nVerification : {df_verification.count()} lignes sauvegardees\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## RESUME DES CONCEPTS APPRIS",
        "",
        "Dans cet exercice, vous avez appris :",
        "",
        "1. **Charger des donnees** : `spark.table()`",
        "2. **Selectionner des colonnes** : `select()`, `alias()`",
        "3. **Filtrer des lignes** : `filter()`, `isin()`, `like()`",
        "4. **Trier des donnees** : `orderBy()`, `asc()`, `desc()`",
        "5. **Creer des colonnes** : `withColumn()`, `when()`, `otherwise()`",
        "6. **Transformer du texte** : `concat()`, `upper()`, `length()`",
        "7. **Enchainer des operations** : Pipeline avec `\\`",
        "8. **Sauvegarder** : `write.saveAsTable()`",
        "",
        "## Prochaine etape",
        "",
        "Passez a l'**Exercice 02 : Transformations et agregations** pour apprendre :",
        "- Les fonctions d'agregation (sum, avg, count, etc.)",
        "- Les regroupements (groupBy)",
        "- Les jointures entre tables",
        "- Les fonctions de date et heure",
        ""
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Exercice_01_Manipulation_DataFrames",
      "dashboards": [],
      "language": "python",
      "widgets": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}