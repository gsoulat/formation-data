# üêò Brief Pratique Hadoop - Pipeline Big Data E-commerce

**Dur√©e estim√©e :** 8-10 heures
**Niveau :** Interm√©diaire
**Modalit√© :** Pratique individuelle ou bin√¥me
**Pr√©requis :** Avoir suivi la formation Hadoop (Parties 1-6)

---

## üéØ Objectifs du Brief

√Ä l'issue de ce brief, vous serez capable de :

- Mettre en place un environnement Hadoop complet
- Ing√©rer et stocker des donn√©es dans HDFS
- Effectuer des traitements avec MapReduce
- Analyser des donn√©es avec Hive
- Cr√©er un pipeline Big Data de bout en bout

---

## üìã Contexte

Vous √™tes Data Engineer chez **DataMart**, une plateforme e-commerce en pleine croissance. L'entreprise g√©n√®re des millions de transactions par jour et souhaite analyser ses donn√©es pour :

- Comprendre le comportement d'achat des clients
- Identifier les produits les plus vendus
- Analyser les tendances de ventes par cat√©gorie et par r√©gion
- D√©tecter les p√©riodes de forte activit√©

Votre mission : Cr√©er un **pipeline Big Data** complet avec Hadoop pour ing√©rer, traiter et analyser ces donn√©es.

---

## üìä Donn√©es Fournies

Vous disposerez de 3 datasets :

### 1. **transactions.csv**
Format : `transaction_id,user_id,product_id,quantity,amount,timestamp,region`

Exemple :
```
TXN001,U1001,P5001,2,49.99,2025-01-15 10:23:45,EU
TXN002,U1002,P5002,1,129.99,2025-01-15 11:34:12,NA
TXN003,U1001,P5003,3,24.99,2025-01-15 12:45:33,EU
```

### 2. **products.csv**
Format : `product_id,product_name,category,price,stock`

Exemple :
```
P5001,Wireless Mouse,Electronics,24.99,150
P5002,Gaming Keyboard,Electronics,129.99,80
P5003,USB Cable,Accessories,7.99,500
```

### 3. **users.csv**
Format : `user_id,username,email,registration_date,country`

Exemple :
```
U1001,alice_smith,alice@email.com,2024-03-12,France
U1002,bob_jones,bob@email.com,2024-05-20,USA
U1003,charlie_brown,charlie@email.com,2024-07-08,Germany
```

---

## üöÄ Partie 1 : Pr√©paration de l'Environnement (1h30)

### T√¢che 1.1 : Installation et Configuration de Hadoop

**Objectif :** Mettre en place un cluster Hadoop fonctionnel en mode pseudo-distribu√©.

**√âtapes √† r√©aliser :**

1. Installer Java (OpenJDK 8 ou 11)
2. T√©l√©charger et installer Hadoop 3.3.6
3. Configurer les fichiers suivants :
   - `hadoop-env.sh`
   - `core-site.xml`
   - `hdfs-site.xml`
   - `mapred-site.xml`
   - `yarn-site.xml`
4. Formater le NameNode
5. D√©marrer les services HDFS et YARN

**Crit√®res de validation :**
- ‚úÖ La commande `jps` affiche : NameNode, DataNode, SecondaryNameNode, ResourceManager, NodeManager
- ‚úÖ L'interface web du NameNode est accessible sur http://localhost:9870
- ‚úÖ L'interface web YARN est accessible sur http://localhost:8088
- ‚úÖ La commande `hdfs dfsadmin -report` affiche le statut du cluster

### T√¢che 1.2 : G√©n√©ration des Donn√©es de Test

**Objectif :** Cr√©er des jeux de donn√©es r√©alistes pour tester le pipeline.

**√Ä faire :**

1. Cr√©er un script Python ou Shell pour g√©n√©rer :
   - 10 000 transactions
   - 100 produits
   - 500 utilisateurs
2. Les donn√©es doivent respecter les formats indiqu√©s ci-dessus
3. G√©n√©rer des donn√©es coh√©rentes (les IDs doivent correspondre entre les fichiers)

**Indications :**
- Utilisez des librairies comme `faker` (Python) pour g√©n√©rer des donn√©es r√©alistes
- R√©partissez les transactions sur 30 jours
- Variez les r√©gions : EU, NA, AS, SA
- Cr√©ez au moins 5 cat√©gories de produits diff√©rentes

**Crit√®res de validation :**
- ‚úÖ 3 fichiers CSV cr√©√©s : `transactions.csv`, `products.csv`, `users.csv`
- ‚úÖ Les donn√©es sont coh√©rentes (pas de product_id ou user_id inexistant dans transactions)
- ‚úÖ Les timestamps sont r√©partis sur une p√©riode d'un mois
- ‚úÖ Les fichiers ont au moins les quantit√©s mentionn√©es

---

## üìÇ Partie 2 : Ingestion des Donn√©es dans HDFS (1h)

### T√¢che 2.1 : Cr√©ation de l'Architecture de R√©pertoires

**Objectif :** Organiser les donn√©es dans HDFS de mani√®re structur√©e.

**√Ä faire :**

Cr√©er l'arborescence suivante dans HDFS :

```
/user/datamart/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ transactions/
‚îÇ   ‚îú‚îÄ‚îÄ products/
‚îÇ   ‚îî‚îÄ‚îÄ users/
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ (sera utilis√© plus tard)
‚îî‚îÄ‚îÄ analytics/
    ‚îî‚îÄ‚îÄ (sera utilis√© plus tard)
```

**Commandes √† utiliser :** `hdfs dfs -mkdir`

**Crit√®res de validation :**
- ‚úÖ L'arborescence est cr√©√©e correctement
- ‚úÖ La commande `hdfs dfs -ls -R /user/datamart/` affiche tous les r√©pertoires

### T√¢che 2.2 : Chargement des Donn√©es dans HDFS

**Objectif :** Copier les fichiers CSV depuis le syst√®me local vers HDFS.

**√Ä faire :**

1. Copier `transactions.csv` dans `/user/datamart/raw/transactions/`
2. Copier `products.csv` dans `/user/datamart/raw/products/`
3. Copier `users.csv` dans `/user/datamart/raw/users/`

**Commandes √† utiliser :** `hdfs dfs -put`

**Crit√®res de validation :**
- ‚úÖ Les 3 fichiers sont pr√©sents dans HDFS
- ‚úÖ La commande `hdfs dfs -ls /user/datamart/raw/transactions/` affiche le fichier
- ‚úÖ La commande `hdfs dfs -cat /user/datamart/raw/transactions/transactions.csv | head -5` affiche les 5 premi√®res lignes

### T√¢che 2.3 : V√©rification et Statistiques

**Objectif :** S'assurer que les donn√©es sont bien charg√©es.

**√Ä faire :**

1. Afficher le nombre de lignes de chaque fichier
2. V√©rifier la taille totale des donn√©es dans HDFS
3. Afficher le facteur de r√©plication

**Commandes utiles :**
```bash
hdfs dfs -cat <fichier> | wc -l
hdfs dfs -du -h <r√©pertoire>
hdfs dfs -stat %r <fichier>
```

**Crit√®res de validation :**
- ‚úÖ Le nombre de lignes correspond aux donn√©es g√©n√©r√©es
- ‚úÖ La taille des fichiers est coh√©rente
- ‚úÖ Le facteur de r√©plication est 1 (mode pseudo-distribu√©)

---

## ‚öôÔ∏è Partie 3 : Traitement MapReduce (2h30)

### T√¢che 3.1 : Job MapReduce - Chiffre d'Affaires par R√©gion

**Objectif :** Calculer le chiffre d'affaires total par r√©gion.

**√Ä r√©aliser :**

√âcrire un job MapReduce en Java qui :
- Lit le fichier `transactions.csv`
- Calcule le chiffre d'affaires total (somme des montants) par r√©gion
- Stocke le r√©sultat dans `/user/datamart/processed/revenue_by_region/`

**Structure du Mapper :**
- Input : `transaction_id,user_id,product_id,quantity,amount,timestamp,region`
- Output : `<region, amount>`

**Structure du Reducer :**
- Input : `<region, [amount1, amount2, ...]>`
- Output : `<region, total_revenue>`

**Indications :**
- Utilisez `LongWritable` pour les cl√©s d'entr√©e
- Utilisez `Text` pour les r√©gions
- Utilisez `DoubleWritable` pour les montants
- N'oubliez pas le Combiner pour optimiser

**Crit√®res de validation :**
- ‚úÖ Le code Java compile sans erreur
- ‚úÖ Le JAR est cr√©√© avec succ√®s
- ‚úÖ Le job s'ex√©cute sans erreur via `hadoop jar`
- ‚úÖ Le r√©sultat est stock√© dans le r√©pertoire HDFS sp√©cifi√©
- ‚úÖ Les revenus par r√©gion sont corrects

### T√¢che 3.2 : Job MapReduce - Top 10 Produits les Plus Vendus

**Objectif :** Identifier les 10 produits ayant g√©n√©r√© le plus de ventes (en quantit√©).

**√Ä r√©aliser :**

√âcrire un job MapReduce qui :
- Lit `transactions.csv`
- Calcule la quantit√© totale vendue par `product_id`
- Trie les r√©sultats et garde uniquement le top 10
- Stocke dans `/user/datamart/processed/top_products/`

**Indications :**
- Utilisez un second job MapReduce pour trier (ou utilisez un seul reducer)
- Pour le top N, vous pouvez utiliser un TreeMap dans le Reducer

**Crit√®res de validation :**
- ‚úÖ Le job s'ex√©cute avec succ√®s
- ‚úÖ Le r√©sultat contient exactement 10 produits
- ‚úÖ Les produits sont tri√©s par quantit√© d√©croissante
- ‚úÖ Les quantit√©s sont correctes

### T√¢che 3.3 : Job MapReduce - Analyse Temporelle

**Objectif :** Analyser le nombre de transactions par jour.

**√Ä r√©aliser :**

Cr√©er un job MapReduce qui :
- Lit `transactions.csv`
- Extrait la date du timestamp (format : YYYY-MM-DD)
- Compte le nombre de transactions par jour
- Stocke dans `/user/datamart/processed/daily_transactions/`

**Indications :**
- Utilisez `SimpleDateFormat` pour parser les timestamps
- Map output : `<date, 1>`
- Reduce output : `<date, count>`

**Crit√®res de validation :**
- ‚úÖ Le job fonctionne correctement
- ‚úÖ Les dates sont au bon format
- ‚úÖ Le comptage est exact
- ‚úÖ Les r√©sultats sont tri√©s par date

---

## üêù Partie 4 : Analyse avec Apache Hive (2h)

### T√¢che 4.1 : Installation et Configuration de Hive

**Objectif :** Installer Apache Hive sur votre environnement Hadoop.

**√Ä faire :**

1. T√©l√©charger Apache Hive 3.1.3
2. Configurer les variables d'environnement
3. Configurer `hive-site.xml` avec un metastore Derby
4. Initialiser le sch√©ma du metastore
5. D√©marrer Hive CLI ou Beeline

**Commandes cl√©s :**
```bash
schematool -dbType derby -initSchema
hive
```

**Crit√®res de validation :**
- ‚úÖ Hive est install√© et configur√©
- ‚úÖ `hive --version` affiche la version
- ‚úÖ Le shell Hive d√©marre sans erreur

### T√¢che 4.2 : Cr√©ation des Tables Hive

**Objectif :** Cr√©er des tables Hive externes pointant vers les donn√©es HDFS.

**√Ä faire :**

Cr√©er 3 tables externes :

1. **transactions** : pointant vers `/user/datamart/raw/transactions/`
2. **products** : pointant vers `/user/datamart/raw/products/`
3. **users** : pointant vers `/user/datamart/raw/users/`

**Indications :**
- Utilisez `CREATE EXTERNAL TABLE`
- Sp√©cifiez `ROW FORMAT DELIMITED FIELDS TERMINATED BY ','`
- Ignorez les en-t√™tes avec `tblproperties ("skip.header.line.count"="1")`

**Exemple de structure pour transactions :**
```sql
CREATE EXTERNAL TABLE transactions (
    transaction_id STRING,
    user_id STRING,
    product_id STRING,
    quantity INT,
    amount DOUBLE,
    transaction_timestamp STRING,
    region STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/datamart/raw/transactions/'
TBLPROPERTIES ("skip.header.line.count"="1");
```

**Crit√®res de validation :**
- ‚úÖ Les 3 tables sont cr√©√©es
- ‚úÖ `SHOW TABLES;` affiche les 3 tables
- ‚úÖ `SELECT * FROM transactions LIMIT 5;` retourne des donn√©es
- ‚úÖ `SELECT COUNT(*) FROM transactions;` retourne le bon nombre

### T√¢che 4.3 : Requ√™tes Analytiques avec HiveQL

**Objectif :** Effectuer des analyses business avec SQL.

**Requ√™tes √† √©crire :**

#### Requ√™te 1 : Chiffre d'Affaires Total
Calculer le chiffre d'affaires total de l'entreprise.

```sql
-- Votre requ√™te ici
```

#### Requ√™te 2 : Chiffre d'Affaires par Cat√©gorie
Calculer le CA par cat√©gorie de produit (n√©cessite une jointure).

```sql
-- Votre requ√™te ici
```

#### Requ√™te 3 : Top 5 Clients (par montant d√©pens√©)
Identifier les 5 clients ayant d√©pens√© le plus.

```sql
-- Votre requ√™te ici
```

#### Requ√™te 4 : Panier Moyen par R√©gion
Calculer le montant moyen des transactions par r√©gion.

```sql
-- Votre requ√™te ici
```

#### Requ√™te 5 : Analyse Temporelle
Compter le nombre de transactions par jour et trier par date.

```sql
-- Votre requ√™te ici
-- Indice : utilisez substr() ou to_date() pour extraire la date
```

#### Requ√™te 6 : Produits Jamais Vendus
Lister les produits qui n'ont jamais √©t√© vendus (LEFT JOIN).

```sql
-- Votre requ√™te ici
```

**Crit√®res de validation :**
- ‚úÖ Toutes les requ√™tes s'ex√©cutent sans erreur
- ‚úÖ Les r√©sultats sont coh√©rents avec les donn√©es
- ‚úÖ Les jointures produisent les bons r√©sultats
- ‚úÖ Les agr√©gations sont correctes

### T√¢che 4.4 : Optimisation avec Partitionnement

**Objectif :** Am√©liorer les performances en partitionnant les donn√©es.

**√Ä faire :**

1. Cr√©er une table `transactions_partitioned` partitionn√©e par `region`
2. Utiliser le format de stockage **Parquet** (plus performant que CSV)
3. Charger les donn√©es depuis la table `transactions` vers `transactions_partitioned`

**Structure attendue :**
```sql
CREATE TABLE transactions_partitioned (
    transaction_id STRING,
    user_id STRING,
    product_id STRING,
    quantity INT,
    amount DOUBLE,
    transaction_timestamp STRING
)
PARTITIONED BY (region STRING)
STORED AS PARQUET;
```

**Chargement des donn√©es :**
```sql
SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT INTO TABLE transactions_partitioned PARTITION(region)
SELECT transaction_id, user_id, product_id, quantity, amount, transaction_timestamp, region
FROM transactions;
```

**Requ√™te √† tester :**
Comparer les performances d'une requ√™te sur la table normale vs partitionn√©e.

```sql
-- Table normale
SELECT SUM(amount) FROM transactions WHERE region = 'EU';

-- Table partitionn√©e
SELECT SUM(amount) FROM transactions_partitioned WHERE region = 'EU';
```

**Crit√®res de validation :**
- ‚úÖ La table partitionn√©e est cr√©√©e
- ‚úÖ Les donn√©es sont charg√©es dans les partitions
- ‚úÖ `SHOW PARTITIONS transactions_partitioned;` affiche les partitions
- ‚úÖ La requ√™te sur la table partitionn√©e est plus rapide (visible dans les logs Hive)

---

## üîÑ Partie 5 : Pipeline Complet avec Sqoop (Bonus - 1h)

**Objectif :** Importer des donn√©es depuis une base MySQL vers Hadoop.

### T√¢che 5.1 : Configuration de MySQL

**√Ä faire :**

1. Installer MySQL ou MariaDB
2. Cr√©er une base de donn√©es `ecommerce_source`
3. Cr√©er une table `customers` avec des donn√©es fictives :
   ```sql
   CREATE TABLE customers (
       customer_id INT PRIMARY KEY,
       first_name VARCHAR(50),
       last_name VARCHAR(50),
       email VARCHAR(100),
       city VARCHAR(50),
       country VARCHAR(50)
   );
   ```
4. Ins√©rer 100 enregistrements de test

### T√¢che 5.2 : Import avec Sqoop

**√Ä faire :**

Utiliser Sqoop pour importer la table `customers` vers HDFS :
- Destination : `/user/datamart/raw/customers/`
- Format de sortie : Parquet
- Import incr√©mental si possible

**Commande de base :**
```bash
sqoop import \
  --connect jdbc:mysql://localhost/ecommerce_source \
  --username <user> \
  --password <password> \
  --table customers \
  --target-dir /user/datamart/raw/customers \
  --as-parquetfile \
  -m 1
```

**Crit√®res de validation :**
- ‚úÖ Les donn√©es sont import√©es dans HDFS
- ‚úÖ Le format est bien Parquet
- ‚úÖ Toutes les lignes sont pr√©sentes

### T√¢che 5.3 : Cr√©ation d'une Table Hive sur les Donn√©es Import√©es

Cr√©er une table externe Hive pointant vers les donn√©es Parquet import√©es.

**Crit√®res de validation :**
- ‚úÖ La table Hive fonctionne
- ‚úÖ Les requ√™tes SELECT retournent les bonnes donn√©es

---

## üì§ Livrables

√Ä la fin du brief, vous devez fournir :

### 1. **Code Source**
- Tous les jobs MapReduce (fichiers .java)
- Scripts de g√©n√©ration de donn√©es
- Fichiers SQL Hive (toutes vos requ√™tes)

### 2. **Fichiers de Configuration**
- `core-site.xml`
- `hdfs-site.xml`
- `mapred-site.xml`
- `yarn-site.xml`
- `hive-site.xml`

### 3. **Documentation**
Un fichier README.md contenant :
- Instructions de d√©ploiement du pipeline
- Architecture du pipeline (sch√©ma)
- Description de chaque job MapReduce
- R√©sultats des analyses Hive
- Probl√®mes rencontr√©s et solutions
- Pistes d'am√©lioration

### 4. **Captures d'√âcran**
- Interface web NameNode (montrant HDFS)
- Interface web YARN (montrant les jobs ex√©cut√©s)
- R√©sultats de vos requ√™tes Hive
- Arborescence HDFS compl√®te

---

## ‚úÖ Crit√®res d'√âvaluation

| Crit√®re | Points | Description |
|---------|--------|-------------|
| **Installation Hadoop** | 10 | Cluster fonctionnel avec HDFS et YARN |
| **Ingestion HDFS** | 10 | Donn√©es correctement organis√©es dans HDFS |
| **Jobs MapReduce** | 30 | 3 jobs fonctionnels et optimis√©s |
| **Analyse Hive** | 25 | Toutes les requ√™tes SQL fonctionnent |
| **Optimisation** | 10 | Partitionnement et format Parquet |
| **Documentation** | 10 | README clair et complet |
| **Bonus Sqoop** | 5 | Import MySQL ‚Üí HDFS fonctionnel |
| **Total** | **100** | |

---

## üí° Conseils

### D√©bogage
- Consultez TOUJOURS les logs en cas d'erreur :
  - Logs Hadoop : `$HADOOP_HOME/logs/`
  - Logs YARN : Interface web port 8088
  - Logs Hive : `/tmp/<user>/hive.log`

### Performance
- Utilisez des Combiners dans MapReduce pour r√©duire le shuffle
- Pr√©f√©rez le format Parquet √† CSV pour les grandes donn√©es
- Partitionnez vos tables Hive sur les colonnes fr√©quemment filtr√©es

### Organisation
- Cr√©ez un repository Git pour versionner votre code
- Testez chaque √©tape avant de passer √† la suivante
- Documentez au fur et √† mesure

### Ressources
- Documentation officielle Hadoop : https://hadoop.apache.org/docs/
- Documentation Hive : https://cwiki.apache.org/confluence/display/Hive/
- Stack Overflow pour les erreurs courantes

---

## üéì Comp√©tences D√©velopp√©es

√Ä l'issue de ce brief, vous aurez pratiqu√© :

‚úÖ Installation et configuration d'un cluster Hadoop
‚úÖ Manipulation de HDFS (commandes CLI)
‚úÖ D√©veloppement de jobs MapReduce en Java
‚úÖ Analyse de donn√©es avec Hive (HiveQL)
‚úÖ Optimisation de requ√™tes Big Data (partitionnement, formats de fichiers)
‚úÖ Int√©gration de donn√©es avec Sqoop
‚úÖ Documentation technique

---

## üìû Support

En cas de blocage :
1. Consultez la documentation du cours (Parties 1-6)
2. V√©rifiez les logs d'erreur
3. Recherchez l'erreur sur Stack Overflow
4. Contactez le formateur si le probl√®me persiste

---

**Bon courage et bon d√©veloppement ! üöÄ**

---

*Brief cr√©√© pour la formation Data Engineering - Simplon - 2025*
