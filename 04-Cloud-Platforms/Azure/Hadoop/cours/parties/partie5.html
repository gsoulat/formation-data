<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Partie 5 : Ã‰cosystÃ¨me Hadoop - Formation Hadoop</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

    <link rel="stylesheet" href="../assets/styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸŒ Partie 5 : Ã‰cosystÃ¨me Hadoop</h1>
            <p class="subtitle">DÃ©couvrir les outils complÃ©mentaires de Hadoop</p>
            <div class="duration">â±ï¸ DurÃ©e : 3h</div>
        </header>

        <nav class="nav-menu">
            <ul>
                <li><a href="../index.html">ğŸ  Accueil</a></li>
                <li><a href="partie4.html">â† Partie 4</a></li>
                <li><a href="partie5.html" class="active">Partie 5</a></li>
                <li><a href="partie6.html">Partie 6 â†’</a></li>
            </ul>
        </nav>

        <div class="content">
            <div class="objectives">
                <h2>ğŸ¯ Objectifs d'Apprentissage</h2>
                <ul>
                    <li>DÃ©couvrir Apache Hive pour requÃªter avec SQL</li>
                    <li>Utiliser Apache Pig pour le scripting de donnÃ©es</li>
                    <li>Comprendre HBase comme base NoSQL</li>
                    <li>MaÃ®triser Sqoop pour l'import/export de donnÃ©es</li>
                    <li>Explorer Flume pour la collecte de logs</li>
                </ul>
            </div>

            <section class="section">
                <h2>ğŸ—ºï¸ 1. Vue d'Ensemble de l'Ã‰cosystÃ¨me</h2>

                <div class="workflow-diagram">
                    <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Ã‰COSYSTÃˆME HADOOP                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  INGESTION          STOCKAGE        TRAITEMENT    ANALYSE  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€   â”‚
â”‚  Flume              HDFS            MapReduce     Hive     â”‚
â”‚  Sqoop              HBase           Spark         Pig      â”‚
â”‚  Kafka                              Tez           Impala   â”‚
â”‚                                                             â”‚
â”‚  ORCHESTRATION      COORDINATION    MONITORING             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  Oozie              ZooKeeper       Ambari                 â”‚
â”‚                                     Ganglia                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    </pre>
                </div>

                <table>
                    <thead>
                        <tr>
                            <th>Outil</th>
                            <th>CatÃ©gorie</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Hive</strong></td>
                            <td>SQL/Analyse</td>
                            <td>EntrepÃ´t de donnÃ©es avec interface SQL (HiveQL)</td>
                        </tr>
                        <tr>
                            <td><strong>Pig</strong></td>
                            <td>Scripting</td>
                            <td>Langage de haut niveau pour flux de donnÃ©es</td>
                        </tr>
                        <tr>
                            <td><strong>HBase</strong></td>
                            <td>Base NoSQL</td>
                            <td>Base de donnÃ©es distribuÃ©e orientÃ©e colonnes</td>
                        </tr>
                        <tr>
                            <td><strong>Sqoop</strong></td>
                            <td>Import/Export</td>
                            <td>Transfert entre Hadoop et SGBD relationnels</td>
                        </tr>
                        <tr>
                            <td><strong>Flume</strong></td>
                            <td>Ingestion</td>
                            <td>Collecte et agrÃ©gation de logs streaming</td>
                        </tr>
                        <tr>
                            <td><strong>Oozie</strong></td>
                            <td>Orchestration</td>
                            <td>Planificateur de workflows Hadoop</td>
                        </tr>
                        <tr>
                            <td><strong>ZooKeeper</strong></td>
                            <td>Coordination</td>
                            <td>Service de coordination distribuÃ©e</td>
                        </tr>
                        <tr>
                            <td><strong>Spark</strong></td>
                            <td>Traitement</td>
                            <td>Moteur de traitement in-memory rapide</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section class="section">
                <h2>ğŸ 2. Apache Hive : SQL sur Hadoop</h2>

                <h3>Qu'est-ce que Hive ?</h3>
                <p>
                    <strong>Apache Hive</strong> est un entrepÃ´t de donnÃ©es construit sur Hadoop qui permet d'interroger
                    et d'analyser de grandes quantitÃ©s de donnÃ©es avec <strong>HiveQL</strong>, un langage similaire Ã  SQL.
                </p>

                <div class="grid">
                    <div class="grid-item">
                        <h4>âœ… Avantages</h4>
                        <ul>
                            <li>Syntaxe SQL familiÃ¨re</li>
                            <li>Pas besoin d'Ã©crire du MapReduce</li>
                            <li>Support de gros volumes</li>
                            <li>Optimiseur de requÃªtes</li>
                        </ul>
                    </div>
                    <div class="grid-item">
                        <h4>âŒ InconvÃ©nients</h4>
                        <ul>
                            <li>Latence Ã©levÃ©e (batch, pas temps rÃ©el)</li>
                            <li>Pas de modification en place (INSERT only)</li>
                            <li>Pas idÃ©al pour les petites requÃªtes</li>
                        </ul>
                    </div>
                </div>

                <h3>Architecture Hive</h3>
                <div class="workflow-diagram">
                    <pre>
Client (Hive CLI / Beeline / JDBC)
         â†“
    Hive Server 2
         â†“
    Metastore (MySQL/PostgreSQL) â† Stocke les schÃ©mas
         â†“
    Driver (Query Compiler, Optimizer, Executor)
         â†“
    Execution Engine (MapReduce / Tez / Spark)
         â†“
    HDFS (DonnÃ©es)
                    </pre>
                </div>

                <h3>Exemple de RequÃªtes HiveQL</h3>

                <h4>CrÃ©er une Table</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">CREATE TABLE IF NOT EXISTS employees (
    id INT,
    name STRING,
    department STRING,
    salary DOUBLE
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;</code></pre>
                </div>

                <h4>Charger des DonnÃ©es</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">-- Charger depuis un fichier local
LOAD DATA LOCAL INPATH '/tmp/employees.csv' INTO TABLE employees;

-- Charger depuis HDFS
LOAD DATA INPATH '/user/data/employees.csv' INTO TABLE employees;</code></pre>
                </div>

                <h4>RequÃªtes SELECT</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">-- SÃ©lection simple
SELECT * FROM employees WHERE salary > 50000;

-- AgrÃ©gation
SELECT department, AVG(salary) as avg_salary
FROM employees
GROUP BY department
HAVING avg_salary > 60000;

-- Jointure
SELECT e.name, d.department_name
FROM employees e
JOIN departments d ON e.department = d.dept_id;</code></pre>
                </div>

                <h4>Partitionnement</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">-- Table partitionnÃ©e par annÃ©e
CREATE TABLE logs (
    timestamp STRING,
    level STRING,
    message STRING
)
PARTITIONED BY (year INT, month INT)
STORED AS PARQUET;

-- InsÃ©rer dans une partition
INSERT INTO TABLE logs PARTITION (year=2025, month=1)
SELECT timestamp, level, message FROM raw_logs
WHERE year(timestamp) = 2025 AND month(timestamp) = 1;</code></pre>
                </div>

                <div class="alert alert-success">
                    <h4>Bonnes Pratiques Hive</h4>
                    <ul>
                        <li>Utiliser le partitionnement pour les grandes tables</li>
                        <li>PrÃ©fÃ©rer les formats colonnaires (Parquet, ORC) pour les performances</li>
                        <li>Utiliser Tez ou Spark au lieu de MapReduce comme moteur</li>
                        <li>Analyser les tables pour optimiser les requÃªtes (<code>ANALYZE TABLE</code>)</li>
                    </ul>
                </div>
            </section>

            <section class="section">
                <h2>ğŸ· 3. Apache Pig : Scripting de DonnÃ©es</h2>

                <h3>Qu'est-ce que Pig ?</h3>
                <p>
                    <strong>Apache Pig</strong> est une plateforme pour analyser de grandes donnÃ©es avec
                    <strong>Pig Latin</strong>, un langage de haut niveau orientÃ© flux de donnÃ©es.
                </p>

                <div class="alert alert-info">
                    <h4>Quand utiliser Pig plutÃ´t que Hive ?</h4>
                    <ul>
                        <li><strong>Pig :</strong> Transformations complexes, ETL, traitement semi-structurÃ©</li>
                        <li><strong>Hive :</strong> RequÃªtes SQL classiques, reporting, analyses SQL</li>
                    </ul>
                </div>

                <h3>Exemple de Script Pig Latin</h3>

                <h4>WordCount en Pig</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-pig">-- Charger les donnÃ©es
lines = LOAD '/user/data/input.txt' AS (line:chararray);

-- DÃ©couper en mots
words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;

-- Grouper par mot
grouped = GROUP words BY word;

-- Compter
word_counts = FOREACH grouped GENERATE group AS word, COUNT(words) AS count;

-- Trier par compte dÃ©croissant
sorted = ORDER word_counts BY count DESC;

-- Stocker le rÃ©sultat
STORE sorted INTO '/user/data/output' USING PigStorage(',');</code></pre>
                </div>

                <h4>Filtrage et Transformation</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-pig">-- Charger les logs
logs = LOAD '/logs/access.log' USING PigStorage(' ')
       AS (ip:chararray, timestamp:chararray, method:chararray,
           url:chararray, status:int, bytes:int);

-- Filtrer les erreurs 404
errors = FILTER logs BY status == 404;

-- Extraire l'IP et l'URL
result = FOREACH errors GENERATE ip, url;

-- Grouper par URL
grouped = GROUP result BY url;

-- Compter les erreurs par URL
error_counts = FOREACH grouped GENERATE group AS url, COUNT(result) AS count;

-- Stocker
STORE error_counts INTO '/output/404_errors';</code></pre>
                </div>

                <h4>Jointure en Pig</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-pig">-- Charger deux datasets
users = LOAD '/data/users' AS (user_id:int, name:chararray);
orders = LOAD '/data/orders' AS (order_id:int, user_id:int, amount:double);

-- Jointure
joined = JOIN users BY user_id, orders BY user_id;

-- Stocker
STORE joined INTO '/output/user_orders';</code></pre>
                </div>

                <h3>ExÃ©cuter un Script Pig</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Mode local (pour tester)
pig -x local script.pig

# Mode MapReduce
pig -x mapreduce script.pig

# Mode interactif (Grunt shell)
pig</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>ğŸ—„ï¸ 4. Apache HBase : Base de DonnÃ©es NoSQL</h2>

                <h3>Qu'est-ce que HBase ?</h3>
                <p>
                    <strong>Apache HBase</strong> est une base de donnÃ©es NoSQL distribuÃ©e, orientÃ©e colonnes,
                    construite sur HDFS. InspirÃ©e de Google BigTable.
                </p>

                <h3>CaractÃ©ristiques</h3>
                <div class="grid">
                    <div class="grid-item">
                        <h4>ğŸ“ˆ ScalabilitÃ©</h4>
                        <p>Supporte des milliards de lignes et millions de colonnes</p>
                    </div>
                    <div class="grid-item">
                        <h4>âš¡ AccÃ¨s Rapide</h4>
                        <p>Lecture/Ã©criture en temps rÃ©el (millisecondes)</p>
                    </div>
                    <div class="grid-item">
                        <h4>ğŸ”‘ ClÃ©-Valeur</h4>
                        <p>AccÃ¨s par clÃ© primaire (row key)</p>
                    </div>
                    <div class="grid-item">
                        <h4>ğŸ“Š OrientÃ© Colonnes</h4>
                        <p>Stockage en familles de colonnes</p>
                    </div>
                </div>

                <h3>ModÃ¨le de DonnÃ©es</h3>
                <div class="workflow-diagram">
                    <pre>
Table
  â”œâ”€ Row Key (clÃ© unique)
  â””â”€ Column Families
       â”œâ”€ Family 1
       â”‚    â”œâ”€ Column 1:1 (avec timestamp)
       â”‚    â””â”€ Column 1:2
       â””â”€ Family 2
            â””â”€ Column 2:1
                    </pre>
                </div>

                <h3>Commandes HBase Shell</h3>

                <h4>CrÃ©er et GÃ©rer des Tables</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Lancer HBase shell
hbase shell

# CrÃ©er une table avec deux familles de colonnes
create 'users', 'profile', 'contacts'

# Lister les tables
list

# DÃ©crire une table
describe 'users'

# DÃ©sactiver et supprimer une table
disable 'users'
drop 'users'</code></pre>
                </div>

                <h4>InsÃ©rer et Lire des DonnÃ©es</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># InsÃ©rer des donnÃ©es
put 'users', 'user1', 'profile:name', 'Alice'
put 'users', 'user1', 'profile:age', '30'
put 'users', 'user1', 'contacts:email', 'alice@example.com'

# Lire une ligne
get 'users', 'user1'

# Lire une colonne spÃ©cifique
get 'users', 'user1', 'profile:name'

# Scanner toute la table
scan 'users'

# Scanner avec filtre
scan 'users', {FILTER => "ValueFilter(=, 'binary:Alice')"}

# Supprimer une cellule
delete 'users', 'user1', 'profile:age'</code></pre>
                </div>

                <h3>Cas d'Usage HBase</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Domaine</th>
                            <th>Cas d'Usage</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>RÃ©seaux Sociaux</td>
                            <td>Profils utilisateurs, fils d'actualitÃ©, messages</td>
                        </tr>
                        <tr>
                            <td>IoT</td>
                            <td>Stockage de sÃ©ries temporelles de capteurs</td>
                        </tr>
                        <tr>
                            <td>Finance</td>
                            <td>Historique de transactions en temps rÃ©el</td>
                        </tr>
                        <tr>
                            <td>E-commerce</td>
                            <td>Catalogue produits, historique des commandes</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section class="section">
                <h2>ğŸ”„ 5. Apache Sqoop : Import/Export de DonnÃ©es</h2>

                <h3>Qu'est-ce que Sqoop ?</h3>
                <p>
                    <strong>Apache Sqoop</strong> (SQL to Hadoop) est un outil pour transfÃ©rer des donnÃ©es
                    entre Hadoop et des bases de donnÃ©es relationnelles (MySQL, PostgreSQL, Oracle, etc.).
                </p>

                <h3>Commandes Principales</h3>

                <h4>Import depuis SGBD vers HDFS</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Importer une table entiÃ¨re
sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --target-dir /user/hadoop/employees

# Importer avec une requÃªte WHERE
sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --where "department = 'IT'" \
  --target-dir /user/hadoop/it_employees

# Importer toutes les tables d'une base
sqoop import-all-tables \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --warehouse-dir /user/hadoop/warehouse</code></pre>
                </div>

                <h4>Export depuis HDFS vers SGBD</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Exporter vers une table MySQL
sqoop export \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees_export \
  --export-dir /user/hadoop/employees_processed</code></pre>
                </div>

                <h4>Import IncrÃ©mental</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Import incrÃ©mental basÃ© sur une colonne d'ID
sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table orders \
  --incremental append \
  --check-column order_id \
  --last-value 1000 \
  --target-dir /user/hadoop/orders</code></pre>
                </div>

                <h4>Import Direct vers Hive</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash">sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --hive-import \
  --hive-table employees \
  --create-hive-table</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>ğŸ“¡ 6. Apache Flume : Collecte de Logs</h2>

                <h3>Qu'est-ce que Flume ?</h3>
                <p>
                    <strong>Apache Flume</strong> est un service distribuÃ© pour collecter, agrÃ©ger et dÃ©placer
                    efficacement de grandes quantitÃ©s de donnÃ©es de logs vers HDFS.
                </p>

                <h3>Architecture Flume</h3>
                <div class="workflow-diagram">
                    <pre>
Source â†’ Channel â†’ Sink

Exemples:
- Source : Netcat, Exec, Avro, Kafka
- Channel : Memory, File, JDBC
- Sink : HDFS, HBase, Logger, Avro
                    </pre>
                </div>

                <h3>Exemple de Configuration</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-properties"># flume-conf.properties

# DÃ©finir les composants
agent1.sources = source1
agent1.channels = channel1
agent1.sinks = sink1

# Configurer la source (Ã©coute sur un port)
agent1.sources.source1.type = netcat
agent1.sources.source1.bind = localhost
agent1.sources.source1.port = 44444

# Configurer le channel (mÃ©moire)
agent1.channels.channel1.type = memory
agent1.channels.channel1.capacity = 1000
agent1.channels.channel1.transactionCapacity = 100

# Configurer le sink (HDFS)
agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path = /user/flume/events/%Y-%m-%d
agent1.sinks.sink1.hdfs.fileType = DataStream
agent1.sinks.sink1.hdfs.rollInterval = 60
agent1.sinks.sink1.hdfs.rollSize = 0
agent1.sinks.sink1.hdfs.rollCount = 0

# Relier les composants
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1</code></pre>
                </div>

                <h4>Lancer Flume</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash">flume-ng agent \
  --conf-file flume-conf.properties \
  --name agent1 \
  -Dflume.root.logger=INFO,console</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>ğŸ“ RÃ©sumÃ© de la Partie 5</h2>
                <div class="key-points">
                    <h3>Points ClÃ©s Ã  Retenir</h3>
                    <ul>
                        <li><strong>Hive</strong> : SQL sur Hadoop, idÃ©al pour analyses et reporting</li>
                        <li><strong>Pig</strong> : Scripting pour ETL et transformations complexes</li>
                        <li><strong>HBase</strong> : Base NoSQL temps rÃ©el sur HDFS</li>
                        <li><strong>Sqoop</strong> : Import/Export entre Hadoop et SGBD relationnels</li>
                        <li><strong>Flume</strong> : Ingestion de logs en streaming vers HDFS</li>
                        <li>L'Ã©cosystÃ¨me Hadoop est riche et chaque outil a son cas d'usage spÃ©cifique</li>
                        <li>Ces outils peuvent Ãªtre combinÃ©s pour crÃ©er des pipelines Big Data complets</li>
                    </ul>
                </div>
            </section>

            <div class="alert alert-success" style="margin-top: 40px;">
                <h4>âœ… PrÃªt pour la Suite ?</h4>
                <p>Vous connaissez maintenant l'Ã©cosystÃ¨me Hadoop ! Dans la derniÃ¨re partie, nous verrons comment <strong>installer et configurer</strong> un cluster Hadoop.</p>
            </div>
        </div>

        <footer>
            <p>&copy; 2025 Formation Hadoop - Data Engineering | Simplon</p>
            <p><a href="partie6.html">Partie 6 : Installation et Configuration â†’</a></p>
        </footer>
    </div>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-xml.min.js"></script>
</body>
</html>
