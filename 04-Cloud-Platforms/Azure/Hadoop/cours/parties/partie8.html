<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Partie 8 : Exercices Pratiques Guid√©s - Formation Hadoop</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

    <link rel="stylesheet" href="../assets/styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>üéØ Partie 8 : Exercices Pratiques Guid√©s</h1>
            <p class="subtitle">Mettre en pratique vos comp√©tences Hadoop √©tape par √©tape</p>
            <div class="duration">‚è±Ô∏è Dur√©e : 4h</div>
        </header>

        <nav class="nav-menu">
            <ul>
                <li><a href="../index.html">üè† Accueil</a></li>
                <li><a href="partie7.html">‚Üê Partie 7</a></li>
                <li><a href="partie8.html" class="active">Partie 8</a></li>
            </ul>
        </nav>

        <div class="content">
            <div class="objectives">
                <h2>üéØ Objectifs de cette Partie</h2>
                <ul>
                    <li>Manipuler HDFS avec des donn√©es r√©elles</li>
                    <li>Cr√©er des jobs MapReduce Python pas √† pas</li>
                    <li>Analyser des donn√©es avec Hive</li>
                    <li>R√©soudre des probl√®mes courants</li>
                </ul>
            </div>

            <section class="section">
                <h2>üìö Exercice 1 : Manipulation HDFS (30 min)</h2>

                <h3>üéØ Objectif</h3>
                <p>Ma√Ætriser les commandes HDFS en cr√©ant une structure de donn√©es pour un projet d'analyse de logs.</p>

                <h3>üìù Contexte</h3>
                <p>
                    Vous √™tes Data Engineer et devez organiser les logs d'une application web.
                    Les logs sont r√©partis en plusieurs cat√©gories : access, error, et performance.
                </p>

                <h3>‚úÖ √âtape 1 : Cr√©er l'arborescence</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Se connecter √† votre cluster Hadoop

# Cr√©er l'arborescence de base
hdfs dfs -mkdir -p /user/$USER/logs/raw/access
hdfs dfs -mkdir -p /user/$USER/logs/raw/error
hdfs dfs -mkdir -p /user/$USER/logs/raw/performance
hdfs dfs -mkdir -p /user/$USER/logs/processed
hdfs dfs -mkdir -p /user/$USER/logs/archive

# V√©rifier la structure
hdfs dfs -ls -R /user/$USER/logs</code></pre>
                </div>

                <div class="alert alert-success">
                    <h4>R√©sultat attendu</h4>
                    <p>Vous devriez voir une arborescence avec 5 r√©pertoires cr√©√©s.</p>
                </div>

                <h3>‚úÖ √âtape 2 : G√©n√©rer des fichiers de logs de test</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Cr√©er des logs d'acc√®s (access logs)
cat > access.log << 'EOF'
192.168.1.10 - - [15/Jan/2025:10:23:45] "GET /home HTTP/1.1" 200 1234
192.168.1.11 - - [15/Jan/2025:10:24:12] "GET /products HTTP/1.1" 200 5678
192.168.1.10 - - [15/Jan/2025:10:25:33] "POST /api/login HTTP/1.1" 200 890
192.168.1.12 - - [15/Jan/2025:10:26:45] "GET /about HTTP/1.1" 200 432
192.168.1.11 - - [15/Jan/2025:10:27:22] "GET /contact HTTP/1.1" 200 234
EOF

# Cr√©er des logs d'erreur (error logs)
cat > error.log << 'EOF'
[2025-01-15 10:30:15] ERROR Database connection failed: timeout
[2025-01-15 10:31:22] ERROR File not found: /data/config.xml
[2025-01-15 10:32:45] WARNING Slow query detected: SELECT * FROM users
[2025-01-15 10:33:10] ERROR Out of memory exception
[2025-01-15 10:34:55] ERROR API rate limit exceeded for user 12345
EOF

# Cr√©er des logs de performance
cat > performance.log << 'EOF'
2025-01-15 10:00:00,homepage,250ms
2025-01-15 10:05:00,search,1200ms
2025-01-15 10:10:00,checkout,850ms
2025-01-15 10:15:00,homepage,180ms
2025-01-15 10:20:00,api_call,3500ms
EOF

# Afficher un aper√ßu
echo "=== Access Log ==="
head -3 access.log
echo -e "\n=== Error Log ==="
head -3 error.log
echo -e "\n=== Performance Log ==="
head -3 performance.log</code></pre>
                </div>

                <h3>‚úÖ √âtape 3 : Charger les logs dans HDFS</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Copier les fichiers dans HDFS
hdfs dfs -put access.log /user/$USER/logs/raw/access/
hdfs dfs -put error.log /user/$USER/logs/raw/error/
hdfs dfs -put performance.log /user/$USER/logs/raw/performance/

# V√©rifier le chargement
hdfs dfs -ls /user/$USER/logs/raw/access/
hdfs dfs -ls /user/$USER/logs/raw/error/
hdfs dfs -ls /user/$USER/logs/raw/performance/

# Afficher le contenu d'un fichier
hdfs dfs -cat /user/$USER/logs/raw/access/access.log</code></pre>
                </div>

                <h3>‚úÖ √âtape 4 : Manipulations avanc√©es</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Compter le nombre de lignes dans access.log
hdfs dfs -cat /user/$USER/logs/raw/access/access.log | wc -l

# Chercher les erreurs de type "ERROR"
hdfs dfs -cat /user/$USER/logs/raw/error/error.log | grep "ERROR"

# Voir la taille des fichiers
hdfs dfs -du -h /user/$USER/logs/raw/

# Copier un fichier dans HDFS
hdfs dfs -cp /user/$USER/logs/raw/access/access.log /user/$USER/logs/archive/access_backup.log

# Changer les permissions
hdfs dfs -chmod 755 /user/$USER/logs/raw/access/access.log

# Voir les statistiques d'un fichier
hdfs dfs -stat "Taille: %b bytes, R√©plication: %r, Modifi√©: %y" /user/$USER/logs/raw/access/access.log</code></pre>
                </div>

                <div class="alert alert-info">
                    <h4>üìå Points √† retenir</h4>
                    <ul>
                        <li>Les commandes HDFS sont similaires aux commandes Linux</li>
                        <li>Toujours v√©rifier le r√©sultat avec <code>-ls</code></li>
                        <li>Les pipes Unix fonctionnent avec <code>hdfs dfs -cat</code></li>
                    </ul>
                </div>
            </section>

            <section class="section">
                <h2>üêç Exercice 2 : Job MapReduce - Analyse d'Access Logs (1h)</h2>

                <h3>üéØ Objectif</h3>
                <p>Cr√©er un job MapReduce Python pour compter le nombre de requ√™tes par adresse IP.</p>

                <h3>‚úÖ √âtape 1 : Cr√©er le Mapper</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">#!/usr/bin/env python3
# Fichier: ip_count_mapper.py
"""
Mapper pour compter les requ√™tes par IP
Extrait l'IP de chaque ligne de log et √©met (IP, 1)
"""
import sys
import re

def main():
    # Pattern pour extraire l'IP (d√©but de ligne)
    ip_pattern = re.compile(r'^(\d+\.\d+\.\d+\.\d+)')

    for line in sys.stdin:
        line = line.strip()

        # Extraire l'IP
        match = ip_pattern.match(line)
        if match:
            ip = match.group(1)
            # √âmettre (IP, 1)
            print(f"{ip}\t1")

if __name__ == "__main__":
    main()</code></pre>
                </div>

                <p><strong>üíæ Cr√©er le fichier :</strong></p>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Cr√©er le mapper
cat > ip_count_mapper.py << 'EOF'
[Copier le code Python ci-dessus]
EOF

chmod +x ip_count_mapper.py</code></pre>
                </div>

                <h3>‚úÖ √âtape 2 : Cr√©er le Reducer</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">#!/usr/bin/env python3
# Fichier: ip_count_reducer.py
"""
Reducer pour compter les requ√™tes par IP
Re√ßoit (IP, [1, 1, 1, ...]) et calcule le total
"""
import sys

def main():
    current_ip = None
    current_count = 0

    for line in sys.stdin:
        line = line.strip()

        try:
            ip, count = line.split('\t')
            count = int(count)
        except ValueError:
            continue

        if current_ip == ip:
            current_count += count
        else:
            if current_ip:
                print(f"{current_ip}\t{current_count}")
            current_ip = ip
            current_count = count

    # Dernier IP
    if current_ip:
        print(f"{current_ip}\t{current_count}")

if __name__ == "__main__":
    main()</code></pre>
                </div>

                <p><strong>üíæ Cr√©er le fichier :</strong></p>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash">cat > ip_count_reducer.py << 'EOF'
[Copier le code Python ci-dessus]
EOF

chmod +x ip_count_reducer.py</code></pre>
                </div>

                <h3>‚úÖ √âtape 3 : Tester en Local</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Test du mapper seul
cat access.log | ./ip_count_mapper.py

# R√©sultat attendu :
# 192.168.1.10    1
# 192.168.1.11    1
# 192.168.1.10    1
# 192.168.1.12    1
# 192.168.1.11    1

# Test complet (mapper + sort + reducer)
cat access.log | ./ip_count_mapper.py | sort -k1,1 | ./ip_count_reducer.py

# R√©sultat attendu :
# 192.168.1.10    2
# 192.168.1.11    2
# 192.168.1.12    1</code></pre>
                </div>

                <div class="alert alert-success">
                    <h4>‚úÖ Validation</h4>
                    <p>Si les tests locaux fonctionnent, vous √™tes pr√™t pour Hadoop !</p>
                </div>

                <h3>‚úÖ √âtape 4 : Ex√©cuter sur Hadoop</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Lancer le job MapReduce
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -input /user/$USER/logs/raw/access/access.log \
    -output /user/$USER/logs/processed/ip_counts \
    -mapper ip_count_mapper.py \
    -reducer ip_count_reducer.py \
    -file ip_count_mapper.py \
    -file ip_count_reducer.py

# Voir les r√©sultats
hdfs dfs -cat /user/$USER/logs/processed/ip_counts/part-00000

# R√©sultat attendu :
# 192.168.1.10    2
# 192.168.1.11    2
# 192.168.1.12    1</code></pre>
                </div>

                <h3>‚úÖ √âtape 5 : Analyser le Job</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Voir les logs du job
yarn logs -applicationId <application_ID>

# Ou via l'interface web YARN
# http://localhost:8088 (local)
# https://[cluster].azurehdinsight.net/yarnui (Azure)</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üìä Exercice 3 : Analyse avec Hive (1h)</h2>

                <h3>üéØ Objectif</h3>
                <p>Cr√©er des tables Hive et effectuer des analyses SQL sur les logs.</p>

                <h3>‚úÖ √âtape 1 : D√©marrer Hive</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Lancer Hive CLI
hive

# Ou Beeline (plus moderne)
beeline -u jdbc:hive2://localhost:10000</code></pre>
                </div>

                <h3>‚úÖ √âtape 2 : Cr√©er une Table pour les Access Logs</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">-- Cr√©er une table externe pour les access logs
CREATE EXTERNAL TABLE IF NOT EXISTS access_logs (
    ip STRING,
    identity STRING,
    user STRING,
    timestamp STRING,
    request STRING,
    status INT,
    size INT
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "^(\\S+) (\\S+) (\\S+) \\[([^\\]]+)\\] \"([^\"]+)\" (\\d+) (\\d+)"
)
STORED AS TEXTFILE
LOCATION '/user/$USER/logs/raw/access';

-- V√©rifier que la table est cr√©√©e
SHOW TABLES;

-- Voir la structure
DESCRIBE access_logs;</code></pre>
                </div>

                <h3>‚úÖ √âtape 3 : Requ√™tes d'Analyse</h3>

                <h4>Requ√™te 1 : Compter le nombre total de requ√™tes</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">SELECT COUNT(*) as total_requests
FROM access_logs;</code></pre>
                </div>

                <h4>Requ√™te 2 : Top 3 des IPs les plus actives</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">SELECT ip, COUNT(*) as request_count
FROM access_logs
GROUP BY ip
ORDER BY request_count DESC
LIMIT 3;</code></pre>
                </div>

                <h4>Requ√™te 3 : Requ√™tes r√©ussies (status 200)</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">SELECT COUNT(*) as successful_requests
FROM access_logs
WHERE status = 200;</code></pre>
                </div>

                <h4>Requ√™te 4 : Taille totale des donn√©es transf√©r√©es par IP</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">SELECT ip, SUM(size) as total_bytes
FROM access_logs
GROUP BY ip
ORDER BY total_bytes DESC;</code></pre>
                </div>

                <h3>‚úÖ √âtape 4 : Cr√©er une Table pour les Error Logs</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">-- Table pour les error logs
CREATE EXTERNAL TABLE IF NOT EXISTS error_logs (
    timestamp STRING,
    level STRING,
    message STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "\\[([^\\]]+)\\] (\\w+) (.+)"
)
STORED AS TEXTFILE
LOCATION '/user/$USER/logs/raw/error';

-- Compter les erreurs par niveau (ERROR, WARNING)
SELECT level, COUNT(*) as count
FROM error_logs
GROUP BY level;</code></pre>
                </div>

                <h3>‚úÖ √âtape 5 : Export des R√©sultats</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-sql">-- Cr√©er une table avec les r√©sultats
CREATE TABLE ip_statistics AS
SELECT
    ip,
    COUNT(*) as request_count,
    SUM(size) as total_bytes,
    AVG(size) as avg_bytes
FROM access_logs
GROUP BY ip;

-- Exporter vers HDFS
INSERT OVERWRITE DIRECTORY '/user/$USER/logs/processed/ip_statistics'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT * FROM ip_statistics;</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üîß Exercice 4 : Job MapReduce Avanc√© - Top N (1h)</h2>

                <h3>üéØ Objectif</h3>
                <p>Cr√©er un job qui trouve les 3 pages les plus visit√©es dans les access logs.</p>

                <h3>‚úÖ √âtape 1 : Cr√©er le Mapper</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">#!/usr/bin/env python3
# top_pages_mapper.py
import sys
import re

def extract_url(request):
    """Extrait l'URL de la requ√™te HTTP"""
    match = re.search(r'GET (\S+)', request)
    if match:
        return match.group(1)
    return None

def main():
    for line in sys.stdin:
        parts = line.strip().split('"')
        if len(parts) >= 2:
            request = parts[1]
            url = extract_url(request)
            if url:
                print(f"{url}\t1")

if __name__ == "__main__":
    main()</code></pre>
                </div>

                <h3>‚úÖ √âtape 2 : Cr√©er le Reducer avec Top N</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">#!/usr/bin/env python3
# top_pages_reducer.py
import sys
from heapq import nlargest

def main():
    url_counts = {}

    # Agr√©ger tous les comptages
    for line in sys.stdin:
        line = line.strip()
        try:
            url, count = line.split('\t')
            count = int(count)
            url_counts[url] = url_counts.get(url, 0) + count
        except ValueError:
            continue

    # Trouver le top 3
    top_3 = nlargest(3, url_counts.items(), key=lambda x: x[1])

    # √âmettre le top 3
    for url, count in top_3:
        print(f"{url}\t{count}")

if __name__ == "__main__":
    main()</code></pre>
                </div>

                <h3>‚úÖ √âtape 3 : Test et Ex√©cution</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Test local
cat access.log | ./top_pages_mapper.py | sort -k1,1 | ./top_pages_reducer.py

# Ex√©cution Hadoop
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -input /user/$USER/logs/raw/access/access.log \
    -output /user/$USER/logs/processed/top_pages \
    -mapper top_pages_mapper.py \
    -reducer top_pages_reducer.py \
    -file top_pages_mapper.py \
    -file top_pages_reducer.py

# R√©sultats
hdfs dfs -cat /user/$USER/logs/processed/top_pages/part-00000</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üêõ Exercice 5 : D√©pannage (30 min)</h2>

                <h3>üéØ Objectif</h3>
                <p>Apprendre √† d√©boguer les probl√®mes courants de Hadoop.</p>

                <h3>Probl√®me 1 : Job MapReduce √©choue</h3>

                <div class="alert alert-danger">
                    <h4>Erreur Simul√©e</h4>
                    <p><code>Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads()</code></p>
                </div>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Solutions :

# 1. V√©rifier que les scripts sont ex√©cutables
chmod +x mapper.py reducer.py

# 2. V√©rifier le shebang
head -1 mapper.py
# Doit afficher : #!/usr/bin/env python3

# 3. Tester en local AVANT Hadoop
cat input.txt | ./mapper.py | sort | ./reducer.py

# 4. Voir les logs d'erreur
yarn logs -applicationId <app_id></code></pre>
                </div>

                <h3>Probl√®me 2 : Fichier HDFS introuvable</h3>

                <div class="alert alert-danger">
                    <h4>Erreur</h4>
                    <p><code>Input path does not exist: /user/data/file.txt</code></p>
                </div>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Solution :

# V√©rifier le chemin exact
hdfs dfs -ls /user/
hdfs dfs -ls /user/$USER/

# V√©rifier les permissions
hdfs dfs -ls -d /user/$USER/data

# Cr√©er le r√©pertoire si n√©cessaire
hdfs dfs -mkdir -p /user/$USER/data</code></pre>
                </div>

                <h3>Probl√®me 3 : R√©sultats vides ou incorrects</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Debugging :

# 1. V√©rifier l'entr√©e
hdfs dfs -cat /input/file.txt | head -10

# 2. Tester le mapper seul
hdfs dfs -cat /input/file.txt | ./mapper.py

# 3. Ajouter des prints de debug dans le code Python
import sys
print(f"DEBUG: Processing line: {line}", file=sys.stderr)

# 4. Voir stderr dans les logs YARN
yarn logs -applicationId <app_id> 2>&1 | grep DEBUG</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üìù R√©sum√© et Prochaines √âtapes</h2>
                <div class="key-points">
                    <h3>Comp√©tences Acquises</h3>
                    <ul>
                        <li>‚úÖ Manipulation HDFS : cr√©ation, copie, permissions</li>
                        <li>‚úÖ Jobs MapReduce Python avec Hadoop Streaming</li>
                        <li>‚úÖ Test local avant ex√©cution Hadoop</li>
                        <li>‚úÖ Analyse de donn√©es avec Hive et HiveQL</li>
                        <li>‚úÖ Debugging et r√©solution de probl√®mes</li>
                        <li>‚úÖ Export de r√©sultats depuis Hive</li>
                    </ul>
                </div>

                <div class="alert alert-success">
                    <h4>üéì F√©licitations !</h4>
                    <p>
                        Vous avez termin√© les exercices pratiques ! Vous √™tes maintenant capable de :
                    </p>
                    <ul>
                        <li>G√©rer des donn√©es dans HDFS</li>
                        <li>Cr√©er des pipelines MapReduce</li>
                        <li>Analyser des donn√©es avec Hive</li>
                        <li>R√©soudre les probl√®mes courants</li>
                    </ul>
                    <p>
                        <strong>Prochaine √©tape :</strong> Passez au Brief pratique complet pour un projet de bout en bout !
                    </p>
                </div>
            </section>
        </div>

        <footer>
            <p>&copy; 2025 Formation Hadoop - Data Engineering | Simplon</p>
            <p><a href="../index.html">‚Üê Retour √† l'accueil</a></p>
        </footer>
    </div>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-xml.min.js"></script>
</body>
</html>
