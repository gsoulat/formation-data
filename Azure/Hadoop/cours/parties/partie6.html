<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Partie 6 : Installation et Configuration - Formation Hadoop</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

    <link rel="stylesheet" href="../assets/styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>‚öôÔ∏è Partie 6 : Installation et Configuration</h1>
            <p class="subtitle">D√©ployer votre propre cluster Hadoop</p>
            <div class="duration">‚è±Ô∏è Dur√©e : 2h30</div>
        </header>

        <nav class="nav-menu">
            <ul>
                <li><a href="../index.html">üè† Accueil</a></li>
                <li><a href="partie5.html">‚Üê Partie 5</a></li>
                <li><a href="partie6.html" class="active">Partie 6</a></li>
            </ul>
        </nav>

        <div class="content">
            <div class="objectives">
                <h2>üéØ Objectifs d'Apprentissage</h2>
                <ul>
                    <li>Comprendre les modes de d√©ploiement Hadoop</li>
                    <li>Installer Hadoop en mode Pseudo-distribu√©</li>
                    <li>Configurer HDFS et YARN</li>
                    <li>D√©marrer et arr√™ter les services Hadoop</li>
                    <li>V√©rifier l'installation</li>
                </ul>
            </div>

            <section class="section">
                <h2>üîß 1. Modes de D√©ploiement</h2>

                <table>
                    <thead>
                        <tr>
                            <th>Mode</th>
                            <th>Description</th>
                            <th>Cas d'Usage</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Standalone</strong></td>
                            <td>Processus unique, pas de HDFS ni YARN</td>
                            <td>D√©veloppement, d√©bogage local</td>
                        </tr>
                        <tr>
                            <td><strong>Pseudo-distribu√©</strong></td>
                            <td>Tous les d√©mons sur une seule machine</td>
                            <td>Apprentissage, tests, d√©veloppement</td>
                        </tr>
                        <tr>
                            <td><strong>Distribu√©</strong></td>
                            <td>Cluster multi-n≈ìuds (production)</td>
                            <td>Production, environnements r√©els</td>
                        </tr>
                    </tbody>
                </table>

                <div class="alert alert-info">
                    <h4>Pour ce TP</h4>
                    <p>
                        Nous allons installer Hadoop en <strong>mode Pseudo-distribu√©</strong> sur une machine Linux unique.
                        C'est le meilleur mode pour apprendre car il simule un vrai cluster avec tous les d√©mons Hadoop.
                    </p>
                </div>
            </section>

            <section class="section">
                <h2>üìã 2. Pr√©requis</h2>

                <h3>Environnement Requis</h3>
                <div class="grid">
                    <div class="grid-item">
                        <h4>üíª Syst√®me d'Exploitation</h4>
                        <p>Linux (Ubuntu, CentOS, Debian) ou macOS</p>
                    </div>
                    <div class="grid-item">
                        <h4>‚òï Java</h4>
                        <p>OpenJDK 8 ou 11 (JDK 8 recommand√© pour Hadoop 3.x)</p>
                    </div>
                    <div class="grid-item">
                        <h4>üîë SSH</h4>
                        <p>OpenSSH install√© et configur√©</p>
                    </div>
                    <div class="grid-item">
                        <h4>üíæ Ressources</h4>
                        <p>Minimum 4 GB RAM, 20 GB disque</p>
                    </div>
                </div>

                <h3>Installation de Java</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Ubuntu/Debian
sudo apt update
sudo apt install openjdk-8-jdk -y

# CentOS/RHEL
sudo yum install java-1.8.0-openjdk-devel -y

# V√©rifier l'installation
java -version

# Configurer JAVA_HOME
echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> ~/.bashrc
echo 'export PATH=$PATH:$JAVA_HOME/bin' >> ~/.bashrc
source ~/.bashrc</code></pre>
                </div>

                <h3>Configuration SSH sans mot de passe</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Installer SSH (si n√©cessaire)
sudo apt install openssh-server openssh-client -y

# G√©n√©rer une cl√© SSH
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# Autoriser la connexion sans mot de passe
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# Tester
ssh localhost
# Tapez 'exit' pour quitter</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üì• 3. T√©l√©chargement et Installation de Hadoop</h2>

                <h3>T√©l√©charger Hadoop</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Aller dans le r√©pertoire home
cd ~

# T√©l√©charger Hadoop 3.3.6 (version stable)
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# Extraire l'archive
tar -xzvf hadoop-3.3.6.tar.gz

# Renommer pour simplifier
mv hadoop-3.3.6 hadoop

# Supprimer l'archive
rm hadoop-3.3.6.tar.gz</code></pre>
                </div>

                <h3>Configurer les Variables d'Environnement</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Ajouter √† ~/.bashrc
cat >> ~/.bashrc << 'EOF'
# Hadoop Environment Variables
export HADOOP_HOME=$HOME/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
EOF

# Recharger le fichier
source ~/.bashrc

# V√©rifier
hadoop version</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>‚öôÔ∏è 4. Configuration de Hadoop</h2>

                <p>Les fichiers de configuration se trouvent dans <code>$HADOOP_HOME/etc/hadoop/</code></p>

                <h3>1. hadoop-env.sh</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># √âditer le fichier
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# Ajouter/modifier cette ligne :
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</code></pre>
                </div>

                <h3>2. core-site.xml</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
    &lt;!-- URI du syst√®me de fichiers par d√©faut (HDFS NameNode) --&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- R√©pertoire temporaire --&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/home/&lt;votreuser&gt;/hadoop_tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
                </div>

                <div class="alert alert-warning">
                    <h4>Important</h4>
                    <p>Remplacez <code>&lt;votreuser&gt;</code> par votre nom d'utilisateur Linux.</p>
                </div>

                <h3>3. hdfs-site.xml</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
    &lt;!-- Facteur de r√©plication (1 car une seule machine) --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- R√©pertoire du NameNode --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;/home/&lt;votreuser&gt;/hadoop_data/namenode&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- R√©pertoire du DataNode --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;/home/&lt;votreuser&gt;/hadoop_data/datanode&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
                </div>

                <h3>4. mapred-site.xml</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
    &lt;!-- Framework MapReduce utilise YARN --&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- ApplicationMaster pour MapReduce --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;
        &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.map.env&lt;/name&gt;
        &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;
        &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
                </div>

                <h3>5. yarn-site.xml</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;configuration&gt;
    &lt;!-- Classe du shuffle handler pour MapReduce --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- ResourceManager hostname --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
        &lt;value&gt;localhost&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- M√©moire disponible pour YARN (ajuster selon vos ressources) --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- VCores disponibles --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
        &lt;value&gt;2&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
                </div>

                <h3>Cr√©er les R√©pertoires</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Cr√©er les r√©pertoires de donn√©es
mkdir -p ~/hadoop_tmp
mkdir -p ~/hadoop_data/namenode
mkdir -p ~/hadoop_data/datanode</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üöÄ 5. D√©marrage de Hadoop</h2>

                <h3>Formater le NameNode</h3>
                <div class="alert alert-danger">
                    <h4>‚ö†Ô∏è Attention</h4>
                    <p>Ne formater qu'√† la <strong>premi√®re installation</strong>. Formater √† nouveau supprime toutes les donn√©es HDFS !</p>
                </div>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Formater le NameNode
hdfs namenode -format

# Vous devriez voir : "Storage directory ... has been successfully formatted."</code></pre>
                </div>

                <h3>D√©marrer HDFS</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># D√©marrer NameNode et DataNode
start-dfs.sh

# V√©rifier les processus actifs
jps

# Vous devriez voir :
# - NameNode
# - DataNode
# - SecondaryNameNode
# - Jps</code></pre>
                </div>

                <h3>D√©marrer YARN</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># D√©marrer ResourceManager et NodeManager
start-yarn.sh

# V√©rifier avec jps
jps

# Vous devriez maintenant voir en plus :
# - ResourceManager
# - NodeManager</code></pre>
                </div>

                <h3>Arr√™ter Hadoop</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Arr√™ter YARN
stop-yarn.sh

# Arr√™ter HDFS
stop-dfs.sh

# Ou tout arr√™ter d'un coup
stop-all.sh</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>‚úÖ 6. V√©rification de l'Installation</h2>

                <h3>1. Interfaces Web</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Service</th>
                            <th>URL</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>NameNode</td>
                            <td><a href="http://localhost:9870">http://localhost:9870</a></td>
                            <td>Interface HDFS</td>
                        </tr>
                        <tr>
                            <td>ResourceManager</td>
                            <td><a href="http://localhost:8088">http://localhost:8088</a></td>
                            <td>Interface YARN</td>
                        </tr>
                        <tr>
                            <td>Secondary NameNode</td>
                            <td><a href="http://localhost:9868">http://localhost:9868</a></td>
                            <td>Checkpoint NameNode</td>
                        </tr>
                    </tbody>
                </table>

                <h3>2. Tests en Ligne de Commande</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Cr√©er un r√©pertoire dans HDFS
hdfs dfs -mkdir -p /user/$USER

# Cr√©er un fichier de test local
echo "Hello Hadoop World" > test.txt

# Copier le fichier dans HDFS
hdfs dfs -put test.txt /user/$USER/

# Lister les fichiers
hdfs dfs -ls /user/$USER/

# Afficher le contenu
hdfs dfs -cat /user/$USER/test.txt

# Voir l'espace disque HDFS
hdfs dfs -df -h

# Rapport HDFS
hdfs dfsadmin -report</code></pre>
                </div>

                <h3>3. Test avec un Job MapReduce</h3>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Pr√©parer les donn√©es d'entr√©e
hdfs dfs -mkdir -p /user/$USER/wordcount/input
echo "Hello Hadoop Hello World" > words.txt
echo "Hadoop is powerful" >> words.txt
hdfs dfs -put words.txt /user/$USER/wordcount/input/

# Ex√©cuter l'exemple WordCount fourni avec Hadoop
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
    wordcount \
    /user/$USER/wordcount/input \
    /user/$USER/wordcount/output

# Voir les r√©sultats
hdfs dfs -cat /user/$USER/wordcount/output/part-r-00000

# R√©sultat attendu :
# Hadoop  2
# Hello   2
# World   1
# is      1
# powerful 1</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üõ†Ô∏è 7. D√©pannage</h2>

                <h3>Probl√®mes Courants</h3>

                <h4>Les d√©mons ne d√©marrent pas</h4>
                <div class="card">
                    <p><strong>V√©rifier les logs :</strong></p>
                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># Logs dans $HADOOP_HOME/logs/
tail -f $HADOOP_HOME/logs/hadoop-*-namenode-*.log</code></pre>
                    </div>
                    <p><strong>Causes fr√©quentes :</strong></p>
                    <ul>
                        <li>JAVA_HOME mal configur√©</li>
                        <li>Ports d√©j√† utilis√©s</li>
                        <li>SSH sans mot de passe non configur√©</li>
                        <li>Permissions incorrectes sur les r√©pertoires</li>
                    </ul>
                </div>

                <h4>DataNode ne se connecte pas au NameNode</h4>
                <div class="card">
                    <p><strong>Solutions :</strong></p>
                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># Arr√™ter tout
stop-all.sh

# Nettoyer les donn√©es
rm -rf ~/hadoop_data/*
rm -rf ~/hadoop_tmp/*

# Reformater
hdfs namenode -format

# Red√©marrer
start-dfs.sh
start-yarn.sh</code></pre>
                    </div>
                </div>

                <h4>Commandes Utiles de Diagnostic</h4>
                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># V√©rifier les processus Java en cours
jps

# Tester la connectivit√© SSH
ssh localhost

# V√©rifier les ports ouverts
netstat -tuln | grep -E '9870|8088|9000'

# Voir la version de Hadoop
hadoop version

# Rapport d√©taill√© HDFS
hdfs dfsadmin -report</code></pre>
                </div>
            </section>

            <section class="section">
                <h2>üîí 8. Bonnes Pratiques et S√©curit√©</h2>

                <div class="grid">
                    <div class="grid-item">
                        <h4>üìä Monitoring</h4>
                        <ul>
                            <li>Consulter r√©guli√®rement les UI web</li>
                            <li>Surveiller les logs</li>
                            <li>V√©rifier l'espace disque HDFS</li>
                        </ul>
                    </div>
                    <div class="grid-item">
                        <h4>üíæ Backups</h4>
                        <ul>
                            <li>Sauvegarder les donn√©es critiques</li>
                            <li>Exporter les m√©tadonn√©es du NameNode</li>
                            <li>Documenter la configuration</li>
                        </ul>
                    </div>
                    <div class="grid-item">
                        <h4>üîê S√©curit√©</h4>
                        <ul>
                            <li>Configurer Kerberos en production</li>
                            <li>Utiliser des ACLs HDFS</li>
                            <li>S√©curiser les ports avec firewall</li>
                        </ul>
                    </div>
                    <div class="grid-item">
                        <h4>‚ö° Performance</h4>
                        <ul>
                            <li>Ajuster la m√©moire YARN</li>
                            <li>Optimiser le facteur de r√©plication</li>
                            <li>Utiliser la compression</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="section">
                <h2>üìù R√©sum√© de la Partie 6</h2>
                <div class="key-points">
                    <h3>Points Cl√©s √† Retenir</h3>
                    <ul>
                        <li>3 modes de d√©ploiement : Standalone, Pseudo-distribu√©, Distribu√©</li>
                        <li>Pr√©requis : Java, SSH, ressources syst√®me suffisantes</li>
                        <li>Configuration principale via 5 fichiers XML dans etc/hadoop/</li>
                        <li>Formater le NameNode uniquement √† la premi√®re installation</li>
                        <li>D√©marrage : start-dfs.sh puis start-yarn.sh</li>
                        <li>V√©rification via interfaces web (ports 9870 et 8088) et commandes CLI</li>
                        <li>Les logs sont essentiels pour le d√©pannage</li>
                    </ul>
                </div>
            </section>

            <div class="alert alert-success" style="margin-top: 40px;">
                <h4>üéâ F√©licitations !</h4>
                <p>
                    Vous avez termin√© le cours Hadoop ! Vous √™tes maintenant capable d'installer, configurer et utiliser
                    un cluster Hadoop. Pour mettre en pratique vos connaissances, passez au <strong>Brief pratique</strong>
                    qui vous guidera dans la cr√©ation d'un pipeline Big Data complet.
                </p>
            </div>
        </div>

        <footer>
            <p>&copy; 2025 Formation Hadoop - Data Engineering | Simplon</p>
            <p><a href="../index.html">‚Üê Retour √† l'accueil</a></p>
        </footer>
    </div>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-xml.min.js"></script>
</body>
</html>
