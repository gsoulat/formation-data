{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "# Exercice 02 : Transformations et Agregations",
        "",
        "## Objectifs pedagogiques",
        "",
        "A la fin de cet exercice, vous serez capable de :",
        "- Utiliser les fonctions d'agregation (sum, avg, count, min, max)",
        "- Regrouper des donnees avec groupBy",
        "- Calculer des statistiques par groupe",
        "- Utiliser les fonctions de date et heure",
        "- Faire des jointures entre tables",
        "- Creer des pivots",
        "",
        "## Duree estimee : 60 minutes",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 1 : Concepts theoriques",
        "",
        "### Fonctions d'agregation",
        "",
        "Les fonctions d'agregation calculent une valeur unique a partir de plusieurs lignes :",
        "- `count()` : Nombre de lignes",
        "- `sum()` : Somme",
        "- `avg()` : Moyenne",
        "- `min()` : Minimum",
        "- `max()` : Maximum",
        "- `first()` : Premiere valeur",
        "- `last()` : Derniere valeur",
        "- `collect_list()` : Liste de toutes les valeurs",
        "- `collect_set()` : Ensemble de valeurs uniques",
        "",
        "### GroupBy",
        "",
        "`groupBy()` permet de regrouper les lignes selon une ou plusieurs colonnes,",
        "puis d'appliquer des agregations sur chaque groupe.",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 2 : Agregations simples",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 2.1 : Agregations globales\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import count, countDistinct, avg, min, max\n\n# Charger les donnees\ndf_gares = spark.table(\"gares_silver\")\n\n# Agregations simples\nprint(\"Statistiques globales :\")\ndf_stats = df_gares.agg(\n    count(\"*\").alias(\"nombre_total_gares\"),\n    countDistinct(\"code_departement\").alias(\"nombre_departements\"),\n    countDistinct(\"segment_clean\").alias(\"nombre_segments\"),\n    avg(\"latitude\").alias(\"latitude_moyenne\"),\n    min(\"latitude\").alias(\"latitude_min\"),\n    max(\"latitude\").alias(\"latitude_max\")\n)\n\ndf_stats.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 2.2",
        "",
        "Calculez les statistiques suivantes :",
        "- Nombre total de gares",
        "- Nombre de trigrammes uniques",
        "- Longitude minimale et maximale",
        "- Nombre de categories de gares distinctes",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\nfrom pyspark.sql.functions import count, countDistinct, min, max\n\ndf_stats_exercice = df_gares.agg(\n    count(\"*\").alias(\"total_gares\"),\n    countDistinct(\"trigramme\").alias(\"trigrammes_uniques\"),\n    min(\"longitude\").alias(\"longitude_min\"),\n    max(\"longitude\").alias(\"longitude_max\"),\n    countDistinct(\"categorie_gare\").alias(\"nombre_categories\")\n)\n\ndf_stats_exercice.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 3 : GroupBy et agregations",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 2.3 : Regroupement par segment\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import count, avg, col\n\n# Regrouper par segment et compter\ndf_par_segment = df_gares.groupBy(\"segment_clean\").agg(\n    count(\"*\").alias(\"nombre_gares\"),\n    avg(\"latitude\").alias(\"latitude_moyenne\")\n).orderBy(\"segment_clean\")\n\nprint(\"Repartition par segment :\")\ndf_par_segment.show()\n\n# Regrouper par departement\ndf_par_dept = df_gares.groupBy(\"code_departement\").agg(\n    count(\"*\").alias(\"nombre_gares\")\n).orderBy(col(\"nombre_gares\").desc())\n\nprint(\"\\nTop 10 departements avec le plus de gares :\")\ndf_par_dept.show(10)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 2.4",
        "",
        "Regroupez les gares par categorie_gare et calculez :",
        "- Le nombre de gares par categorie",
        "- La latitude moyenne",
        "- La longitude moyenne",
        "- Triez par nombre de gares decroissant",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\nfrom pyspark.sql.functions import count, avg, col\n\ndf_par_categorie = df_gares.groupBy(\"categorie_gare\").agg(\n    count(\"*\").alias(\"nombre_gares\"),\n    avg(\"latitude\").alias(\"latitude_moyenne\"),\n    avg(\"longitude\").alias(\"longitude_moyenne\")\n).orderBy(col(\"nombre_gares\").desc())\n\ndf_par_categorie.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 4 : Agregations multiples",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 2.5 : Multiples agregations par groupe\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import count, avg, min, max, sum, when, col\n\n# Agregations complexes par departement\ndf_stats_dept = df_gares.groupBy(\"code_departement\").agg(\n    count(\"*\").alias(\"total_gares\"),\n    count(when(col(\"segment_clean\") == \"A\", 1)).alias(\"gares_A\"),\n    count(when(col(\"segment_clean\") == \"B\", 1)).alias(\"gares_B\"),\n    count(when(col(\"segment_clean\") == \"C\", 1)).alias(\"gares_C\"),\n    avg(\"latitude\").alias(\"lat_moyenne\"),\n    min(\"latitude\").alias(\"lat_min\"),\n    max(\"latitude\").alias(\"lat_max\")\n).orderBy(col(\"total_gares\").desc())\n\nprint(\"Statistiques detaillees par departement :\")\ndf_stats_dept.show(15)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 2.6",
        "",
        "Pour chaque code_departement, calculez :",
        "- Le nombre total de gares",
        "- Le pourcentage de gares principales (segment A)",
        "- La gare la plus au nord (max latitude)",
        "- La gare la plus au sud (min latitude)",
        "",
        "Affichez uniquement les departements avec plus de 10 gares",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\nfrom pyspark.sql.functions import count, sum, when, col, min, max, round\n\ndf_stats_avancees = df_gares.groupBy(\"code_departement\").agg(\n    count(\"*\").alias(\"total_gares\"),\n    (count(when(col(\"segment_clean\") == \"A\", 1)) * 100.0 / count(\"*\")).alias(\"pourcentage_A\"),\n    max(\"latitude\").alias(\"gare_plus_nord\"),\n    min(\"latitude\").alias(\"gare_plus_sud\")\n).filter(col(\"total_gares\") > 10) \\\n .orderBy(col(\"total_gares\").desc())\n\ndf_stats_avancees.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 5 : Agregations avec HAVING (filtre apres groupBy)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 2.7 : Filtrer apres agregation\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import count, col\n\n# Trouver les departements avec plus de 20 gares\ndf_depts_importants = df_gares.groupBy(\"code_departement\").agg(\n    count(\"*\").alias(\"nombre_gares\")\n).filter(col(\"nombre_gares\") > 20) \\\n .orderBy(col(\"nombre_gares\").desc())\n\nprint(\"Departements avec plus de 20 gares :\")\ndf_depts_importants.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 6 : Jointures",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 2.8 : Jointures entre tables\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import col\n\n# Creer une table de reference des departements\ndata_depts = [\n    (\"75\", \"Paris\", \"Ile-de-France\"),\n    (\"69\", \"Rhone\", \"Auvergne-Rhone-Alpes\"),\n    (\"13\", \"Bouches-du-Rhone\", \"Provence-Alpes-Cote d'Azur\"),\n    (\"59\", \"Nord\", \"Hauts-de-France\"),\n    (\"33\", \"Gironde\", \"Nouvelle-Aquitaine\"),\n    (\"44\", \"Loire-Atlantique\", \"Pays de la Loire\"),\n    (\"67\", \"Bas-Rhin\", \"Grand Est\"),\n    (\"31\", \"Haute-Garonne\", \"Occitanie\"),\n    (\"06\", \"Alpes-Maritimes\", \"Provence-Alpes-Cote d'Azur\"),\n    (\"92\", \"Hauts-de-Seine\", \"Ile-de-France\")\n]\n\ndf_depts_ref = spark.createDataFrame(\n    data_depts,\n    [\"code_dept\", \"nom_dept\", \"region\"]\n)\n\n# Sauvegarder la table de reference\ndf_depts_ref.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"departements_ref\")\n\nprint(\"Table de reference creee\")\ndf_depts_ref.show()\n\n# Jointure INNER (garde uniquement les correspondances)\ndf_join_inner = df_gares.join(\n    df_depts_ref,\n    df_gares.code_departement == df_depts_ref.code_dept,\n    \"inner\"\n).select(\n    \"nom\",\n    \"trigramme\",\n    \"nom_dept\",\n    \"region\",\n    \"segment_clean\"\n)\n\nprint(f\"\\nJointure INNER : {df_join_inner.count()} lignes\")\ndf_join_inner.show(10)\n\n# Jointure LEFT (garde toutes les gares, meme sans correspondance)\ndf_join_left = df_gares.join(\n    df_depts_ref,\n    df_gares.code_departement == df_depts_ref.code_dept,\n    \"left\"\n).select(\n    \"nom\",\n    \"code_departement\",\n    \"nom_dept\",\n    \"region\"\n)\n\nprint(f\"\\nJointure LEFT : {df_join_left.count()} lignes\")\ndf_join_left.filter(col(\"nom_dept\").isNull()).show(10)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE PRATIQUE 2.9",
        "",
        "Faites une jointure entre gares_silver et departements_ref, puis :",
        "1. Comptez le nombre de gares par region",
        "2. Calculez le nombre de gares principales (A) par region",
        "3. Triez par nombre de gares decroissant",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER\nfrom pyspark.sql.functions import count, when, col\n\n# Jointure\ndf_gares_region = df_gares.join(\n    df_depts_ref,\n    df_gares.code_departement == df_depts_ref.code_dept,\n    \"inner\"\n)\n\n# Agregation par region\ndf_stats_region = df_gares_region.groupBy(\"region\").agg(\n    count(\"*\").alias(\"total_gares\"),\n    count(when(col(\"segment_clean\") == \"A\", 1)).alias(\"gares_principales\")\n).orderBy(col(\"total_gares\").desc())\n\nprint(\"Statistiques par region :\")\ndf_stats_region.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 7 : Fonctions de fenetre (apercu)",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 2.10 : Introduction aux Window Functions\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, dense_rank, col\n\n# Creer une fenetre partitionnee par departement, triee par nom\nwindow_spec = Window.partitionBy(\"code_departement\").orderBy(\"nom\")\n\n# Ajouter un numero de ligne par departement\ndf_avec_rang = df_gares \\\n    .filter(col(\"code_departement\").isin(\"75\", \"69\", \"13\")) \\\n    .withColumn(\"rang_dans_dept\", row_number().over(window_spec)) \\\n    .select(\"code_departement\", \"nom\", \"trigramme\", \"rang_dans_dept\")\n\nprint(\"Rang de chaque gare dans son departement :\")\ndf_avec_rang.show(20)\n\n# Trouver la premiere gare de chaque departement (ordre alphabetique)\ndf_premieres = df_avec_rang.filter(col(\"rang_dans_dept\") == 1)\nprint(\"\\nPremiere gare de chaque departement (alphabetiquement) :\")\ndf_premieres.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 8 : Pivot tables",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nprint(\"EXERCICE 2.11 : Tables croisees dynamiques\")\nprint(\"=\" * 70)\n\nfrom pyspark.sql.functions import count, col\n\n# Creer un pivot : segments en lignes, departements en colonnes\ndf_pivot = df_gares \\\n    .filter(col(\"code_departement\").isin(\"75\", \"69\", \"13\", \"59\", \"33\")) \\\n    .groupBy(\"segment_clean\") \\\n    .pivot(\"code_departement\") \\\n    .agg(count(\"*\"))\n\nprint(\"Pivot : nombre de gares par segment et departement :\")\ndf_pivot.show()\n\n# Pivot inverse : departements en lignes, segments en colonnes\ndf_pivot_inverse = df_gares \\\n    .filter(col(\"code_departement\").isin(\"75\", \"69\", \"13\", \"59\", \"33\")) \\\n    .groupBy(\"code_departement\") \\\n    .pivot(\"segment_clean\") \\\n    .agg(count(\"*\"))\n\nprint(\"\\nPivot inverse :\")\ndf_pivot_inverse.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## EXERCICE FINAL 2.12 : Projet complet d'analyse",
        "",
        "Creez une analyse complete qui :",
        "",
        "1. Joint les gares avec la table de reference des departements",
        "2. Groupe par region et segment",
        "3. Calcule pour chaque groupe :",
        "- Le nombre de gares",
        "- La latitude moyenne",
        "- La longitude moyenne",
        "4. Filtre uniquement les groupes avec plus de 5 gares",
        "5. Trie par region puis par nombre de gares decroissant",
        "6. Sauvegarde le resultat dans une nouvelle table",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# A COMPLETER - SOLUTION PROPOSEE\nfrom pyspark.sql.functions import count, avg, col\n\n# Pipeline complete\ndf_analyse_complete = df_gares \\\n    .join(df_depts_ref, df_gares.code_departement == df_depts_ref.code_dept, \"inner\") \\\n    .groupBy(\"region\", \"segment_clean\") \\\n    .agg(\n        count(\"*\").alias(\"nombre_gares\"),\n        avg(\"latitude\").alias(\"latitude_moyenne\"),\n        avg(\"longitude\").alias(\"longitude_moyenne\")\n    ) \\\n    .filter(col(\"nombre_gares\") > 5) \\\n    .orderBy(\"region\", col(\"nombre_gares\").desc())\n\nprint(\"Analyse complete :\")\ndf_analyse_complete.show(30)\n\n# Sauvegarder\ndf_analyse_complete.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"analyse_gares_region\")\nprint(\"\\nTable 'analyse_gares_region' creee !\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## PARTIE 9 : Agregations avec SQL",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n%sql\n-- EXERCICE 2.13 : Meme analyse en SQL\n\nSELECT\nsegment_clean,\nCOUNT(*) as nombre_gares,\nROUND(AVG(latitude), 4) as lat_moyenne,\nROUND(AVG(longitude), 4) as lon_moyenne,\nMIN(latitude) as lat_min,\nMAX(latitude) as lat_max\nFROM gares_silver\nGROUP BY segment_clean\nORDER BY segment_clean\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n%sql\n-- Top 10 departements\nSELECT\ncode_departement,\nCOUNT(*) as total,\nSUM(CASE WHEN segment_clean = 'A' THEN 1 ELSE 0 END) as gares_A,\nSUM(CASE WHEN segment_clean = 'B' THEN 1 ELSE 0 END) as gares_B,\nSUM(CASE WHEN segment_clean = 'C' THEN 1 ELSE 0 END) as gares_C\nFROM gares_silver\nGROUP BY code_departement\nORDER BY total DESC\nLIMIT 10\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "",
        "## RESUME DES CONCEPTS APPRIS",
        "",
        "Dans cet exercice, vous avez appris :",
        "",
        "1. **Agregations simples** : `count()`, `sum()`, `avg()`, `min()`, `max()`",
        "2. **Regroupements** : `groupBy()`, `agg()`",
        "3. **Agregations conditionnelles** : `count(when())`",
        "4. **Filtres post-agregation** : `filter()` apres `groupBy()`",
        "5. **Jointures** : `join()` avec INNER, LEFT, RIGHT",
        "6. **Fonctions de fenetre** : `Window`, `row_number()`, `rank()`",
        "7. **Pivot tables** : `pivot()`",
        "8. **SQL avance** : `GROUP BY`, `HAVING`, `CASE WHEN`",
        "",
        "## Prochaine etape",
        "",
        "Passez a l'**Exercice 03 : Fonctions de fenetre avancees** pour maitriser :",
        "- Les calculs cumules (running totals)",
        "- Les classements (ranking)",
        "- Les decalages (lag/lead)",
        "- Les moyennes mobiles",
        ""
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Exercice_02_Agregations",
      "dashboards": [],
      "language": "python",
      "widgets": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}