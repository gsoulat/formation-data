<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projet Fil Rouge - Pipeline ETL Sales Data</title>

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

    <link rel="stylesheet" href="assets/styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸš€ Projet Fil Rouge</h1>
            <p class="subtitle">Pipeline ETL Sales Data - Projet complet de bout en bout</p>
            <span class="duration">â±ï¸ DurÃ©e : 1 heure</span>
        </header>

        <nav class="nav-menu">
            <ul>
                <li><a href="index.html">ğŸ  Accueil</a></li>
                <li><a href="exercices.html">â† Exercices</a></li>
            </ul>
        </nav>

        <div class="content">
            <div class="objectives">
                <h2>ğŸ¯ Objectifs du projet</h2>
                <ul>
                    <li>CrÃ©er un pipeline ETL complet avec Git dÃ¨s le dÃ©but</li>
                    <li>Appliquer les stratÃ©gies de branches (Git Flow)</li>
                    <li>GÃ©rer des features en parallÃ¨le</li>
                    <li>RÃ©soudre des conflits rÃ©alistes</li>
                    <li>Collaborer (simulation)</li>
                    <li>Configurer pre-commit hooks</li>
                    <li>DÃ©ployer une "release" en production</li>
                </ul>
            </div>

            <section class="section">
                <h2>ğŸ“‹ Contexte du projet</h2>

                <div class="card">
                    <h3>Scenario</h3>
                    <p>
                        Vous Ãªtes Data Engineer chez <strong>DataMart Inc.</strong> et devez crÃ©er un pipeline ETL
                        pour analyser les donnÃ©es de ventes. Le pipeline doit :
                    </p>
                    <ul>
                        <li>Extraire des donnÃ©es depuis une API et une base PostgreSQL</li>
                        <li>Transformer et nettoyer les donnÃ©es</li>
                        <li>Charger dans un Data Warehouse (Snowflake simulÃ©)</li>
                        <li>GÃ©nÃ©rer des rapports automatiques</li>
                    </ul>
                    <p>
                        Vous travaillerez avec Git pour gÃ©rer le code, les branches et les dÃ©ploiements.
                    </p>
                </div>

                <div class="workflow-diagram">
                    <pre>
Architecture du Pipeline :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API REST  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Extract    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Raw Data  â”‚
â”‚  (Orders)   â”‚         â”‚   Module     â”‚         â”‚   (JSON)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ PostgreSQL  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Extract    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ (Customers) â”‚         â”‚   Module     â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â–¼
                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚  Transform  â”‚
                                                  â”‚   Module    â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚    Load     â”‚
                                                  â”‚   Module    â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚  Snowflake  â”‚
                                                  â”‚     DWH     â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    </pre>
                </div>
            </section>

            <!-- PHASE 1 : INITIALISATION -->
            <section class="section">
                <h2>Phase 1 : Initialisation du projet (10 min)</h2>

                <div class="exercise">
                    <h4>Ã‰tape 1.1 : CrÃ©er le projet et initialiser Git</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># CrÃ©er la structure du projet
mkdir sales-etl-pipeline
cd sales-etl-pipeline

# Initialiser Git
git init
git config user.name "Votre Nom"
git config user.email "votre@email.com"

# CrÃ©er la structure de dossiers
mkdir -p src/{extract,transform,load} tests data/{raw,processed} config docs

# CrÃ©er la structure
tree -L 2
# sales-etl-pipeline/
# â”œâ”€â”€ src/
# â”‚   â”œâ”€â”€ extract/
# â”‚   â”œâ”€â”€ transform/
# â”‚   â””â”€â”€ load/
# â”œâ”€â”€ tests/
# â”œâ”€â”€ data/
# â”‚   â”œâ”€â”€ raw/
# â”‚   â””â”€â”€ processed/
# â”œâ”€â”€ config/
# â””â”€â”€ docs/</code></pre>
                    </div>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 1.2 : CrÃ©er le .gitignore et README</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># CrÃ©er .gitignore pour Data Engineering
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
venv/
.venv/
*.egg-info/

# DonnÃ©es - NE JAMAIS VERSIONNER
data/raw/*
data/processed/*
*.csv
*.parquet
*.json
!data/raw/.gitkeep
!data/processed/.gitkeep

# Credentials
.env
.env.*
!.env.example
config/secrets.yaml
credentials.json

# IDE
.vscode/
.idea/
.DS_Store

# Logs
logs/
*.log

# Jupyter
.ipynb_checkpoints/

# Tests
.pytest_cache/
.coverage
htmlcov/
EOF

# CrÃ©er les fichiers .gitkeep pour versionner les dossiers vides
touch data/raw/.gitkeep
touch data/processed/.gitkeep

# CrÃ©er README.md
cat > README.md << 'EOF'
# Sales ETL Pipeline

Pipeline ETL pour analyser les donnÃ©es de ventes de DataMart Inc.

## Architecture

- **Extract** : Extraction depuis API REST et PostgreSQL
- **Transform** : Nettoyage et enrichissement des donnÃ©es
- **Load** : Chargement dans Snowflake Data Warehouse

## Installation

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## Configuration

Copier `.env.example` vers `.env` et remplir les credentials.

## Utilisation

```bash
python src/main.py --date 2024-01-01
```

## Tests

```bash
pytest tests/
```

## Auteur

DataMart Inc. - Data Engineering Team
EOF

# CrÃ©er requirements.txt
cat > requirements.txt << 'EOF'
pandas==2.1.0
psycopg2-binary==2.9.9
requests==2.31.0
python-dotenv==1.0.0
pyyaml==6.0.1
pytest==7.4.3
EOF

# CrÃ©er .env.example
cat > .env.example << 'EOF'
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=sales_db
DB_USER=your_username
DB_PASSWORD=your_password

# API Configuration
API_URL=https://api.example.com/orders
API_KEY=your_api_key

# Snowflake Configuration
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=ANALYTICS
SNOWFLAKE_SCHEMA=SALES
EOF

# Premier commit
git add .
git commit -m "chore: Initial project structure with configuration"</code></pre>
                    </div>
                </div>

                <div class="alert alert-success">
                    <h4>âœ… Checkpoint Phase 1</h4>
                    <p>Vous devriez avoir :</p>
                    <ul>
                        <li>âœ“ DÃ©pÃ´t Git initialisÃ©</li>
                        <li>âœ“ Structure de dossiers crÃ©Ã©e</li>
                        <li>âœ“ .gitignore configurÃ©</li>
                        <li>âœ“ README.md documentÃ©</li>
                        <li>âœ“ Premier commit rÃ©alisÃ©</li>
                    </ul>
                    <p><strong>VÃ©rification :</strong> <code>git log --oneline</code> devrait montrer 1 commit</p>
                </div>
            </section>

            <!-- PHASE 2 : DÃ‰VELOPPEMENT DES MODULES -->
            <section class="section">
                <h2>Phase 2 : DÃ©veloppement des modules (25 min)</h2>

                <div class="alert alert-info">
                    <h4>StratÃ©gie de branches</h4>
                    <p>
                        Nous allons utiliser <strong>Git Flow</strong> :
                    </p>
                    <ul>
                        <li><code>main</code> : Production (vide pour l'instant)</li>
                        <li><code>develop</code> : IntÃ©gration</li>
                        <li><code>feature/*</code> : DÃ©veloppement des fonctionnalitÃ©s</li>
                    </ul>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 2.1 : CrÃ©er la branche develop</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># Renommer main si nÃ©cessaire
git branch -M main

# CrÃ©er develop depuis main
git checkout -b develop

# La branche develop sera notre branche d'intÃ©gration</code></pre>
                    </div>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 2.2 : Feature 1 - Module d'extraction</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># CrÃ©er branche feature
git checkout -b feature/extract-module

# CrÃ©er le module d'extraction API
cat > src/extract/api_extractor.py << 'EOF'
"""Module d'extraction depuis API REST"""
import requests
import json
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()

class APIExtractor:
    def __init__(self):
        self.api_url = os.getenv('API_URL')
        self.api_key = os.getenv('API_KEY')

    def extract_orders(self, date: str) -> list:
        """Extrait les commandes pour une date donnÃ©e"""
        headers = {'Authorization': f'Bearer {self.api_key}'}
        params = {'date': date}

        try:
            response = requests.get(
                f"{self.api_url}/orders",
                headers=headers,
                params=params
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error extracting orders: {e}")
            return []

    def save_raw_data(self, data: list, filename: str):
        """Sauvegarde les donnÃ©es brutes"""
        output_path = f"data/raw/{filename}"
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"Saved {len(data)} records to {output_path}")

if __name__ == "__main__":
    extractor = APIExtractor()
    orders = extractor.extract_orders("2024-01-01")
    extractor.save_raw_data(orders, "orders_2024-01-01.json")
EOF

# CrÃ©er le module d'extraction DB
cat > src/extract/db_extractor.py << 'EOF'
"""Module d'extraction depuis PostgreSQL"""
import psycopg2
import pandas as pd
import os
from dotenv import load_dotenv

load_dotenv()

class DBExtractor:
    def __init__(self):
        self.conn_params = {
            'host': os.getenv('DB_HOST'),
            'port': os.getenv('DB_PORT'),
            'database': os.getenv('DB_NAME'),
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD')
        }

    def extract_customers(self) -> pd.DataFrame:
        """Extrait la table customers"""
        query = """
        SELECT
            customer_id,
            customer_name,
            email,
            country,
            created_at
        FROM customers
        WHERE active = true
        """

        try:
            conn = psycopg2.connect(**self.conn_params)
            df = pd.read_sql(query, conn)
            conn.close()
            return df
        except Exception as e:
            print(f"Error extracting customers: {e}")
            return pd.DataFrame()

    def save_to_parquet(self, df: pd.DataFrame, filename: str):
        """Sauvegarde en format Parquet"""
        output_path = f"data/raw/{filename}"
        df.to_parquet(output_path, index=False)
        print(f"Saved {len(df)} customers to {output_path}")

if __name__ == "__main__":
    extractor = DBExtractor()
    customers = extractor.extract_customers()
    extractor.save_to_parquet(customers, "customers.parquet")
EOF

# CrÃ©er __init__.py
touch src/extract/__init__.py

# Commiter
git add src/extract/
git commit -m "feat(extract): Add API and DB extraction modules

- Add APIExtractor for orders from REST API
- Add DBExtractor for customers from PostgreSQL
- Support JSON and Parquet output formats"

# Merger dans develop
git checkout develop
git merge --no-ff feature/extract-module -m "Merge feature/extract-module into develop"

# Supprimer la branche feature
git branch -d feature/extract-module</code></pre>
                    </div>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 2.3 : Feature 2 - Module de transformation (en parallÃ¨le)</h4>

                    <p><strong>Simulation :</strong> Un autre dÃ©veloppeur travaille sur la transformation pendant que vous faisiez l'extraction.</p>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># CrÃ©er branche depuis develop
git checkout develop
git checkout -b feature/transform-module

# CrÃ©er le module de transformation
cat > src/transform/data_transformer.py << 'EOF'
"""Module de transformation et nettoyage des donnÃ©es"""
import pandas as pd
import json
from datetime import datetime

class DataTransformer:
    def __init__(self):
        pass

    def transform_orders(self, orders_file: str) -> pd.DataFrame:
        """Transforme les donnÃ©es de commandes"""
        with open(orders_file, 'r') as f:
            orders = json.load(f)

        df = pd.DataFrame(orders)

        # Nettoyage
        df = df.dropna(subset=['order_id', 'customer_id'])

        # Conversion des types
        df['order_date'] = pd.to_datetime(df['order_date'])
        df['amount'] = df['amount'].astype(float)

        # Ajout de colonnes calculÃ©es
        df['year'] = df['order_date'].dt.year
        df['month'] = df['order_date'].dt.month
        df['quarter'] = df['order_date'].dt.quarter

        return df

    def transform_customers(self, customers_file: str) -> pd.DataFrame:
        """Transforme les donnÃ©es clients"""
        df = pd.read_parquet(customers_file)

        # Nettoyage des emails
        df['email'] = df['email'].str.lower().str.strip()

        # Suppression des doublons
        df = df.drop_duplicates(subset=['customer_id'])

        return df

    def join_data(self, orders_df: pd.DataFrame, customers_df: pd.DataFrame) -> pd.DataFrame:
        """Joint orders et customers"""
        result = orders_df.merge(
            customers_df,
            on='customer_id',
            how='left'
        )
        return result

    def save_transformed_data(self, df: pd.DataFrame, filename: str):
        """Sauvegarde les donnÃ©es transformÃ©es"""
        output_path = f"data/processed/{filename}"
        df.to_parquet(output_path, index=False)
        print(f"Saved {len(df)} transformed records to {output_path}")

if __name__ == "__main__":
    transformer = DataTransformer()

    # Transform
    orders_df = transformer.transform_orders("data/raw/orders_2024-01-01.json")
    customers_df = transformer.transform_customers("data/raw/customers.parquet")

    # Join
    final_df = transformer.join_data(orders_df, customers_df)

    # Save
    transformer.save_transformed_data(final_df, "sales_enriched_2024-01-01.parquet")
EOF

touch src/transform/__init__.py

# Commiter
git add src/transform/
git commit -m "feat(transform): Add data transformation module

- Clean and validate orders data
- Enrich with customer information
- Add calculated columns (year, month, quarter)
- Join orders with customers"

# Merger dans develop
git checkout develop
git merge --no-ff feature/transform-module -m "Merge feature/transform-module into develop"
git branch -d feature/transform-module</code></pre>
                    </div>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 2.4 : Feature 3 - Module de chargement</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># CrÃ©er branche
git checkout develop
git checkout -b feature/load-module

# CrÃ©er le module de chargement
cat > src/load/snowflake_loader.py << 'EOF'
"""Module de chargement dans Snowflake"""
import pandas as pd
import os
from dotenv import load_dotenv

load_dotenv()

class SnowflakeLoader:
    def __init__(self):
        # Simulation: En rÃ©alitÃ© on utiliserait snowflake-connector-python
        self.account = os.getenv('SNOWFLAKE_ACCOUNT')
        self.warehouse = os.getenv('SNOWFLAKE_WAREHOUSE')
        self.database = os.getenv('SNOWFLAKE_DATABASE')
        self.schema = os.getenv('SNOWFLAKE_SCHEMA')

    def load_to_snowflake(self, df: pd.DataFrame, table_name: str):
        """Charge les donnÃ©es dans Snowflake"""
        # Simulation du chargement
        print(f"Connecting to Snowflake: {self.account}")
        print(f"Target: {self.database}.{self.schema}.{table_name}")
        print(f"Loading {len(df)} records...")

        # En production, on ferait:
        # conn = snowflake.connector.connect(...)
        # cursor = conn.cursor()
        # df.to_sql(table_name, con=conn, if_exists='append')

        print(f"âœ… Successfully loaded {len(df)} records to {table_name}")

    def validate_load(self, table_name: str) -> dict:
        """Valide le chargement"""
        # Simulation
        return {
            'status': 'success',
            'table': table_name,
            'row_count': 1000,
            'load_time': '2.5s'
        }

if __name__ == "__main__":
    loader = SnowflakeLoader()

    # Charger les donnÃ©es
    df = pd.read_parquet("data/processed/sales_enriched_2024-01-01.parquet")
    loader.load_to_snowflake(df, "FACT_SALES")

    # Valider
    result = loader.validate_load("FACT_SALES")
    print(f"Validation: {result}")
EOF

touch src/load/__init__.py

# Commiter
git add src/load/
git commit -m "feat(load): Add Snowflake loading module

- Connect to Snowflake DWH
- Load transformed data to FACT_SALES table
- Validate successful load"

# Merger dans develop
git checkout develop
git merge --no-ff feature/load-module -m "Merge feature/load-module into develop"
git branch -d feature/load-module</code></pre>
                    </div>
                </div>

                <div class="alert alert-success">
                    <h4>âœ… Checkpoint Phase 2</h4>
                    <p>VÃ©rifiez votre progression :</p>
                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># Visualiser l'historique
git log --oneline --graph --all --decorate

# Devrait montrer:
# * merge feature/load-module
# * commit load module
# * merge feature/transform-module
# * commit transform module
# * merge feature/extract-module
# * commit extract module
# * initial commit</code></pre>
                    </div>
                </div>
            </section>

            <!-- PHASE 3 : ORCHESTRATION ET TESTS -->
            <section class="section">
                <h2>Phase 3 : Orchestration et Tests (15 min)</h2>

                <div class="exercise">
                    <h4>Ã‰tape 3.1 : CrÃ©er le fichier principal main.py</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># Sur develop
git checkout develop

# CrÃ©er main.py
cat > src/main.py << 'EOF'
"""Pipeline ETL principal"""
import argparse
from datetime import datetime
from extract.api_extractor import APIExtractor
from extract.db_extractor import DBExtractor
from transform.data_transformer import DataTransformer
from load.snowflake_loader import SnowflakeLoader

def run_etl_pipeline(date: str):
    """ExÃ©cute le pipeline ETL complet"""
    print(f"ğŸš€ Starting ETL Pipeline for {date}")
    print("=" * 50)

    # Extract
    print("\nğŸ“¥ Step 1: Extraction")
    api_extractor = APIExtractor()
    orders = api_extractor.extract_orders(date)
    api_extractor.save_raw_data(orders, f"orders_{date}.json")

    db_extractor = DBExtractor()
    customers = db_extractor.extract_customers()
    db_extractor.save_to_parquet(customers, "customers.parquet")

    # Transform
    print("\nğŸ”„ Step 2: Transformation")
    transformer = DataTransformer()
    orders_df = transformer.transform_orders(f"data/raw/orders_{date}.json")
    customers_df = transformer.transform_customers("data/raw/customers.parquet")
    final_df = transformer.join_data(orders_df, customers_df)
    transformer.save_transformed_data(final_df, f"sales_enriched_{date}.parquet")

    # Load
    print("\nğŸ“¤ Step 3: Loading")
    loader = SnowflakeLoader()
    loader.load_to_snowflake(final_df, "FACT_SALES")
    result = loader.validate_load("FACT_SALES")

    print("\nâœ… Pipeline completed successfully!")
    print(f"   - Extracted: {len(orders)} orders, {len(customers)} customers")
    print(f"   - Transformed: {len(final_df)} enriched records")
    print(f"   - Loaded: {result['row_count']} records to Snowflake")
    print("=" * 50)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run Sales ETL Pipeline')
    parser.add_argument('--date', type=str, required=True, help='Date (YYYY-MM-DD)')
    args = parser.parse_args()

    run_etl_pipeline(args.date)
EOF

# Commiter
git add src/main.py
git commit -m "feat: Add main ETL orchestration pipeline

- Orchestrate Extract, Transform, Load steps
- Add command-line interface with argparse
- Add logging and progress tracking"</code></pre>
                    </div>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 3.2 : Ajouter des tests (nouvelle branche)</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># CrÃ©er branche test
git checkout -b feature/add-tests

# CrÃ©er tests
cat > tests/test_transformer.py << 'EOF'
"""Tests pour le module de transformation"""
import pytest
import pandas as pd
import sys
sys.path.insert(0, 'src')
from transform.data_transformer import DataTransformer

def test_transform_orders_removes_nulls():
    transformer = DataTransformer()
    # TODO: ImplÃ©menter le test
    assert True

def test_join_data_correct_columns():
    transformer = DataTransformer()
    # TODO: ImplÃ©menter le test
    assert True
EOF

cat > tests/__init__.py << 'EOF'
EOF

# Commiter
git add tests/
git commit -m "test: Add unit tests for transformer module"

# Merger dans develop
git checkout develop
git merge feature/add-tests -m "Merge feature/add-tests into develop"
git branch -d feature/add-tests</code></pre>
                    </div>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 3.3 : Configurer pre-commit hooks</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># CrÃ©er .pre-commit-config.yaml
cat > .pre-commit-config.yaml << 'EOF'
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
        args: ['--maxkb=1000']
      - id: detect-private-key

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black

  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
        args: ['--max-line-length=88']
EOF

# Installer pre-commit (simulation - normalement: pip install pre-commit && pre-commit install)
git add .pre-commit-config.yaml
git commit -m "chore: Configure pre-commit hooks for code quality"</code></pre>
                    </div>
                </div>
            </section>

            <!-- PHASE 4 : HOTFIX ET RELEASE -->
            <section class="section">
                <h2>Phase 4 : Hotfix et Release (10 min)</h2>

                <div class="alert alert-danger">
                    <h4>ğŸš¨ ScÃ©nario : Bug en production !</h4>
                    <p>
                        Un bug critique a Ã©tÃ© dÃ©tectÃ© dans le module d'extraction.
                        Vous devez crÃ©er un <strong>hotfix</strong> urgent.
                    </p>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 4.1 : Hotfix depuis main</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># D'abord, merger develop dans main pour avoir une base
git checkout main
git merge develop -m "Merge develop into main for initial release"

# CrÃ©er branche hotfix
git checkout -b hotfix/fix-api-timeout

# Corriger le bug
cat > src/extract/api_extractor.py << 'EOF'
"""Module d'extraction depuis API REST"""
import requests
import json
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()

class APIExtractor:
    def __init__(self):
        self.api_url = os.getenv('API_URL')
        self.api_key = os.getenv('API_KEY')
        self.timeout = 30  # FIX: Ajout du timeout

    def extract_orders(self, date: str) -> list:
        """Extrait les commandes pour une date donnÃ©e"""
        headers = {'Authorization': f'Bearer {self.api_key}'}
        params = {'date': date}

        try:
            response = requests.get(
                f"{self.api_url}/orders",
                headers=headers,
                params=params,
                timeout=self.timeout  # FIX: Ajout du timeout
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.Timeout:  # FIX: Gestion timeout
            print(f"Error: API timeout after {self.timeout}s")
            return []
        except requests.exceptions.RequestException as e:
            print(f"Error extracting orders: {e}")
            return []

    def save_raw_data(self, data: list, filename: str):
        """Sauvegarde les donnÃ©es brutes"""
        output_path = f"data/raw/{filename}"
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"Saved {len(data)} records to {output_path}")

if __name__ == "__main__":
    extractor = APIExtractor()
    orders = extractor.extract_orders("2024-01-01")
    extractor.save_raw_data(orders, "orders_2024-01-01.json")
EOF

# Commiter le fix
git add src/extract/api_extractor.py
git commit -m "fix(extract): Add timeout handling for API requests

- Add 30s timeout to prevent hanging
- Add specific timeout exception handling
- Fixes production issue #42"

# Merger dans main
git checkout main
git merge hotfix/fix-api-timeout -m "Merge hotfix/fix-api-timeout into main"

# IMPORTANT: Reporter le fix dans develop aussi
git checkout develop
git merge hotfix/fix-api-timeout -m "Merge hotfix/fix-api-timeout into develop"

# Supprimer la branche hotfix
git branch -d hotfix/fix-api-timeout</code></pre>
                    </div>
                </div>

                <div class="exercise">
                    <h4>Ã‰tape 4.2 : CrÃ©er une release avec tag</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># Sur main
git checkout main

# CrÃ©er un tag pour la version 1.0.0
git tag -a v1.0.0 -m "Release version 1.0.0

Features:
- Complete ETL pipeline (Extract, Transform, Load)
- API and Database extraction
- Data transformation and enrichment
- Snowflake loading
- Unit tests
- Pre-commit hooks

Fixes:
- API timeout handling (hotfix #42)"

# Voir tous les tags
git tag -l

# Voir les dÃ©tails du tag
git show v1.0.0</code></pre>
                    </div>
                </div>

                <div class="alert alert-success">
                    <h4>âœ… Checkpoint Final</h4>
                    <p>Visualisez votre magnifique historique Git :</p>
                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># Visualiser tout l'historique
git log --oneline --graph --all --decorate

# Voir les diffÃ©rences entre versions
git diff v1.0.0 develop

# Voir les branches
git branch -a

# Voir les tags
git tag -l</code></pre>
                    </div>
                </div>
            </section>

            <!-- PHASE 5 : COLLABORATION (OPTIONNEL) -->
            <section class="section">
                <h2>Phase 5 : Pousser vers GitHub (Optionnel - 5 min)</h2>

                <div class="exercise">
                    <h4>Ã‰tape 5.1 : CrÃ©er un repo GitHub et pousser</h4>

                    <div class="code-container">
                        <div class="code-header">
                            <span class="code-dot red"></span>
                            <span class="code-dot yellow"></span>
                            <span class="code-dot green"></span>
                        </div>
                        <pre><code class="language-bash"># 1. CrÃ©er un repo sur GitHub: sales-etl-pipeline

# 2. Ajouter le remote
git remote add origin https://github.com/VOTRE_USERNAME/sales-etl-pipeline.git

# 3. Pousser toutes les branches
git push -u origin main
git push -u origin develop

# 4. Pousser les tags
git push --tags

# 5. CrÃ©er une Pull Request sur GitHub
# - De develop vers main
# - Ajouter description dÃ©taillÃ©e
# - Merger aprÃ¨s review (simulation)</code></pre>
                    </div>
                </div>
            </section>

            <!-- RÃ‰CAPITULATIF -->
            <section class="section">
                <div class="key-points">
                    <h3>ğŸ“ RÃ©capitulatif : Ce que vous avez appris</h3>
                    <ul>
                        <li>âœ… Initialiser un projet Data Engineering avec Git</li>
                        <li>âœ… Configurer un .gitignore adaptÃ© Ã  la Data</li>
                        <li>âœ… Utiliser Git Flow (main, develop, feature, hotfix)</li>
                        <li>âœ… CrÃ©er et merger des branches feature</li>
                        <li>âœ… GÃ©rer un hotfix urgent en production</li>
                        <li>âœ… CrÃ©er des tags pour les releases</li>
                        <li>âœ… Configurer des pre-commit hooks</li>
                        <li>âœ… Documenter avec README et commits clairs</li>
                        <li>âœ… Organiser un pipeline ETL complet</li>
                        <li>âœ… Visualiser l'historique avec git log</li>
                    </ul>
                </div>

                <div class="workflow-diagram">
                    <pre>
Votre workflow final :

main â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â–¶ (v1.0.0)
              â†‘               â†‘
              â”‚               â”‚ hotfix
              â”‚               â—
              â”‚              /
develop â”€â”€â—â”€â”€â”€â”´â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
          â†‘       â†‘   â†‘   â†‘
          â”‚       â”‚   â”‚   â”‚
feature/  â”‚       â”‚   â”‚   â”‚
extract â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
                      â”‚   â”‚
feature/              â”‚   â”‚
transform â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜
                          â”‚
feature/                  â”‚
load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    </pre>
                </div>
            </section>

            <div class="alert alert-success">
                <h4>ğŸ‰ Projet terminÃ© ! FÃ©licitations !</h4>
                <p>
                    Vous avez rÃ©ussi Ã  crÃ©er un pipeline ETL complet en utilisant Git de maniÃ¨re professionnelle.
                    Vous Ãªtes maintenant prÃªt Ã  appliquer ces compÃ©tences sur vos projets rÃ©els !
                </p>

                <h4 style="margin-top: 25px;">ğŸ“š Pour aller plus loin :</h4>
                <ul>
                    <li>Ajoutez des tests unitaires complets avec pytest</li>
                    <li>IntÃ©grez GitHub Actions pour le CI/CD</li>
                    <li>Ajoutez un DAG Airflow pour l'orchestration</li>
                    <li>ImplÃ©mentez la connexion rÃ©elle Ã  Snowflake</li>
                    <li>Ajoutez des mÃ©triques et monitoring</li>
                </ul>

                <p style="margin-top: 25px;">
                    <a href="index.html" style="display: inline-block; padding: 15px 30px; background: #0066cc; color: white; text-decoration: none; border-radius: 8px; font-weight: 600;">
                        â† Retour Ã  l'accueil du cours
                    </a>
                </p>
            </div>
        </div>

        <footer>
            <p>Formation Git pour Data Engineering - 2024</p>
            <p>ğŸ¯ Projet Fil Rouge terminÃ© avec succÃ¨s !</p>
        </footer>
    </div>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
