# √âtape 3 : Transformation Bronze ‚Üí Silver

**Dur√©e estim√©e :** 60-75 minutes  
**Difficult√© :** ‚≠ê‚≠ê Moyen

---

## üéØ Objectifs de cette √©tape

√Ä la fin de cette √©tape, vous aurez :

- ‚úÖ Compris les transformations de la couche Silver
- ‚úÖ Cr√©√© un notebook de transformation PySpark
- ‚úÖ Cr√©√© un notebook de transformation SQL (optionnel)
- ‚úÖ Nettoy√© et standardis√© les donn√©es
- ‚úÖ Enrichi les donn√©es avec des colonnes calcul√©es
- ‚úÖ Charg√© les donn√©es transform√©es dans le Lakehouse Silver
- ‚úÖ Versionn√© vos notebooks sur GitHub

---

## üìã Pr√©requis

- ‚úÖ [√âtape 2 : Ingestion Bronze](03_Etape_2_Ingestion_Bronze.md) compl√©t√©e
- ‚úÖ Donn√©es pr√©sentes dans le Lakehouse Bronze (‚â•2 jours)

---

## üìö Comprendre les transformations Silver

### Objectif de la couche Silver

La couche Silver transforme les donn√©es brutes en donn√©es **nettoy√©es, valid√©es et enrichies** :

**Transformations appliqu√©es :**

1. **üî¢ Nettoyage num√©rique**
   - Arrondir `wind_speed` √† 2 d√©cimales
   - Arrondir `energy_produced` √† 2 d√©cimales

2. **üìÖ Enrichissement temporel**
   - Extraire : `day`, `month`, `quarter`, `year`
   - Calculer la p√©riode de la journ√©e (`time_period`)

3. **üïê D√©composition du temps**
   - Extraire : `hour_of_day`, `minute_of_hour`, `second_of_minute`
   - Corriger le format de `time` (remplacer "-" par ":")

4. **‚ú® Calculs m√©tier**
   - `time_period` : Morning (5-11h), Afternoon (12-16h), Evening (17-20h), Night (autres)

---

## üíª T√¢che 1 : Cr√©er le notebook de transformation (PySpark)

### 1.1 - Cr√©er un nouveau Notebook

1. **Dans votre Workspace** `WindPowerAnalytics`
2. **Cliquez sur "+ New item" ‚Üí "Notebook"**
3. **Nom** : `NB_Bronze_To_Silver_Transformations_Python`
4. **Cliquez sur "Create"**

### 1.2 - Attacher les Lakehouses

Vous devez attacher **2 Lakehouses** :

1. **Cliquez sur "Add lakehouse"**
2. **Ajoutez** `LH_Wind_Power_Bronze` (lecture)
3. **Ajoutez** `LH_Wind_Power_Silver` (√©criture)

> üí° Vous pouvez attacher plusieurs Lakehouses √† un m√™me notebook.

---

## üìù T√¢che 2 : √âcrire le code de transformation

### 2.1 - Cellule Markdown : Documentation

```markdown
# Transformation Bronze ‚Üí Silver

## üìã Objectif
Nettoyer, standardiser et enrichir les donn√©es brutes du Lakehouse Bronze.

## üîÑ Transformations appliqu√©es
1. **Nettoyage num√©rique** : Arrondi √† 2 d√©cimales
2. **Enrichissement temporel** : Extraction jour/mois/ann√©e/trimestre
3. **Standardisation** : Correction du format de time
4. **Calcul m√©tier** : P√©riode de la journ√©e bas√©e sur l'heure

## üì¶ D√©pendances
- **Input** : `LH_Wind_Power_Bronze.dbo.wind_power`
- **Output** : `LH_Wind_Power_Silver.dbo.wind_power`

## ‚öôÔ∏è Mode de sauvegarde
- **Mode** : Overwrite (√©crasement complet)
- **Raison** : Simplicit√© pour ce projet p√©dagogique
```

### 2.2 - Cellule 1 : Imports

```python
from pyspark.sql.functions import (
    round, col, dayofmonth, month, year, quarter, 
    substring, when, regexp_replace
)
```

### 2.3 - Cellule 2 : Charger les donn√©es depuis Bronze

```python
# Chemin vers la table Bronze
bronze_table_path = "abfss://WindPowerAnalytics@onelake.dfs.fabric.microsoft.com/LH_Wind_Power_Bronze.Lakehouse/Tables/dbo/wind_power"

# Charger les donn√©es
df = spark.read.format("delta").load(bronze_table_path)

# Afficher le sch√©ma et un aper√ßu
print("üìä Sch√©ma des donn√©es Bronze :")
df.printSchema()

print(f"\nüìà Nombre de lignes : {df.count()}")

print("\nüîç Aper√ßu des 5 premi√®res lignes :")
df.show(5, truncate=False)
```

**Ex√©cutez** cette cellule pour v√©rifier que les donn√©es se chargent correctement.

### 2.4 - Cellule 3 : Appliquer les transformations

```python
# Appliquer toutes les transformations en une seule op√©ration cha√Æn√©e
df_transformed = (df
    # üî¢ Arrondir les valeurs num√©riques √† 2 d√©cimales
    .withColumn("wind_speed", round(col("wind_speed"), 2))
    .withColumn("energy_produced", round(col("energy_produced"), 2))
    
    # üìÖ Extraire les composants de date
    .withColumn("day", dayofmonth(col("date")))
    .withColumn("month", month(col("date")))
    .withColumn("quarter", quarter(col("date")))
    .withColumn("year", year(col("date")))
    
    # üïê Corriger le format de time (remplacer - par :)
    .withColumn("time", regexp_replace(col("time"), "-", ":"))
    
    # ‚è∞ Extraire les composants de temps
    .withColumn("hour_of_day", substring(col("time"), 1, 2).cast("int"))
    .withColumn("minute_of_hour", substring(col("time"), 4, 2).cast("int"))
    .withColumn("second_of_minute", substring(col("time"), 7, 2).cast("int"))
    
    # üåÖ Calculer la p√©riode de la journ√©e
    .withColumn("time_period", 
        when((col("hour_of_day") >= 5) & (col("hour_of_day") < 12), "Morning")
        .when((col("hour_of_day") >= 12) & (col("hour_of_day") < 17), "Afternoon")
        .when((col("hour_of_day") >= 17) & (col("hour_of_day") < 21), "Evening")
        .otherwise("Night")
    )
)

print("‚úÖ Transformations appliqu√©es avec succ√®s !")
```

### 2.5 - Cellule 4 : Afficher un √©chantillon transform√©

```python
# Afficher un √©chantillon des donn√©es transform√©es
print("üìä Aper√ßu des donn√©es transform√©es :")
df_transformed.select(
    "date", "time", "turbine_name", 
    "wind_speed", "energy_produced", 
    "day", "month", "year", "quarter",
    "hour_of_day", "time_period"
).show(10)

print(f"\nüìà Nombre de colonnes : {len(df_transformed.columns)}")
print(f"üìã Nouvelles colonnes ajout√©es : day, month, quarter, year, hour_of_day, minute_of_hour, second_of_minute, time_period")
```

### 2.6 - Cellule 5 : V√©rifications de qualit√©

```python
from pyspark.sql.functions import count, when, isnan, col, min as spark_min, max as spark_max

# V√©rifier qu'il n'y a pas de valeurs nulles dans les colonnes critiques
print("=== üîç V√©rification des valeurs nulles ===")
null_counts = df_transformed.select([
    count(when(col(c).isNull(), c)).alias(c) 
    for c in ["wind_speed", "energy_produced", "day", "month", "year", "time_period"]
])
null_counts.show()

# V√©rifier les valeurs uniques de time_period
print("\n=== üìä Distribution des p√©riodes de la journ√©e ===")
df_transformed.groupBy("time_period").count().orderBy("count", ascending=False).show()

# V√©rifier les plages de dates
print("\n=== üìÖ Plage de dates ===")
df_transformed.select(
    spark_min("date").alias("Date minimale"),
    spark_max("date").alias("Date maximale")
).show()

# Statistiques descriptives
print("\n=== üìà Statistiques sur les mesures ===")
df_transformed.select("wind_speed", "energy_produced").describe().show()
```

### 2.7 - Cellule 6 : Sauvegarder dans Silver

```python
# Chemin vers la table Silver
silver_table_path = "abfss://WindPowerAnalytics@onelake.dfs.fabric.microsoft.com/LH_Wind_Power_Silver.Lakehouse/Tables/dbo/wind_power"

# Sauvegarder en mode overwrite (√©crasement complet)
df_transformed.write.format("delta").mode("overwrite").save(silver_table_path)

print("‚úÖ Donn√©es transform√©es et sauvegard√©es dans Silver")
print(f"üìä Nombre de lignes sauvegard√©es : {df_transformed.count()}")
```

---

## üóÉÔ∏è T√¢che 3 : Cr√©er la version SQL (Optionnel mais recommand√©)

### 3.1 - Cr√©er un nouveau Notebook SQL

1. **Cr√©ez un nouveau Notebook** : `NB_Bronze_To_Silver_Transformations_SQL`
2. **Attachez les 2 Lakehouses** (Bronze et Silver)

### 3.2 - Cellule 1 : Cr√©er une vue temporaire

```sql
%%sql
-- Cr√©er une vue temporaire de la table Bronze
CREATE OR REPLACE TEMPORARY VIEW bronze_wind_power AS
SELECT *
FROM WindPowerAnalytics.LH_Wind_Power_Bronze.dbo.wind_power;
```

### 3.3 - Cellule 2 : Appliquer les transformations SQL

```sql
%%sql
-- Nettoyer et enrichir les donn√©es
CREATE OR REPLACE TEMPORARY VIEW transformed_wind_power AS
SELECT
    production_id,
    date,
    turbine_name,
    capacity,
    location_name,
    latitude,
    longitude,
    region,
    status,
    responsible_department,
    wind_direction,
    
    -- üî¢ Arrondi des valeurs num√©riques
    ROUND(wind_speed, 2) AS wind_speed,
    ROUND(energy_produced, 2) AS energy_produced,
    
    -- üìÖ Extraction des composants de date
    DAY(date) AS day,
    MONTH(date) AS month,
    QUARTER(date) AS quarter,
    YEAR(date) AS year,
    
    -- üïê Correction du format de time
    REGEXP_REPLACE(time, '-', ':') AS time,
    
    -- ‚è∞ Extraction des composants de temps
    CAST(SUBSTRING(time, 1, 2) AS INT) AS hour_of_day,
    CAST(SUBSTRING(time, 4, 2) AS INT) AS minute_of_hour,
    CAST(SUBSTRING(time, 7, 2) AS INT) AS second_of_minute,
    
    -- üåÖ Calcul de la p√©riode de la journ√©e
    CASE
        WHEN CAST(SUBSTRING(time, 1, 2) AS INT) BETWEEN 5 AND 11 THEN 'Morning'
        WHEN CAST(SUBSTRING(time, 1, 2) AS INT) BETWEEN 12 AND 16 THEN 'Afternoon'
        WHEN CAST(SUBSTRING(time, 1, 2) AS INT) BETWEEN 17 AND 20 THEN 'Evening'
        ELSE 'Night'
    END AS time_period
    
FROM bronze_wind_power;
```

### 3.4 - Cellule 3 : Supprimer l'ancienne table Silver

```sql
%%sql
-- Supprimer l'ancienne table Silver si elle existe
DROP TABLE IF EXISTS WindPowerAnalytics.LH_Wind_Power_Silver.dbo.wind_power;
```

### 3.5 - Cellule 4 : Cr√©er la nouvelle table Silver

```sql
%%sql
-- Cr√©er la nouvelle table Silver avec les donn√©es transform√©es
CREATE TABLE WindPowerAnalytics.LH_Wind_Power_Silver.dbo.wind_power
USING delta
AS
SELECT * FROM transformed_wind_power;
```

### 3.6 - Cellule 5 : V√©rification

```sql
%%sql
-- V√©rifier que la table a √©t√© cr√©√©e avec succ√®s
SELECT 
    COUNT(*) as total_rows,
    MIN(date) as min_date,
    MAX(date) as max_date,
    COUNT(DISTINCT turbine_name) as turbine_count
FROM WindPowerAnalytics.LH_Wind_Power_Silver.dbo.wind_power;
```

---

## ‚úÖ T√¢che 4 : V√©rifier les donn√©es dans Silver

### 4.1 - Explorer le Lakehouse Silver

1. **Ouvrez le Lakehouse** `LH_Wind_Power_Silver`
2. **Naviguez vers** Tables ‚Üí dbo ‚Üí wind_power
3. **Cliquez sur la table** pour voir son contenu

### 4.2 - V√©rifier les nouvelles colonnes

Vous devriez voir les colonnes suivantes ajout√©es :
- `day`, `month`, `quarter`, `year`
- `hour_of_day`, `minute_of_hour`, `second_of_minute`
- `time_period`

**üì∏ Capture d'√©cran √† prendre :** `03_silver_table_schema.png`

### 4.3 - Comparer avec Bronze

| Aspect | Bronze | Silver |
|--------|--------|--------|
| Nombre de colonnes | 14 | 22 (+8) |
| Format time | avec "-" | avec ":" |
| Valeurs num√©riques | Brutes | Arrondies |
| Colonnes temporelles | Basiques | Enrichies |

---

## üóÇÔ∏è T√¢che 5 : Versionner sur GitHub

### 5.1 - T√©l√©charger les notebooks

1. **Notebook PySpark** : T√©l√©chargez `NB_Bronze_To_Silver_Transformations_Python.ipynb`
2. **Notebook SQL** (si cr√©√©) : T√©l√©chargez `NB_Bronze_To_Silver_Transformations_SQL.ipynb`

### 5.2 - Uploader sur GitHub

1. **GitHub** ‚Üí `notebooks/silver/`
2. **Uploadez les 2 notebooks**
3. **Commit message** : `feat: Add Bronze to Silver transformation notebooks`

**üì∏ Capture d'√©cran √† prendre :** `03_github_silver_notebooks.png`

---

## ‚úÖ V√©rification de l'√©tape

- [ ] ‚úÖ Notebook PySpark cr√©√© et fonctionnel
- [ ] ‚úÖ (Optionnel) Notebook SQL cr√©√© et fonctionnel
- [ ] ‚úÖ Table `wind_power` cr√©√©e dans Silver avec 22 colonnes
- [ ] ‚úÖ Donn√©es nettoy√©es (valeurs arrondies)
- [ ] ‚úÖ Donn√©es enrichies (colonnes temporelles)
- [ ] ‚úÖ Format de time corrig√© (: au lieu de -)
- [ ] ‚úÖ V√©rifications de qualit√© effectu√©es
- [ ] ‚úÖ Notebooks versionn√©s sur GitHub
- [ ] ‚úÖ 2 captures d'√©cran prises

---

## üéì Ce que vous avez appris

- ‚úÖ Utiliser les fonctions PySpark pour transformer des donn√©es
- ‚úÖ Cr√©er des colonnes calcul√©es avec `withColumn()`
- ‚úÖ Utiliser les fonctions temporelles (day, month, quarter, year)
- ‚úÖ Appliquer des conditions avec `when().otherwise()`
- ‚úÖ Utiliser les expressions r√©guli√®res avec `regexp_replace()`
- ‚úÖ Faire la m√™me chose en SQL pour comparer les approches
- ‚úÖ Effectuer des v√©rifications de qualit√© de donn√©es

---

## üìä Comparaison PySpark vs SQL

| Aspect | PySpark | SQL |
|--------|---------|-----|
| **Syntaxe** | M√©thodes chain√©es | D√©claratif |
| **Lisibilit√©** | Peut √™tre verbeux | Tr√®s lisible |
| **Performance** | Identique | Identique |
| **Flexibilit√©** | Tr√®s √©lev√©e | Bonne |
| **Cas d'usage** | Transformations complexes | Requ√™tes simples |

> üí° **Conseil** : Utilisez PySpark pour les transformations complexes et SQL pour les requ√™tes simples et lisibles.

---

## ‚ö†Ô∏è Probl√®mes courants et solutions

### Probl√®me 1 : "Colonne not found"

**Cause** : Le sch√©ma de Bronze ne correspond pas au code.

**Solution** :
```python
# V√©rifiez les colonnes disponibles
df.printSchema()
df.columns
```

### Probl√®me 2 : "time_period avec des valeurs NULL"

**Cause** : La logique de CASE/when ne couvre pas tous les cas.

**Solution** : V√©rifiez que vous avez un `otherwise()` ou `ELSE` pour les cas non couverts.

### Probl√®me 3 : "Performance lente"

**Cause** : Trop de donn√©es √† traiter.

**Solution** :
```python
# Utilisez le caching pour √©viter de recalculer
df_transformed.cache()
df_transformed.count()  # Force l'√©valuation
```

---

## üéØ Prochaine √©tape

Excellent ! Vos donn√©es sont maintenant nettoy√©es et enrichies dans la couche Silver.

‚û°Ô∏è **[√âtape 4 : Transformation Silver ‚Üí Gold](05_Etape_4_Transformation_Gold.md)**

Dans la prochaine √©tape, vous allez cr√©er le mod√®le dimensionnel (star schema) dans la couche Gold.

---

*√âtape 3 compl√©t√©e ‚úÖ | Temps : ~75 min | Total cumul√© : ~210 min (~3h30)*
