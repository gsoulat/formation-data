# Ã‰tape 2 : Ingestion des donnÃ©es (Bronze Layer)

**DurÃ©e estimÃ©e :** 45-60 minutes  
**DifficultÃ© :** â­â­ Moyen

---

## ğŸ¯ Objectifs de cette Ã©tape

Ã€ la fin de cette Ã©tape, vous aurez :

- âœ… CrÃ©Ã© votre premier Notebook PySpark
- âœ… Compris la logique d'ingestion incrÃ©mentale
- âœ… TÃ©lÃ©chargÃ© des donnÃ©es CSV depuis GitHub
- âœ… ChargÃ© les donnÃ©es dans le Lakehouse Bronze
- âœ… InitialisÃ© la table `wind_power` dans Bronze
- âœ… VersionnÃ© votre notebook sur GitHub

---

## ğŸ“‹ PrÃ©requis

- âœ… [Ã‰tape 1 : CrÃ©ation des Lakehouses](02_Etape_1_Creation_Lakehouses.md) complÃ©tÃ©e
- âœ… Les 3 Lakehouses (Bronze, Silver, Gold) crÃ©Ã©s

---

## ğŸ“š Comprendre l'ingestion incrÃ©mentale

### Pourquoi l'ingestion incrÃ©mentale ?

Dans un contexte rÃ©el, les donnÃ©es arrivent progressivement (quotidiennement, horaire, etc.). L'ingestion incrÃ©mentale permet de :

- ğŸ’° **RÃ©duire les coÃ»ts** : On ne recharge pas tout Ã  chaque fois
- âš¡ **AmÃ©liorer les performances** : Traitements plus rapides
- ğŸ”„ **Faciliter la maintenance** : Moins de risques d'erreurs

### Logique de notre ingestion

Notre notebook va :
1. **Lire** la date la plus rÃ©cente dans le Lakehouse Bronze
2. **Calculer** le jour suivant (date + 1)
3. **TÃ©lÃ©charger** le fichier CSV correspondant depuis GitHub
4. **Ajouter** les nouvelles donnÃ©es (mode append)

---

## ğŸ¨ TÃ¢che 1 : CrÃ©er le Notebook d'ingestion

### 1.1 - CrÃ©er un nouveau Notebook

1. **Dans votre Workspace** `WindPowerAnalytics`
2. **Cliquez sur "+ New item"**
3. **SÃ©lectionnez "Notebook"**
4. **Nom** : `NB_Get_Daily_Data_Python`
5. **Cliquez sur "Create"**

### 1.2 - Attacher le Lakehouse Bronze

Un notebook doit Ãªtre attachÃ© Ã  un Lakehouse pour pouvoir y Ã©crire des donnÃ©es.

1. **Dans le panneau de gauche** du notebook, recherchez **"Add lakehouse"**
2. **Cliquez sur "Add"**
3. **SÃ©lectionnez "Existing lakehouse"**
4. **Choisissez** `LH_Wind_Power_Bronze`
5. **Cliquez sur "Add"**

> ğŸ’¡ Vous devriez maintenant voir le Lakehouse Bronze dans le panneau de gauche avec ses sections Files et Tables.

---

## ğŸ’» TÃ¢che 2 : Ã‰crire le code d'ingestion

### 2.1 - Cellule 1 : Imports

CrÃ©ez une premiÃ¨re cellule et ajoutez :

```python
import requests
import pandas as pd
from datetime import datetime, timedelta
```

**ExÃ©cutez** cette cellule (Shift+Enter ou cliquez sur le bouton Play).

### 2.2 - Cellule 2 : Configuration

Ajoutez une nouvelle cellule :

```python
# URL de base du repository GitHub
base_url = "https://raw.githubusercontent.com/gsoulat/data-training-fabric/main/eolienne/"

# Chemin vers la table Bronze
bronze_table_path = "abfss://WindPowerAnalytics@onelake.dfs.fabric.microsoft.com/LH_Wind_Power_Bronze.Lakehouse/Tables/dbo/wind_power"
```

**ExÃ©cutez** cette cellule.

> ğŸ“š **Explication** : `abfss://` est le protocole Azure Blob File System utilisÃ© par OneLake.

### 2.3 - Cellule 3 : Initialisation (PREMIÃˆRE FOIS UNIQUEMENT)

Ajoutez une nouvelle cellule pour charger le premier fichier :

```python
# âš ï¸ CELLULE D'INITIALISATION - Ã€ exÃ©cuter UNE SEULE FOIS
# Cette cellule crÃ©e la table Bronze avec le premier fichier de donnÃ©es

initial_date = "20240601"  # Premier jour disponible
initial_url = f"{base_url}{initial_date}_wind_power_data.csv"

print(f"ğŸ“¥ TÃ©lÃ©chargement du fichier initial : {initial_url}")

# TÃ©lÃ©charger et charger le premier fichier
df_initial = pd.read_csv(initial_url)
df_initial['date'] = pd.to_datetime(df_initial['date'])

print(f"âœ… Fichier chargÃ© : {len(df_initial)} lignes")
print(f"ğŸ“Š AperÃ§u des donnÃ©es :")
print(df_initial.head())

# Convertir en Spark DataFrame et sauvegarder
df_spark_initial = spark.createDataFrame(df_initial)
df_spark_initial.write.format('delta').mode("overwrite").save(bronze_table_path)

print("âœ… Table Bronze initialisÃ©e avec succÃ¨s !")
```

**ExÃ©cutez** cette cellule. Cela va prendre environ 30-60 secondes.

> âš ï¸ **IMPORTANT** : AprÃ¨s avoir exÃ©cutÃ© cette cellule UNE FOIS, vous pouvez la commenter ou la supprimer pour Ã©viter de rÃ©initialiser la table par accident.

### 2.4 - VÃ©rification de l'initialisation

Ajoutez une cellule pour vÃ©rifier :

```python
# VÃ©rifier que la table existe et afficher quelques statistiques
df_check = spark.read.format("delta").load(bronze_table_path)

print(f"ğŸ“Š Nombre total de lignes : {df_check.count()}")
print(f"ğŸ“… Colonnes disponibles : {df_check.columns}")
print(f"ğŸ” AperÃ§u des 5 premiÃ¨res lignes :")
df_check.show(5)
```

**ExÃ©cutez** cette cellule.

Vous devriez voir environ 259 200 lignes (3 turbines Ã— 86 400 secondes par jour).

### 2.5 - Cellule 4 : Chargement des donnÃ©es existantes

Maintenant, crÃ©ons la logique incrÃ©mentale. Ajoutez une nouvelle cellule :

```python
# Charger les donnÃ©es existantes depuis Bronze
df_spark = spark.read.format("delta").load(bronze_table_path)

# Convertir en Pandas pour manipulation plus facile
df_pandas = df_spark.toPandas()

print(f"ğŸ“Š DonnÃ©es actuelles dans Bronze : {len(df_pandas)} lignes")
```

### 2.6 - Cellule 5 : Identification de la prochaine date

```python
# Trouver la date la plus rÃ©cente dans les donnÃ©es
most_recent_date = pd.to_datetime(df_pandas['date'], format="%Y%m%d").max()

# Calculer le jour suivant
next_date = (most_recent_date + timedelta(days=1)).strftime("%Y%m%d")

print(f"ğŸ“… Date la plus rÃ©cente : {most_recent_date.strftime('%Y-%m-%d')}")
print(f"â¡ï¸  Prochaine date Ã  charger : {next_date}")
```

### 2.7 - Cellule 6 : TÃ©lÃ©chargement des nouvelles donnÃ©es

```python
# Construire l'URL du fichier CSV
file_url = f"{base_url}{next_date}_wind_power_data.csv"
print(f"ğŸŒ URL du fichier : {file_url}")

try:
    # TÃ©lÃ©charger le CSV depuis GitHub
    df_pandas_new = pd.read_csv(file_url)
    
    # Convertir la colonne date en datetime
    df_pandas_new['date'] = pd.to_datetime(df_pandas_new['date'])
    
    print(f"âœ… Nouvelles donnÃ©es tÃ©lÃ©chargÃ©es : {len(df_pandas_new)} lignes")
    print(f"ğŸ“Š AperÃ§u des nouvelles donnÃ©es :")
    print(df_pandas_new.head())
    
except Exception as e:
    print(f"âŒ Erreur lors du tÃ©lÃ©chargement : {e}")
    print(f"ğŸ’¡ Cela peut signifier que le fichier pour la date {next_date} n'existe pas encore.")
```

### 2.8 - Cellule 7 : Sauvegarde dans Bronze

```python
# Convertir le DataFrame Pandas en Spark DataFrame
df_spark_new = spark.createDataFrame(df_pandas_new, schema=df_spark.schema)

# Ajouter les nouvelles donnÃ©es Ã  la table Bronze (mode append)
df_spark_new.write.format('delta').mode("append").save(bronze_table_path)

print("âœ… DonnÃ©es ajoutÃ©es avec succÃ¨s dans le Lakehouse Bronze")
print(f"ğŸ“Š Total aprÃ¨s ajout : {spark.read.format('delta').load(bronze_table_path).count()} lignes")
```

---

## ğŸ“ TÃ¢che 3 : Documenter le notebook

### 3.1 - Ajouter une cellule Markdown au dÃ©but

En haut du notebook, ajoutez une cellule Markdown (cliquez sur "+ Code" puis changez le type en "Markdown") :

```markdown
# Notebook : Ingestion quotidienne de donnÃ©es Ã©oliennes

## ğŸ“‹ Objectif
Charger de maniÃ¨re incrÃ©mentale les donnÃ©es de production Ã©olienne depuis GitHub vers le Lakehouse Bronze.

## ğŸ”„ Logique
1. Identifier la date la plus rÃ©cente dans Bronze
2. Calculer le jour suivant (date + 1)
3. TÃ©lÃ©charger le fichier CSV correspondant depuis GitHub
4. Ajouter les nouvelles donnÃ©es en mode append

## âš™ï¸ ExÃ©cution
- **FrÃ©quence recommandÃ©e** : Quotidienne (via pipeline)
- **DurÃ©e moyenne** : 50-60 secondes
- **DÃ©pendances** : Repository GitHub public accessible

## ğŸ“¦ DÃ©pendances
- Lakehouse : LH_Wind_Power_Bronze
- Source : https://github.com/gsoulat/data-training-fabric/tree/main/eolienne
```

---

## âœ… TÃ¢che 4 : Tester le notebook

### 4.1 - ExÃ©cution complÃ¨te

1. **Cliquez sur "Run all"** en haut du notebook
2. **Attendez** que toutes les cellules s'exÃ©cutent (environ 1-2 minutes)
3. **VÃ©rifiez** qu'il n'y a pas d'erreurs

### 4.2 - VÃ©rifier les donnÃ©es dans Bronze

1. **Retournez au Lakehouse Bronze**
2. **Naviguez vers Tables â†’ dbo â†’ wind_power**
3. **Cliquez sur la table** pour voir son contenu
4. **Notez le nombre de lignes**

Si tout s'est bien passÃ©, vous devriez avoir environ 518 400 lignes (2 jours de donnÃ©es).

**ğŸ“¸ Capture d'Ã©cran Ã  prendre :** `02_bronze_table_data.png`

---

## ğŸ—‚ï¸ TÃ¢che 5 : Versionner sur GitHub

### 5.1 - TÃ©lÃ©charger le notebook

1. **Dans le notebook**, cliquez sur **"File" â†’ "Download as .ipynb"**
2. **Sauvegardez** le fichier sur votre ordinateur

### 5.2 - Uploader sur GitHub

1. **Allez sur GitHub** â†’ votre repository `fabric-wind-power-pipeline`
2. **Naviguez vers** `notebooks/bronze/`
3. **Cliquez sur "Add file" â†’ "Upload files"**
4. **Glissez-dÃ©posez** votre fichier `.ipynb`
5. **Commit message** : `feat: Add Bronze layer ingestion notebook`
6. **Cliquez sur "Commit changes"**

**ğŸ“¸ Capture d'Ã©cran Ã  prendre :** `02_github_notebook_uploaded.png`

---

## âœ… VÃ©rification de l'Ã©tape

- [ ] âœ… Notebook `NB_Get_Daily_Data_Python` crÃ©Ã©
- [ ] âœ… Lakehouse Bronze attachÃ© au notebook
- [ ] âœ… Table `wind_power` initialisÃ©e dans Bronze
- [ ] âœ… Logique incrÃ©mentale implÃ©mentÃ©e et testÃ©e
- [ ] âœ… Au moins 2 jours de donnÃ©es chargÃ©s (â‰ˆ518k lignes)
- [ ] âœ… Notebook documentÃ© avec cellule Markdown
- [ ] âœ… Notebook versionnÃ© sur GitHub
- [ ] âœ… 2 captures d'Ã©cran prises

---

## ğŸ“ Ce que vous avez appris

- âœ… CrÃ©er et configurer un Notebook dans Fabric
- âœ… Attacher un Lakehouse Ã  un Notebook
- âœ… Utiliser Pandas pour tÃ©lÃ©charger des donnÃ©es depuis une URL
- âœ… Convertir entre Pandas DataFrame et Spark DataFrame
- âœ… Ã‰crire des donnÃ©es au format Delta Lake
- âœ… ImplÃ©menter une logique d'ingestion incrÃ©mentale
- âœ… Versionner du code sur GitHub

---

## ğŸ¯ Prochaine Ã©tape

â¡ï¸ **[Ã‰tape 3 : Transformation Bronze â†’ Silver](04_Etape_3_Transformation_Silver.md)**

*Ã‰tape 2 complÃ©tÃ©e âœ… | Temps : ~60 min | Total cumulÃ© : ~135 min*
