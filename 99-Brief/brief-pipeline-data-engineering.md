# Brief : Pipeline Data Engineering - NYC Taxi Data

## Vue d'ensemble du projet

Ce projet consiste √† construire une pipeline data compl√®te pour l'ingestion, le traitement et l'analyse des donn√©es de taxis de New York (NYC Taxi Trip Records). Vous allez progressivement d√©velopper une infrastructure moderne de data engineering.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ARCHITECTURE FINALE                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    NYC Open Data (2025)
           ‚îÇ
           ‚îÇ ‚ë† T√©l√©chargement
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Parquet     ‚îÇ
    ‚îÇ   Files      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ ‚ë° Ingestion
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    
    ‚îÇ    Azure     ‚îÇ
    ‚îÇ   PostgreSQL ‚îÇ
    ‚îÇ   (Brut)     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ ‚ë¢ Clean
           ‚îÇ 
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ DLT Pipeline ‚îÇ ‚ë£  DLT Migration
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ ‚ë§ D√©ploiement
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ    Azure     ‚îÇ
    ‚îÇ  MongoDB     ‚îÇ
    ‚îÇ  (Nettoy√©)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ ‚ë• Automatisation
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  GitHub      ‚îÇ
    ‚îÇ  Actions     ‚îÇ
    ‚îÇ  (Mensuel)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Objectifs p√©dagogiques

- Ma√Ætriser Git/GitHub avec un workflow professionnel (branches, PRs, semantic release)
- D√©velopper des scripts Python robustes pour l'ingestion de donn√©es
- Travailler avec plusieurs bases de donn√©es (DuckDB, PostgreSQL, MongoDB)
- Conteneuriser une application avec Docker
- Cr√©er une API REST avec FastAPI
- Moderniser une pipeline avec DLT (Data Load Tool)
- D√©ployer sur Azure avec automatisation

## Pr√©requis

- Git et GitHub CLI install√©s
- Python 3.9+ install√©
- Docker et Docker Compose install√©s
- Compte GitHub
- Compte Azure (pour l'√©tape finale)
- Connaissances de base en Python, SQL et Git

## Source des donn√©es

**URL des donn√©es** : https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

**Format** : Fichiers Parquet (Yellow Taxi Trip Records)

**Exemple de fichier** : https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet

**Donn√©es √† utiliser** : Uniquement l'ann√©e 2025

**Structure des donn√©es** :
```
Colonnes principales :
- VendorID : Identifiant du fournisseur
- tpep_pickup_datetime : Date/heure de prise en charge
- tpep_dropoff_datetime : Date/heure de d√©pose
- passenger_count : Nombre de passagers
- trip_distance : Distance du trajet (miles)
- fare_amount : Montant du tarif
- tip_amount : Montant du pourboire
- total_amount : Montant total
- payment_type : Type de paiement
- ...et d'autres colonnes
```

---

## Jour 1 : Configuration Git/GitHub et Ingestion des donn√©es

### Sch√©ma de l'√©tape

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  JOUR 1 : Git + Download                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

GitHub Repo
    ‚îÇ
    ‚îú‚îÄ‚îÄ main (prot√©g√©e)
    ‚îÇ
    ‚îî‚îÄ‚îÄ develop (prot√©g√©e, ne se supprime pas)
         ‚îÇ
         ‚îî‚îÄ‚îÄ feature/semantic-release
         ‚îÇ        ‚îÇ
         ‚îÇ        ‚îî‚îÄ‚ñ∂ Workflow release.yml
         ‚îÇ
         ‚îî‚îÄ‚îÄ feature/data-ingestion
                  ‚îÇ
                  ‚îî‚îÄ‚ñ∂ Script Python download_data.py
                       ‚îÇ
                       ‚ñº
                  data/raw/*.parquet
```

### √âtape 1.1 : Initialiser le projet GitHub (1h)

**üéØ Objectifs** :
- Cr√©er un repository public avec la GitHub CLI
- Configurer les branches et la protection
- Mettre en place Semantic Release

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er le repository via github-cli**
 

2. **Cr√©er un README.md initial**
   - Titre du projet
   - Description concise
   - Liste des technologies utilis√©es (Python, Docker, PostgreSQL, MongoDB, FastAPI, DLT, Azure)

3. **Cr√©er et prot√©ger la branch develop**
   ```bash
   git checkout -b develop
   git push -u origin develop
   ```

   Utiliser la commande `gh api` ou via `github` pour prot√©ger la branch develop :


**üí° Indices** :
- La branch `develop` doit √™tre la branch par d√©faut pour le d√©veloppement
- La branch `main` sera utilis√©e pour les releases
- La protection emp√™che la suppression accidentelle

### √âtape 1.2 : Configuration Semantic Release (1h)

**üéØ Objectifs** :
- Automatiser le versioning avec Semantic Release
- Cr√©er un workflow GitHub Actions pour les releases

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/semantic-release
   ```

2. **Cr√©er le fichier `.github/workflows/release.yml`**


4. **Cr√©er un `.gitignore` complet**

```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# Data files
*.parquet
*.csv
data/

# Databases
*.db
*.duckdb
*.duckdb.wal

# Environment variables
.env
.env.local

# IDEs
.vscode/
.idea/
*.swp
*.swo

# Node
node_modules/
package-lock.json

# Docker
docker-compose.override.yml

# OS
.DS_Store
Thumbs.db

# DLT
.dlt/secrets.toml
.dlt/.sources
```

5. **Commit et merge**
   ```bash
   git add .
   git commit -m "feat: add semantic release workflow and gitignore"
   git push -u origin feature/semantic-release
   git checkout develop
   git merge feature/semantic-release
   git push origin develop
   ```

**üí° Indices** :
- Semantic Release g√©n√®re automatiquement les versions bas√©es sur les commits
- Les commits doivent suivre la convention : `feat:`, `fix:`, `docs:`, `chore:`
- Le workflow ne se d√©clenche que sur la branch `main`

### √âtape 1.3 : T√©l√©chargement des donn√©es Parquet (2-3h)

**üéØ Objectifs** :
- Cr√©er un script Python pour t√©l√©charger les fichiers Parquet de 2025
- √âviter les t√©l√©chargements en double
- G√©rer les erreurs r√©seau

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/data-ingestion
   ```

2. **Cr√©er la structure du projet**
   ```bash
   mkdir -p src/data
   mkdir -p data/raw
   ```

3. **Cr√©er le fichier `requirements.txt`**
   ```
   requests==2.31.0
   pandas==2.2.0
   pyarrow==15.0.0
   fastparquet==2024.2.0
   ```

4. **D√©velopper `src/download_data.py`**

Votre script doit contenir :

**Classe `NYCTaxiDataDownloader`** avec les m√©thodes suivantes :

- `__init__()` :
  - D√©finir les constantes : `BASE_URL`, `YEAR`, `DATA_DIR`
  - Cr√©er le r√©pertoire de destination si n√©cessaire

- `get_file_path(month: int) -> Path` :
  - Construire le chemin du fichier pour un mois donn√©
  - Format : `yellow_tripdata_YYYY-MM.parquet`

- `file_exists(month: int) -> bool` :
  - V√©rifier si le fichier existe d√©j√† localement
  - Retourner True/False

- `download_month(month: int) -> bool` :
  - V√©rifier si le fichier existe d√©j√† (utiliser `file_exists()`)
  - Si oui, afficher un message et retourner True
  - Sinon, t√©l√©charger le fichier depuis `BASE_URL`
  - Utiliser `requests.get()` avec `stream=True`
  - Afficher une barre de progression (optionnel)
  - G√©rer les erreurs avec try/except
  - En cas d'erreur, supprimer le fichier partiel

- `download_all_available() -> list` :
  - D√©terminer le mois actuel
  - Boucler de janvier au mois actuel (si ann√©e 2025)
  - Appeler `download_month()` pour chaque mois
  - Retourner la liste des fichiers t√©l√©charg√©s
  - Afficher un r√©sum√©

**üí° Indices** :
- Utilisez `pathlib.Path` pour la gestion des chemins
- `requests.get(url, stream=True, timeout=30)` pour le t√©l√©chargement
- `response.iter_content(chunk_size=8192)` pour lire par morceaux
- `datetime.now().month` pour obtenir le mois actuel
- N'oubliez pas de g√©rer les exceptions `requests.exceptions.RequestException`

**Structure attendue** :
```
src/download_data.py
   ‚îÇ
   ‚îú‚îÄ class NYCTaxiDataDownloader:
   ‚îÇ     ‚îú‚îÄ __init__(self)
   ‚îÇ     ‚îú‚îÄ get_file_path(self, month)
   ‚îÇ     ‚îú‚îÄ file_exists(self, month)
   ‚îÇ     ‚îú‚îÄ download_month(self, month)
   ‚îÇ     ‚îî‚îÄ download_all_available(self)
   ‚îÇ
   ‚îî‚îÄ if __name__ == "__main__":
         ‚îî‚îÄ Cr√©er instance et t√©l√©charger
```

5. **Tester le script**
   ```bash
   python -m venv venv
   source venv/bin/activate  # ou venv\Scripts\activate sur Windows
   pip install -r requirements.txt
   python src/download_data.py
   ```

6. **Commit et merge**
   ```bash
   git add .
   git commit -m "feat: add NYC taxi data downloader with duplicate prevention"
   git push -u origin feature/data-ingestion
   git checkout develop
   git merge feature/data-ingestion
   git push origin develop
   ```

**üîç Points de v√©rification** :
- [ ] Les fichiers se t√©l√©chargent dans `data/raw/`
- [ ] Les fichiers d√©j√† pr√©sents ne sont pas re-t√©l√©charg√©s
- [ ] Les erreurs r√©seau sont g√©r√©es proprement
- [ ] Un message de progression s'affiche
- [ ] Un r√©sum√© final est affich√©

---

## Jour 2 : Import dans DuckDB et Transition vers Docker

### Sch√©ma de l'√©tape

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  JOUR 2 : DuckDB + Docker + PostgreSQL      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

data/raw/*.parquet
      ‚îÇ
      ‚îÇ ‚ë† Import
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   DuckDB    ‚îÇ  (Fichier local)
‚îÇ   .duckdb   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

      ‚îÇ ‚ë° Migration
      ‚ñº

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Docker Compose                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇPostgreSQL‚îÇ    ‚îÇ   App    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  :5432   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÇ  :8000   ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### √âtape 2.1 : Import des donn√©es dans DuckDB (2h)

**üéØ Objectifs** :
- Cr√©er un script pour importer les fichiers Parquet dans DuckDB
- √âviter les imports en double avec une table de log
- Ne pas versionner la base de donn√©es

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/duckdb-import
   ```

2. **Ajouter DuckDB aux d√©pendances**
   Ajouter dans `requirements.txt` :
   ```
   duckdb==0.10.0
   ```

3. **D√©velopper `src/import_to_duckdb.py`**

**Classe `DuckDBImporter`** avec les m√©thodes suivantes :

- `__init__(db_path: str)` :
  - Se connecter √† DuckDB
  - Appeler `_initialize_database()`

- `_initialize_database()` :
  - Cr√©er la table `yellow_taxi_trips` si elle n'existe pas
  - Cr√©er la table `import_log` pour tracker les imports
  - La table `import_log` doit contenir : `file_name`, `import_date`, `rows_imported`

- `is_file_imported(filename: str) -> bool` :
  - V√©rifier si le fichier est d√©j√† dans `import_log`
  - Retourner True/False

- `import_parquet(file_path: Path) -> bool` :
  - V√©rifier si d√©j√† import√© (utiliser `is_file_imported()`)
  - Si oui, afficher un message et retourner True
  - Sinon :
    - Compter les lignes actuelles dans la table
    - Importer avec `INSERT INTO ... SELECT * FROM read_parquet('...')`
    - Compter les nouvelles lignes
    - Enregistrer dans `import_log`
    - G√©rer les erreurs

- `import_all_parquet_files(data_dir: Path) -> int` :
  - Lister tous les fichiers `.parquet` dans le r√©pertoire
  - Appeler `import_parquet()` pour chacun
  - Retourner le nombre de fichiers import√©s

- `get_statistics()` :
  - Afficher le nombre total de trajets
  - Afficher le nombre de fichiers import√©s
  - Afficher la plage de dates (min/max)
  - Afficher la taille de la base de donn√©es

- `close()` :
  - Fermer la connexion DuckDB

**üí° Indices DuckDB** :
- `duckdb.connect(db_path)` pour se connecter
- `conn.execute(sql_query)` pour ex√©cuter du SQL
- `conn.execute(query).fetchone()` pour r√©cup√©rer un r√©sultat
- DuckDB peut lire directement les Parquet : `read_parquet('file.parquet')`
- Utilisez des transactions pour l'int√©grit√© des donn√©es

**Sch√©ma de la base DuckDB** :
```sql
CREATE TABLE yellow_taxi_trips (
    VendorID BIGINT,
    tpep_pickup_datetime TIMESTAMP,
    tpep_dropoff_datetime TIMESTAMP,
    passenger_count DOUBLE,
    trip_distance DOUBLE,
    RatecodeID DOUBLE,
    store_and_fwd_flag VARCHAR,
    PULocationID BIGINT,
    DOLocationID BIGINT,
    payment_type BIGINT,
    fare_amount DOUBLE,
    extra DOUBLE,
    mta_tax DOUBLE,
    tip_amount DOUBLE,
    tolls_amount DOUBLE,
    improvement_surcharge DOUBLE,
    total_amount DOUBLE,
    congestion_surcharge DOUBLE,
    Airport_fee DOUBLE
);

CREATE TABLE import_log (
    file_name VARCHAR PRIMARY KEY,
    import_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    rows_imported BIGINT
);
```

4. **V√©rifier le `.gitignore`**
   S'assurer que ces patterns sont pr√©sents :
   ```
   *.duckdb
   *.duckdb.wal
   ```

5. **Tester et merger**
   ```bash
   pip install duckdb
   python src/import_to_duckdb.py
   git add .
   git commit -m "feat: add DuckDB importer with duplicate prevention"
   git push -u origin feature/duckdb-import
   git checkout develop
   git merge feature/duckdb-import
   git push origin develop
   ```

**üîç Points de v√©rification** :
- [ ] Les donn√©es sont import√©es dans DuckDB
- [ ] Les fichiers d√©j√† import√©s ne sont pas r√©-import√©s
- [ ] Les statistiques s'affichent correctement
- [ ] La base de donn√©es n'est pas versionn√©e dans Git

### √âtape 2.2 : Dockerisation avec PostgreSQL (3h)

**üéØ Objectifs** :
- Cr√©er un docker-compose avec PostgreSQL
- Migrer de DuckDB vers PostgreSQL
- Cr√©er un workflow pour builder et publier l'image Docker

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/docker-postgres
   ```

2. **Ajouter les d√©pendances PostgreSQL**
   Ajouter dans `requirements.txt` :
   ```
   psycopg2-binary==2.9.9
   sqlalchemy==2.0.25
   ```

3. **Cr√©er le fichier `.env.example`**

```
# PostgreSQL
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=nyc_taxi
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Application
APP_ENV=development
```

4. **Cr√©er le fichier `docker-compose.yml`**

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:16-alpine
    container_name: nyc-taxi-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-nyc_taxi}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: nyc-taxi-app
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-nyc_taxi}
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./src:/app/src
    depends_on:
      postgres:
        condition: service_healthy
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload

volumes:
  postgres_data:
```

5. **Cr√©er le `Dockerfile`**

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Installer les d√©pendances syst√®me
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Copier les requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code source
COPY src/ ./src/
COPY data/ ./data/

# Exposer le port
EXPOSE 8000

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

6. **D√©velopper `src/database.py`**

Ce fichier doit contenir :
- Configuration de la connexion PostgreSQL depuis les variables d'environnement
- Cr√©ation du moteur SQLAlchemy
- Fonction `get_db()` pour obtenir une session (d√©pendance FastAPI)
- Fonction `init_db()` pour initialiser les tables

**üí° Indices SQLAlchemy** :
```python
DATABASE_URL = f"postgresql://{user}:{password}@{host}:{port}/{db}"
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
```

7. **D√©velopper `src/import_to_postgres.py`**

Adapter le script DuckDB pour PostgreSQL :

**Classe `PostgresImporter`** :
- Similaire √† `DuckDBImporter` mais avec PostgreSQL
- Utiliser SQLAlchemy pour les connexions
- Utiliser `pandas.to_sql()` pour l'import
- Cr√©er les m√™mes tables : `yellow_taxi_trips` et `import_log`

**üí° Indices** :
- Renommer les colonnes pour correspondre au sch√©ma PostgreSQL (snake_case)
- Utiliser `df.to_sql('table_name', engine, if_exists='append')`
- Les types PostgreSQL diff√®rent l√©g√®rement : `DOUBLE PRECISION`, `SERIAL PRIMARY KEY`

8. **Cr√©er le workflow `.github/workflows/docker-build.yml`**
    objectif cr√©er une image docker et la push sur le registry de github

9. **Tester et merger**
   ```bash
   cp .env.example .env
   docker-compose up -d postgres
   pip install psycopg2-binary sqlalchemy pandas
   python src/import_to_postgres.py

   git add .
   git commit -m "feat: add Docker support with PostgreSQL and build workflow"
   git push -u origin feature/docker-postgres
   git checkout develop
   git merge feature/docker-postgres
   git push origin develop
   ```

**üîç Points de v√©rification** :
- [ ] PostgreSQL d√©marre dans Docker
- [ ] Les donn√©es s'importent correctement
- [ ] L'image Docker se build sans erreur
- [ ] Le workflow GitHub Actions est configur√©

---

## Jour 3 : API REST avec FastAPI

### Sch√©ma de l'√©tape

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  JOUR 3 : API FastAPI (Architecture MVC)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

        API REST (FastAPI)
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ         ‚îÇ
    ‚ñº         ‚ñº         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Routes ‚îÇ ‚îÇServices‚îÇ ‚îÇ Models ‚îÇ
‚îÇ        ‚îÇ ‚îÇ        ‚îÇ ‚îÇ        ‚îÇ
‚îÇ GET    ‚îÇ‚îÄ‚îÇ CRUD   ‚îÇ‚îÄ‚îÇSQLAlch.‚îÇ
‚îÇ POST   ‚îÇ ‚îÇLogic   ‚îÇ ‚îÇSchemas ‚îÇ
‚îÇ PUT    ‚îÇ ‚îÇ        ‚îÇ ‚îÇ        ‚îÇ
‚îÇ DELETE ‚îÇ ‚îÇ        ‚îÇ ‚îÇ        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
         PostgreSQL
```

### √âtape 3.1 : Cr√©er le CRUD avec FastAPI (4h)

**üéØ Objectifs** :
- Cr√©er un CRUD complet sur PostgreSQL
- Organiser le code avec Models, Services et Routes
- Ajouter une route pour ex√©cuter les scripts de t√©l√©chargement et import
- G√©n√©rer une documentation automatique avec Swagger

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/fastapi-crud
   ```

2. **Ajouter FastAPI aux d√©pendances**
   ```
   fastapi==0.109.0
   uvicorn[standard]==0.27.0
   pydantic==2.5.3
   pydantic-settings==2.1.0
   ```

3. **D√©velopper `src/models.py` (Mod√®les SQLAlchemy)**

Cr√©er les mod√®les SQLAlchemy :

**Classe `YellowTaxiTrip(Base)`** :
- H√©riter de `Base` (SQLAlchemy)
- D√©finir `__tablename__ = "yellow_taxi_trips"`
- D√©finir toutes les colonnes correspondant au sch√©ma PostgreSQL
- Ajouter un `id` en primary key

**Classe `ImportLog(Base)`** :
- Table pour tracker les imports
- Colonnes : `file_name`, `import_date`, `rows_imported`

**üí° Structure attendue** :
```python
from sqlalchemy import Column, Integer, String, Float, DateTime, BigInteger
from src.database import Base

class YellowTaxiTrip(Base):
    __tablename__ = "yellow_taxi_trips"
    # D√©finir toutes les colonnes ici

class ImportLog(Base):
    __tablename__ = "import_log"
    # D√©finir les colonnes de log
```

4. **D√©velopper `src/schemas.py` (Sch√©mas Pydantic)**

Cr√©er les sch√©mas de validation Pydantic :

**Sch√©mas √† cr√©er** :
- `TaxiTripBase` : Sch√©ma de base avec tous les champs
- `TaxiTripCreate` : H√©rite de `TaxiTripBase` (pour cr√©ation)
- `TaxiTripUpdate` : H√©rite de `TaxiTripBase` (pour mise √† jour)
- `TaxiTrip` : H√©rite de `TaxiTripBase` + `id` (pour r√©ponse)
- `TaxiTripList` : Contient `total` et `trips: list[TaxiTrip]`
- `Statistics` : Pour les statistiques (total, dates, moyennes)
- `PipelineResponse` : Pour les r√©ponses de la pipeline

**üí° Indices Pydantic** :
```python
from pydantic import BaseModel, ConfigDict
from datetime import datetime
from typing import Optional

class TaxiTripBase(BaseModel):
    # Tous les champs avec Optional[type] = None

class TaxiTrip(TaxiTripBase):
    id: int
    model_config = ConfigDict(from_attributes=True)
```

5. **D√©velopper `src/services.py` (Logique m√©tier)**

**Classe `TaxiTripService`** avec m√©thodes statiques :

- `get_trip(db: Session, trip_id: int)` :
  - R√©cup√©rer un trajet par ID
  - Retourner None si non trouv√©

- `get_trips(db: Session, skip: int, limit: int)` :
  - R√©cup√©rer une liste de trajets avec pagination
  - Retourner (trips, total)

- `create_trip(db: Session, trip: TaxiTripCreate)` :
  - Cr√©er un nouveau trajet
  - Sauvegarder en base
  - Retourner le trajet cr√©√©

- `update_trip(db: Session, trip_id: int, trip: TaxiTripUpdate)` :
  - Mettre √† jour un trajet existant
  - Retourner None si non trouv√©

- `delete_trip(db: Session, trip_id: int)` :
  - Supprimer un trajet
  - Retourner True/False

- `get_statistics(db: Session)` :
  - Calculer les statistiques (COUNT, MIN, MAX, AVG)
  - Retourner un objet `Statistics`

**üí° Indices SQLAlchemy** :
```python
from sqlalchemy.orm import Session
from sqlalchemy import func

# Requ√™te
db.query(Model).filter(Model.id == id).first()

# Comptage
db.query(func.count(Model.id)).scalar()

# Statistiques
db.execute(text("SELECT AVG(column) FROM table")).fetchone()
```

6. **D√©velopper `src/routes.py` (Endpoints API)**

**Router FastAPI** avec les endpoints suivants :

**CRUD Trajets** :
- `GET /trips` : Liste avec pagination (skip, limit)
- `GET /trips/{trip_id}` : Un trajet par ID
- `POST /trips` : Cr√©er un trajet
- `PUT /trips/{trip_id}` : Mettre √† jour un trajet
- `DELETE /trips/{trip_id}` : Supprimer un trajet

**Statistiques** :
- `GET /statistics` : Obtenir les statistiques

**Pipeline** :
- `POST /pipeline/run` : Ex√©cuter t√©l√©chargement + import

**üí° Structure attendue** :
```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

router = APIRouter()

@router.get("/trips", response_model=TaxiTripList, tags=["Trips"])
def get_trips(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    # Appeler le service
    # Retourner la r√©ponse
```

7. **D√©velopper `src/main.py` (Application principale)**

Cr√©er l'application FastAPI :
- Initialiser l'app avec titre et description
- Configurer CORS
- Inclure le router avec prefix `/api/v1`
- Cr√©er les routes `/` et `/health`
- Cr√©er les tables au d√©marrage

**üí° Structure** :
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.database import engine, Base
from src.routes import router

Base.metadata.create_all(bind=engine)

app = FastAPI(
    title="NYC Taxi Data Pipeline API",
    description="...",
    version="1.0.0"
)

app.add_middleware(CORSMiddleware, ...)
app.include_router(router, prefix="/api/v1")
```

8. **Tester l'API**
   ```bash
   pip install fastapi uvicorn pydantic pydantic-settings
   docker-compose up -d
   uvicorn src.main:app --reload
   ```

   Acc√©der √† la documentation : http://localhost:8000/docs

9. **Merger**
   ```bash
   git add .
   git commit -m "feat: add FastAPI CRUD with models, services and routes"
   git push -u origin feature/fastapi-crud
   git checkout develop
   git merge feature/fastapi-crud
   git push origin develop
   ```

**üîç Points de v√©rification** :
- [ ] L'API d√©marre sans erreur
- [ ] La documentation Swagger est accessible
- [ ] Les endpoints CRUD fonctionnent
- [ ] Les statistiques s'affichent
- [ ] La route pipeline fonctionne

**üìä Endpoints attendus** :
```
GET    /                      ‚Üí Info API
GET    /health                ‚Üí Health check
GET    /api/v1/trips          ‚Üí Liste des trajets
GET    /api/v1/trips/{id}     ‚Üí Un trajet
POST   /api/v1/trips          ‚Üí Cr√©er un trajet
PUT    /api/v1/trips/{id}     ‚Üí Modifier un trajet
DELETE /api/v1/trips/{id}     ‚Üí Supprimer un trajet
GET    /api/v1/statistics     ‚Üí Statistiques
POST   /api/v1/pipeline/run   ‚Üí Ex√©cuter la pipeline
```

---

## Jour 4 : Analyse et nettoyage des donn√©es avec MongoDB

### Sch√©ma de l'√©tape

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  JOUR 4 : Data Cleaning + MongoDB             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PostgreSQL (Donn√©es brutes)
      ‚îÇ
      ‚îÇ ‚ë† Extraction
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DataFrame  ‚îÇ
‚îÇ  (Pandas)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚îÇ ‚ë° Analyse
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  D√©tection ‚îÇ
‚îÇ  Anomalies ‚îÇ
‚îÇ            ‚îÇ
‚îÇ ‚Ä¢ N√©gatifs ‚îÇ
‚îÇ ‚Ä¢ Nulls    ‚îÇ
‚îÇ ‚Ä¢ Outliers ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚îÇ ‚ë¢ Nettoyage
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Filtrage   ‚îÇ
‚îÇ Suppression‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚îÇ ‚ë£ Sauvegarde
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MongoDB   ‚îÇ
‚îÇ  (Clean)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### √âtape 4.1 : Analyser et nettoyer les donn√©es (4h)

**üéØ Objectifs** :
- Analyser les donn√©es pour d√©tecter les anomalies
- Cr√©er un script pour nettoyer les donn√©es
- Sauvegarder les donn√©es nettoy√©es dans MongoDB

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/data-cleaning-mongodb
   ```

2. **Ajouter MongoDB aux d√©pendances**
   ```
   pymongo==4.6.1
   motor==3.3.2
   ```

3. **Ajouter MongoDB dans `docker-compose.yml`**

```yaml
  mongodb:
    image: mongo:7
    container_name: nyc-taxi-mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER:-admin}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD:-admin}
      MONGO_INITDB_DATABASE: ${MONGO_DB:-nyc_taxi_clean}
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db

volumes:
  postgres_data:
  mongodb_data:
```

4. **Ajouter les variables MongoDB dans `.env.example`**
   ```
   # MongoDB
   MONGO_USER=admin
   MONGO_PASSWORD=admin
   MONGO_DB=nyc_taxi_clean
   MONGO_HOST=mongodb
   MONGO_PORT=27017
   ```

5. **D√©velopper `src/data_cleaner.py`**

**Classe `DataCleaner`** avec les m√©thodes suivantes :

**a) Initialisation et connexions** :

- `__init__()` :
  - Cr√©er la connexion PostgreSQL (avec SQLAlchemy)
  - Cr√©er la connexion MongoDB (avec pymongo)
  - Initialiser la collection `cleaned_trips`

- `_get_postgres_engine()` :
  - Construire l'URL de connexion depuis les variables d'environnement
  - Retourner le moteur SQLAlchemy

- `_get_mongo_client()` :
  - Construire l'URL de connexion MongoDB
  - Retourner le client MongoDB

**b) Chargement des donn√©es** :

- `load_data_from_postgres()` :
  - Charger toutes les donn√©es depuis PostgreSQL
  - Utiliser `pd.read_sql()`
  - Retourner un DataFrame

**c) Analyse des donn√©es** :

- `analyze_data(df: pd.DataFrame) -> Dict` :
  - Analyser les valeurs n√©gatives pour : `passenger_count`, `trip_distance`, `fare_amount`, `tip_amount`, `tolls_amount`, `total_amount`
  - Analyser les valeurs nulles
  - Analyser les outliers :
    - `passenger_count` : doit √™tre entre 1 et 8
    - `trip_distance` : ne doit pas d√©passer 100 miles
    - `fare_amount` : ne doit pas d√©passer $500
  - Retourner un dictionnaire avec les statistiques

**d) Nettoyage des donn√©es** :

- `clean_data(df: pd.DataFrame) -> pd.DataFrame` :
  - Supprimer les lignes avec valeurs n√©gatives
  - Supprimer les lignes avec `passenger_count` < 1 ou > 8
  - Supprimer les lignes avec `trip_distance` > 100
  - Supprimer les lignes avec `fare_amount` > 500
  - Supprimer les lignes avec dates manquantes
  - Afficher un r√©sum√© du nettoyage
  - Retourner le DataFrame nettoy√©

**e) Sauvegarde dans MongoDB** :

- `save_to_mongodb(df: pd.DataFrame) -> int` :
  - Convertir le DataFrame en liste de dictionnaires
  - Convertir les Timestamp Pandas en datetime Python
  - Supprimer l'ID PostgreSQL
  - V√©rifier si des donn√©es existent d√©j√† dans MongoDB
  - Si oui, supprimer les anciennes donn√©es
  - Ins√©rer les nouvelles donn√©es
  - Retourner le nombre de documents ins√©r√©s

**f) Fermeture** :

- `close()` :
  - Fermer la connexion MongoDB

**üí° R√®gles de nettoyage** :
```
Valeurs n√©gatives √† supprimer :
- passenger_count < 0
- trip_distance < 0
- fare_amount < 0
- tip_amount < 0
- tolls_amount < 0
- total_amount < 0

Outliers √† supprimer :
- passenger_count < 1 ou > 8
- trip_distance > 100 miles
- fare_amount > $500

Valeurs manquantes :
- tpep_pickup_datetime NULL
- tpep_dropoff_datetime NULL
```

**üí° Indices Pandas/MongoDB** :
```python
# Filtrage Pandas
df = df[df['column'] >= 0]
df = df[(df['col'] >= min) & (df['col'] <= max)]
df = df.dropna(subset=['col1', 'col2'])

# MongoDB
client = MongoClient(f"mongodb://{user}:{password}@{host}:{port}/")
collection.count_documents({})
collection.delete_many({})
collection.insert_many(records)

# Conversion Pandas Timestamp
if isinstance(value, pd.Timestamp):
    value = value.to_pydatetime()
```

6. **Structure du script principal**
   ```python
   if __name__ == "__main__":
       cleaner = DataCleaner()
       try:
           # Charger
           df = cleaner.load_data_from_postgres()

           # Analyser
           analysis = cleaner.analyze_data(df)

           # Nettoyer
           cleaned_df = cleaner.clean_data(df)

           # Sauvegarder
           cleaner.save_to_mongodb(cleaned_df)
       finally:
           cleaner.close()
   ```

7. **Tester et merger**
   ```bash
   pip install pymongo
   docker-compose up -d mongodb
   python src/data_cleaner.py

   git add .
   git commit -m "feat: add data cleaning and MongoDB integration"
   git push -u origin feature/data-cleaning-mongodb
   git checkout develop
   git merge feature/data-cleaning-mongodb
   git push origin develop
   ```

**üîç Points de v√©rification** :
- [ ] L'analyse d√©tecte correctement les anomalies
- [ ] Le nettoyage supprime les lignes probl√©matiques
- [ ] Les donn√©es sont sauvegard√©es dans MongoDB
- [ ] Les statistiques avant/apr√®s sont affich√©es

**üìä Exemple de sortie attendue** :
```
üìä Analyse des donn√©es brutes :
  ‚ö† fare_amount: 1,245 valeurs n√©gatives
  ‚ö† passenger_count: 3,567 valeurs aberrantes (< 1 ou > 8)
  ‚ö† trip_distance: 892 trajets > 100 miles

üßπ Nettoyage des donn√©es...
  ‚úì Supprim√© 1,245 lignes avec fare_amount n√©gatif
  ‚úì Supprim√© 3,567 lignes avec passenger_count invalide
  ‚úì Supprim√© 892 lignes avec trip_distance > 100 miles

  üìà Total supprim√© : 5,704 lignes (2.34%)
  ‚úÖ Lignes restantes : 238,296

üíæ Sauvegarde dans MongoDB...
  ‚úÖ 238,296 documents ins√©r√©s dans MongoDB
```

---

## Jour 5 : Migration vers DLT (Data Load Tool)

### Sch√©ma de l'√©tape

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  JOUR 5 : Migration DLT                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Scripts Python (avant)          DLT Pipeline (apr√®s)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ download_data.py       ‚îÇ     ‚îÇ                      ‚îÇ
‚îÇ import_to_postgres.py  ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ  dlt_pipeline.py     ‚îÇ
‚îÇ data_cleaner.py        ‚îÇ     ‚îÇ                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                               ‚îÇ  ‚îÇ  @dlt.resource ‚îÇ  ‚îÇ
                               ‚îÇ  ‚îÇ  - Download    ‚îÇ  ‚îÇ
                               ‚îÇ  ‚îÇ  - Clean       ‚îÇ  ‚îÇ
                               ‚îÇ  ‚îÇ  - Load        ‚îÇ  ‚îÇ
                               ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ
                                          ‚ñº
                                   PostgreSQL/MongoDB
```

### √âtape 5.1 : Remplacer les scripts par DLT (4-5h)

**üéØ Objectifs** :
- Comprendre DLT et ses avantages
- Cr√©er une pipeline DLT unifi√©e
- Obtenir le m√™me r√©sultat avec moins de code

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/dlt-migration
   ```

2. **Installer DLT**
   ```bash
   pip install dlt[postgres,parquet]
   ```

   Ajouter dans `requirements.txt` :
   ```
   dlt[postgres,parquet]==0.4.3
   ```

3. **Initialiser DLT**
   ```bash
   dlt init nyc_taxi postgres
   ```

4. **Cr√©er la configuration `.dlt/config.toml`**

```toml
[runtime]
log_level = "INFO"

[sources.nyc_taxi]
year = 2025
```

5. **Cr√©er le fichier de secrets `.dlt/secrets.toml`** (NE PAS COMMITTER)

```toml
[destination.postgres.credentials]
database = "nyc_taxi"
username = "postgres"
password = "postgres"
host = "localhost"
port = 5432
```

6. **Ajouter `.dlt/secrets.toml` au `.gitignore`**

7. **D√©velopper `src/dlt_pipeline.py`**

**Comprendre DLT** :

DLT (Data Load Tool) est un framework Python pour cr√©er des pipelines de donn√©es. Les concepts cl√©s :

- **Resource** : Source de donn√©es (d√©corateur `@dlt.resource`)
- **Pipeline** : Orchestration des resources
- **Destination** : O√π stocker les donn√©es (PostgreSQL, BigQuery, etc.)
- **write_disposition** : Comment g√©rer les doublons
  - `append` : Ajouter (pas de doublons)
  - `replace` : Remplacer tout
  - `merge` : Fusionner sur cl√©

**Classe `NYCTaxiDLTPipeline`** :

**a) Initialisation** :

- `__init__()` :
  - D√©finir les constantes (BASE_URL, YEAR, DATA_DIR)
  - Cr√©er le r√©pertoire de donn√©es

**b) Resource DLT** :

- `load_taxi_data()` (d√©cor√©e avec `@dlt.resource`) :
  - Param√®tres du d√©corateur :
    - `name="yellow_taxi_trips"`
    - `write_disposition="append"` (pour √©viter les doublons)
  - Logique :
    1. D√©terminer les mois √† traiter
    2. Pour chaque mois :
       - T√©l√©charger si n√©cessaire (`_download_if_needed()`)
       - Lire le Parquet avec pandas
       - Nettoyer les donn√©es (`_clean_data()`)
       - Normaliser les noms de colonnes
       - Yielder chaque enregistrement (dictionnaire)

**c) M√©thodes priv√©es** :

- `_download_if_needed(month: int) -> Path` :
  - V√©rifier si le fichier existe
  - Sinon, t√©l√©charger depuis l'URL
  - Retourner le chemin du fichier

- `_clean_data(df: pd.DataFrame) -> pd.DataFrame` :
  - Appliquer les m√™mes r√®gles de nettoyage que `data_cleaner.py`
  - Retourner le DataFrame nettoy√©

**d) Ex√©cution de la pipeline** :

- `run_pipeline(destination: str = "postgres")` :
  - Cr√©er la pipeline DLT :
    ```python
    pipeline = dlt.pipeline(
        pipeline_name="nyc_taxi_pipeline",
        destination=destination,
        dataset_name="nyc_taxi_dlt"
    )
    ```
  - Ex√©cuter : `pipeline.run(self.load_taxi_data())`
  - Retourner les informations de chargement

**üí° Structure DLT** :
```python
import dlt
from typing import Iterator, Dict, Any

class NYCTaxiDLTPipeline:

    @dlt.resource(name="yellow_taxi_trips", write_disposition="append")
    def load_taxi_data(self) -> Iterator[Dict[str, Any]]:
        # Pour chaque mois
        for month in range(1, max_month + 1):
            # T√©l√©charger
            file_path = self._download_if_needed(month)

            # Lire et nettoyer
            df = pd.read_parquet(file_path)
            df = self._clean_data(df)

            # Yielder chaque ligne
            for record in df.to_dict('records'):
                yield record

    def run_pipeline(self):
        pipeline = dlt.pipeline(
            pipeline_name="...",
            destination="postgres",
            dataset_name="..."
        )

        load_info = pipeline.run(self.load_taxi_data())
        return load_info
```

8. **Cr√©er la documentation `docs/DLT_MIGRATION.md`**

Cr√©er un fichier markdown expliquant :

**Avantages de DLT** :
- Gestion automatique des doublons
- Sch√©ma automatique
- Monitoring int√©gr√©
- Incr√©mental par d√©faut
- Portable (PostgreSQL, BigQuery, Snowflake, etc.)

**Tableau comparatif** :

| Fonctionnalit√© | Scripts Python | DLT |
|----------------|----------------|-----|
| T√©l√©chargement | Manuel avec requests | Int√©gr√© dans resource |
| D√©doublonnage | Table import_log | write_disposition="append" |
| Nettoyage | Script s√©par√© | Int√©gr√© dans resource |
| Monitoring | print() | Logs DLT + m√©triques |
| Destination | Hardcod√© PostgreSQL | Configurable |
| Sch√©ma | D√©fini manuellement | D√©tection automatique |

**Instructions d'utilisation** :
- Comment installer DLT
- Comment configurer les credentials
- Comment ex√©cuter la pipeline

9. **Tester la pipeline**
   ```bash
   python src/dlt_pipeline.py
   ```

10. **Merger**
   ```bash
   git add .
   git commit -m "feat: migrate to DLT for data pipeline"
   git push -u origin feature/dlt-migration
   git checkout develop
   git merge feature/dlt-migration
   git push origin develop
   ```

**üîç Points de v√©rification** :
- [ ] La pipeline DLT se lance sans erreur
- [ ] Les donn√©es sont t√©l√©charg√©es
- [ ] Les donn√©es sont nettoy√©es
- [ ] Les donn√©es sont charg√©es dans PostgreSQL
- [ ] Pas de doublons lors d'une r√©-ex√©cution

**üí° Avantages DLT constat√©s** :
- Un seul fichier au lieu de 3 scripts
- Gestion automatique des doublons (pas besoin de table import_log)
- Sch√©ma automatiquement d√©tect√©
- Logs et m√©triques int√©gr√©s
- Facilement portable vers d'autres destinations (BigQuery, Snowflake...)

---

## Jour 6 : D√©ploiement Azure et automatisation

### Sch√©ma de l'√©tape

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  JOUR 6 : D√©ploiement Azure + Automatisation  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    GitHub Repository
          ‚îÇ
          ‚îÇ Push sur main
          ‚ñº
    GitHub Actions
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Workflow   ‚îÇ
    ‚îÇ  Scheduler  ‚îÇ
    ‚îÇ  (Mensuel)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ ‚ë† Ex√©cution DLT
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Pipeline   ‚îÇ
    ‚îÇ  DLT        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ ‚ë° Chargement
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Azure     ‚îÇ
    ‚îÇ PostgreSQL  ‚îÇ
    ‚îÇ Flexible    ‚îÇ
    ‚îÇ  Server     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Scheduling:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Jan ‚îÇ Feb ‚îÇ Mar ‚îÇ Apr ‚îÇ ... ‚îÇ
    ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ
       ‚ñº     ‚ñº     ‚ñº     ‚ñº
      Run   Run   Run   Run
```

### √âtape 6.1 : D√©ploiement sur Azure (4-5h)

**üéØ Objectifs** :
- Cr√©er une base de donn√©es PostgreSQL sur Azure
- Configurer les secrets GitHub
- Cr√©er un workflow GitHub Actions schedul√© (mensuel)
- Tester l'ex√©cution automatique

**üìã T√¢ches √† r√©aliser** :

1. **Cr√©er une feature branch**
   ```bash
   git checkout develop
   git checkout -b feature/azure-deployment
   ```

2. **Installer Azure CLI**

   Selon votre OS :
   ```bash
   # macOS
   brew install azure-cli

   # Linux
   curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

   # Windows
   # T√©l√©charger depuis https://aka.ms/installazurecliwindows
   ```

3. **Se connecter √† Azure**
   ```bash
   az login
   ```

4. **Cr√©er le script `scripts/deploy_azure.sh`**

Le script doit :
- Cr√©er un resource group
- Cr√©er un serveur PostgreSQL Flexible
- Cr√©er la base de donn√©es
- Configurer le firewall pour autoriser GitHub Actions

```bash
#!/bin/bash

# Configuration
RESOURCE_GROUP="nyc-taxi-rg"
LOCATION="francecentral"
POSTGRES_SERVER="nyc-taxi-postgres-server"
POSTGRES_DB="nyc_taxi"
POSTGRES_ADMIN="taxiadmin"
POSTGRES_PASSWORD="YourStrongPassword123!"

echo "üöÄ D√©ploiement sur Azure"
echo "="*60

# Cr√©er le resource group
echo "üì¶ Cr√©ation du resource group..."
az group create --name $RESOURCE_GROUP --location $LOCATION

# Cr√©er le serveur PostgreSQL
echo "üêò Cr√©ation du serveur PostgreSQL..."
az postgres flexible-server create \
  --resource-group $RESOURCE_GROUP \
  --name $POSTGRES_SERVER \
  --location $LOCATION \
  --admin-user $POSTGRES_ADMIN \
  --admin-password $POSTGRES_PASSWORD \
  --sku-name Standard_B1ms \
  --tier Burstable \
  --version 16 \
  --storage-size 32 \
  --public-access 0.0.0.0

# Cr√©er la base de donn√©es
echo "üíæ Cr√©ation de la base de donn√©es..."
az postgres flexible-server db create \
  --resource-group $RESOURCE_GROUP \
  --server-name $POSTGRES_SERVER \
  --database-name $POSTGRES_DB

# Configurer le firewall
echo "üî• Configuration du firewall..."
az postgres flexible-server firewall-rule create \
  --resource-group $RESOURCE_GROUP \
  --name $POSTGRES_SERVER \
  --rule-name AllowGitHubActions \
  --start-ip-address 0.0.0.0 \
  --end-ip-address 255.255.255.255

echo "‚úÖ D√©ploiement termin√© !"
echo ""
echo "üìù Informations de connexion :"
echo "  Host: $POSTGRES_SERVER.postgres.database.azure.com"
echo "  Database: $POSTGRES_DB"
echo "  User: $POSTGRES_ADMIN"
echo "  Password: $POSTGRES_PASSWORD"
```

5. **Cr√©er le workflow `.github/workflows/scheduled-dlt-pipeline.yml`**

```yaml
name: Scheduled DLT Pipeline

on:
  schedule:
    # Ex√©cuter le 1er de chaque mois √† 2h du matin (UTC)
    - cron: '0 2 1 * *'
  workflow_dispatch:  # Permet l'ex√©cution manuelle

env:
  PYTHON_VERSION: '3.11'

jobs:
  run-dlt-pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure DLT secrets
        run: |
          mkdir -p .dlt
          cat > .dlt/secrets.toml << EOF
          [destination.postgres.credentials]
          database = "${{ secrets.AZURE_POSTGRES_DB }}"
          username = "${{ secrets.AZURE_POSTGRES_USER }}"
          password = "${{ secrets.AZURE_POSTGRES_PASSWORD }}"
          host = "${{ secrets.AZURE_POSTGRES_HOST }}"
          port = 5432
          EOF

      - name: Run DLT Pipeline
        run: |
          echo "üöÄ D√©marrage de la pipeline DLT"
          python src/dlt_pipeline.py
        env:
          POSTGRES_HOST: ${{ secrets.AZURE_POSTGRES_HOST }}
          POSTGRES_DB: ${{ secrets.AZURE_POSTGRES_DB }}
          POSTGRES_USER: ${{ secrets.AZURE_POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.AZURE_POSTGRES_PASSWORD }}

      - name: Notify on success
        if: success()
        run: |
          echo "‚úÖ Pipeline DLT ex√©cut√©e avec succ√®s !"

      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå √âchec de la pipeline DLT"
          exit 1
```

**üí° Comprendre le Cron** :
```
'0 2 1 * *'
 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ
 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ Jour de la semaine (0-6, 0=Dimanche)
 ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Mois (1-12)
 ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Jour du mois (1-31)
 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Heure (0-23)
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Minute (0-59)

Exemples :
'0 2 1 * *'   ‚Üí Le 1er de chaque mois √† 2h du matin
'0 0 * * 0'   ‚Üí Tous les dimanches √† minuit
'30 14 * * *' ‚Üí Tous les jours √† 14h30
```

6. **Cr√©er la documentation `docs/AZURE_DEPLOYMENT.md`**

Cette documentation doit contenir :

**a) Pr√©requis** :
- Compte Azure
- Azure CLI install√©
- Droits de cr√©ation de ressources

**b) Guide de d√©ploiement** :
- Commandes pour ex√©cuter le script
- Configuration des secrets GitHub
- Test du workflow

**c) Co√ªts estim√©s** :
| Service | Tier | Co√ªt mensuel estim√© |
|---------|------|---------------------|
| PostgreSQL Flexible Server | Burstable B1ms | ~25‚Ç¨ |
| GitHub Actions | 2000 min/mois gratuit | Gratuit |

**d) S√©curit√©** :
- Recommandations pour le firewall
- Utilisation d'Azure Key Vault
- Activation SSL/TLS
- Alertes de monitoring

**e) Commandes utiles** :
```bash
# V√©rifier l'√©tat du serveur
az postgres flexible-server show \
  --resource-group nyc-taxi-rg \
  --name nyc-taxi-postgres-server

# Voir les m√©triques
az monitor metrics list \
  --resource <resource-id> \
  --metric-names cpu_percent,memory_percent

# Supprimer toutes les ressources
az group delete --name nyc-taxi-rg --yes --no-wait
```

7. **Cr√©er la documentation `docs/TESTING_GUIDE.md`**

Guide de test avec :

**Tests locaux** :
- Tester le t√©l√©chargement
- Tester l'import PostgreSQL
- Tester l'API
- Tester DLT

**Tests Azure** :
- Ex√©cution manuelle du workflow
- V√©rification des logs
- Validation des donn√©es

**Requ√™tes de validation** :
```sql
-- PostgreSQL
SELECT COUNT(*) FROM yellow_taxi_trips;
SELECT MIN(tpep_pickup_datetime), MAX(tpep_pickup_datetime) FROM yellow_taxi_trips;
SELECT AVG(trip_distance), AVG(fare_amount) FROM yellow_taxi_trips;
```

```javascript
// MongoDB
db.cleaned_trips.countDocuments()
db.cleaned_trips.aggregate([
  { $group: {
      _id: null,
      avg_distance: { $avg: "$trip_distance" },
      total: { $sum: 1 }
    }
  }
])
```

8. **Mettre √† jour le README principal**

Cr√©er un README complet avec :

**Architecture** (diagramme ASCII) :
```
[NYC Data] ‚Üí [Download] ‚Üí [PostgreSQL] ‚Üí [Clean] ‚Üí [MongoDB]
                ‚Üì
             [DLT Pipeline]
                ‚Üì
          [Azure PostgreSQL]
                ‚Üì
         [Scheduled Job (monthly)]
```

**Technologies** :
- Backend : Python 3.11, FastAPI
- Bases de donn√©es : PostgreSQL, MongoDB, DuckDB
- ETL : DLT (Data Load Tool)
- Containerisation : Docker, Docker Compose
- CI/CD : GitHub Actions, Semantic Release
- Cloud : Azure (PostgreSQL Flexible Server)

**D√©marrage rapide** :
- Installation
- Configuration
- Lancement local
- Acc√®s √† l'API

**Liens vers la documentation** :
- D√©ploiement Azure
- Migration DLT
- Guide de test

**Workflow Git** :
- Branches (main, develop, feature/*)
- Conventions de commits

9. **D√©ployer sur Azure**
   ```bash
   chmod +x scripts/deploy_azure.sh
   ./scripts/deploy_azure.sh
   ```

10. **Configurer les secrets GitHub**

Via l'interface web GitHub (Settings > Secrets and variables > Actions) :

- `AZURE_POSTGRES_HOST` : `votre-serveur.postgres.database.azure.com`
- `AZURE_POSTGRES_DB` : `nyc_taxi`
- `AZURE_POSTGRES_USER` : `taxiadmin`
- `AZURE_POSTGRES_PASSWORD` : `VotreMotDePasse123!`

Ou via GitHub CLI :
```bash
gh secret set AZURE_POSTGRES_HOST --body "votre-serveur.postgres.database.azure.com"
gh secret set AZURE_POSTGRES_DB --body "nyc_taxi"
gh secret set AZURE_POSTGRES_USER --body "taxiadmin"
gh secret set AZURE_POSTGRES_PASSWORD --body "VotreMotDePasse123!"
```

11. **Tester le workflow manuellement**
   ```bash
   gh workflow run scheduled-dlt-pipeline.yml
   ```

   V√©rifier l'ex√©cution :
   ```bash
   gh run list --workflow=scheduled-dlt-pipeline.yml
   gh run view <run-id> --log
   ```

12. **Merger final**
   ```bash
   git add .
   git commit -m "feat: add Azure deployment and scheduled workflow"
   git push -u origin feature/azure-deployment
   git checkout develop
   git merge feature/azure-deployment
   git push origin develop

   # Merger develop dans main pour d√©clencher la release
   git checkout main
   git merge develop
   git push origin main
   ```

**üîç Points de v√©rification** :
- [ ] Le serveur PostgreSQL est cr√©√© sur Azure
- [ ] Les secrets GitHub sont configur√©s
- [ ] Le workflow s'ex√©cute manuellement sans erreur
- [ ] Les donn√©es sont charg√©es dans Azure PostgreSQL
- [ ] Le workflow schedul√© est activ√©

---

## Livrables attendus

√Ä la fin du brief, votre repository devra contenir :

### 1. Repository GitHub structur√©

```
nyc-taxi-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ release.yml
‚îÇ       ‚îú‚îÄ‚îÄ docker-build.yml
‚îÇ       ‚îî‚îÄ‚îÄ scheduled-dlt-pipeline.yml
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ download_data.py
‚îÇ   ‚îú‚îÄ‚îÄ import_to_duckdb.py
‚îÇ   ‚îú‚îÄ‚îÄ import_to_postgres.py
‚îÇ   ‚îú‚îÄ‚îÄ data_cleaner.py
‚îÇ   ‚îú‚îÄ‚îÄ dlt_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ database.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îú‚îÄ‚îÄ services.py
‚îÇ   ‚îú‚îÄ‚îÄ routes.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ deploy_azure.sh
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ AZURE_DEPLOYMENT.md
‚îÇ   ‚îú‚îÄ‚îÄ DLT_MIGRATION.md
‚îÇ   ‚îî‚îÄ‚îÄ TESTING_GUIDE.md
‚îÇ
‚îú‚îÄ‚îÄ .dlt/
‚îÇ   ‚îú‚îÄ‚îÄ config.toml
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml (non versionn√©)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ raw/ (non versionn√©)
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .releaserc.json
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

### 2. Fonctionnalit√©s impl√©ment√©es

- [x] T√©l√©chargement automatique des donn√©es NYC Taxi 2025
- [x] Import dans DuckDB puis PostgreSQL
- [x] Analyse et nettoyage des donn√©es
- [x] Sauvegarde des donn√©es nettoy√©es dans MongoDB
- [x] API REST compl√®te avec FastAPI (CRUD)
- [x] Pipeline DLT unifi√©e
- [x] Conteneurisation Docker
- [x] D√©ploiement Azure
- [x] Automatisation mensuelle via GitHub Actions

### 3. Infrastructure

- Docker Compose avec PostgreSQL et MongoDB
- Image Docker versionn√©e sur GitHub Container Registry
- Base de donn√©es PostgreSQL sur Azure
- Workflow schedul√© fonctionnel

### 4. Documentation

- README principal complet
- Guide de d√©ploiement Azure
- Documentation de migration DLT
- Guide de test et validation

---

## Crit√®res d'√©valuation

| Crit√®re | Points | D√©tails |
|---------|--------|---------|
| **Repository Git** | 10 | Structure, branches, protection, commits |
| **Gestion des branches** | 10 | Feature branches, merges propres, pas de commits directs sur main |
| **Qualit√© du code Python** | 15 | Lisibilit√©, organisation, gestion d'erreurs, docstrings |
| **API FastAPI** | 15 | CRUD complet, sch√©mas Pydantic, documentation Swagger |
| **Int√©gration Docker** | 10 | docker-compose fonctionnel, Dockerfile optimis√© |
| **Migration DLT** | 15 | Pipeline fonctionnelle, pas de doublons, documentation |
| **D√©ploiement Azure** | 15 | PostgreSQL d√©ploy√©, workflow schedul√©, secrets configur√©s |
| **Documentation** | 10 | README, guides, clart√©, exemples |
| **TOTAL** | **100** | |

### Grille d√©taill√©e

**Excellence (90-100)** :
- Code propre et professionnel
- Documentation exhaustive
- Gestion d'erreurs robuste
- Tests de bout en bout
- Optimisations avanc√©es

**Bien (75-89)** :
- Toutes les fonctionnalit√©s impl√©ment√©es
- Code fonctionnel
- Documentation compl√®te
- Quelques am√©liorations possibles

**Satisfaisant (60-74)** :
- Fonctionnalit√©s principales pr√©sentes
- Code fonctionnel mais am√©liorable
- Documentation basique

**Insuffisant (<60)** :
- Fonctionnalit√©s manquantes
- Erreurs fr√©quentes
- Documentation incompl√®te

---

## Ressources

### Documentation officielle

- **NYC Taxi Data** : https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
- **DLT** : https://dlthub.com/docs
- **FastAPI** : https://fastapi.tiangolo.com
- **PostgreSQL** : https://www.postgresql.org/docs/
- **MongoDB** : https://www.mongodb.com/docs/
- **Docker** : https://docs.docker.com
- **Azure PostgreSQL** : https://learn.microsoft.com/azure/postgresql/
- **GitHub Actions** : https://docs.github.com/actions
- **Semantic Release** : https://semantic-release.gitbook.io/

### Tutoriels et guides

- Python Pandas : https://pandas.pydata.org/docs/
- SQLAlchemy : https://docs.sqlalchemy.org/
- Pydantic : https://docs.pydantic.dev/
- DuckDB : https://duckdb.org/docs/

### Outils

- GitHub CLI : https://cli.github.com/
- Azure CLI : https://learn.microsoft.com/cli/azure/
- Docker Desktop : https://www.docker.com/products/docker-desktop/

---

## Conseils et bonnes pratiques

### Git et versioning

- Utilisez des messages de commit descriptifs
- Respectez les conventions : `feat:`, `fix:`, `docs:`, `chore:`
- Ne committez jamais de secrets ou credentials
- Testez localement avant de pousser

### Python

- Utilisez des docstrings pour documenter vos fonctions
- G√©rez les erreurs avec try/except
- Utilisez des noms de variables explicites
- Suivez PEP 8 pour le style

### Docker

- Utilisez des images l√©g√®res (alpine)
- Nettoyez les fichiers temporaires
- Utilisez le cache Docker efficacement
- Documentez les variables d'environnement

### S√©curit√©

- Ne versionnez JAMAIS `.env` ou `secrets.toml`
- Utilisez des mots de passe forts
- Configurez correctement les firewalls
- Limitez les acc√®s au strict n√©cessaire

### Performance

- Utilisez la pagination pour les grandes listes
- Traitez les donn√©es par batch
- Indexez les colonnes fr√©quemment requ√™t√©es
- Optimisez les requ√™tes SQL

---

## Support

### En cas de probl√®me

1. **Consultez la documentation** du projet et des technologies
2. **V√©rifiez les logs** pour identifier l'erreur
3. **Recherchez l'erreur** sur Google/Stack Overflow
4. **Testez √©tape par √©tape** pour isoler le probl√®me
5. **Demandez de l'aide** en fournissant :
   - Message d'erreur complet
   - Code concern√©
   - √âtapes pour reproduire

### Ressources d'aide

- Documentation officielle des technologies
- Stack Overflow
- GitHub Issues des projets open source
- Forums de la communaut√©
- Discord/Slack de d√©veloppeurs

---

## Bon courage ! üöÄ

Ce projet vous donnera une exp√©rience compl√®te en data engineering, de l'ingestion √† l'automatisation en production. Prenez le temps de bien comprendre chaque √©tape et n'h√©sitez pas √† exp√©rimenter !

**Points cl√©s √† retenir** :
- Testez r√©guli√®rement
- Committez souvent
- Documentez au fur et √† mesure
- Automatisez ce qui peut l'√™tre
- Pensez scalabilit√© et maintenabilit√©

Bonne chance dans votre aventure data engineering ! üéØ
