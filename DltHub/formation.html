<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Formation DltHub - De Z√©ro √† Hero</title>

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

    <style>
        :root {
            --primary-color: #0066cc;
            --primary-dark: #0052a3;
            --secondary-color: #00a3e0;
            --success-color: #4caf50;
            --warning-color: #ff9800;
            --danger-color: #f44336;
            --info-color: #2196f3;
            --text-dark: #1a1a1a;
            --text-light: #666;
            --bg-light: #f8f9fa;
            --border-color: #e0e0e0;
            --code-bg: #2d2d2d;
            --dlt-purple: #8b5cf6;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.7;
            color: var(--text-dark);
            background: linear-gradient(135deg, #0066cc 0%, #00a3e0 100%);
            padding: 40px 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #0066cc 0%, #00a3e0 100%);
            color: white;
            padding: 60px 50px;
            position: relative;
            overflow: hidden;
        }

        header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml,<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg"><circle cx="50" cy="50" r="40" fill="rgba(255,255,255,0.05)"/></svg>');
            opacity: 0.3;
        }

        h1 {
            font-size: 3em;
            font-weight: 700;
            margin-bottom: 15px;
            position: relative;
            display: flex;
            align-items: center;
            gap: 20px;
        }

        h1::before {
            content: '‚ö°';
            font-size: 1.2em;
        }

        .subtitle {
            font-size: 1.3em;
            opacity: 0.95;
            font-weight: 300;
            position: relative;
        }

        .duration {
            display: inline-block;
            background: rgba(255, 255, 255, 0.2);
            padding: 8px 20px;
            border-radius: 20px;
            margin-top: 20px;
            font-size: 0.95em;
            backdrop-filter: blur(10px);
        }

        .content {
            padding: 50px;
        }

        .objectives {
            background: linear-gradient(135deg, #0066cc 0%, #00a3e0 100%);
            color: white;
            padding: 35px;
            border-radius: 15px;
            margin-bottom: 40px;
            box-shadow: 0 10px 30px rgba(0, 102, 204, 0.3);
        }

        .objectives h2 {
            color: white;
            font-size: 1.8em;
            margin-bottom: 20px;
            padding-bottom: 0;
            border-bottom: none;
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .objectives h2::before {
            content: 'üéØ';
        }

        .objectives ul {
            list-style: none;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }

        .objectives li {
            padding-left: 30px;
            position: relative;
        }

        .objectives li::before {
            content: '‚úì';
            position: absolute;
            left: 0;
            font-weight: bold;
            font-size: 1.2em;
        }

        .section {
            margin-bottom: 50px;
            scroll-margin-top: 20px;
        }

        h2 {
            color: var(--primary-color);
            font-size: 2em;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 3px solid var(--primary-color);
            font-weight: 600;
        }

        h3 {
            color: var(--text-dark);
            font-size: 1.5em;
            margin: 30px 0 20px 0;
            font-weight: 600;
        }

        h4 {
            color: var(--primary-dark);
            font-size: 1.2em;
            margin: 25px 0 15px 0;
            font-weight: 600;
        }

        p {
            margin-bottom: 20px;
            color: var(--text-dark);
            font-size: 1.05em;
        }

        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }

        li {
            margin-bottom: 12px;
            color: var(--text-dark);
            line-height: 1.7;
        }

        .code-container {
            position: relative;
            margin: 25px 0;
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.15);
        }

        .code-header {
            background: #1e1e1e;
            padding: 12px 20px;
            display: flex;
            align-items: center;
            gap: 8px;
            border-bottom: 1px solid #3a3a3a;
        }

        .code-dot {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            display: inline-block;
        }

        .code-dot.red { background: #ff5f56; }
        .code-dot.yellow { background: #ffbd2e; }
        .code-dot.green { background: #27c93f; }

        pre {
            margin: 0 !important;
            border-radius: 0 !important;
            background: var(--code-bg) !important;
        }

        pre[class*="language-"] {
            padding: 25px !important;
            font-size: 0.95em;
            line-height: 1.6;
            font-family: 'JetBrains Mono', 'Courier New', monospace;
        }

        code {
            font-family: 'JetBrains Mono', 'Courier New', monospace;
            font-size: 0.9em;
        }

        :not(pre) > code {
            background: #f4f4f4;
            padding: 3px 8px;
            border-radius: 4px;
            color: #e83e8c;
            font-weight: 500;
        }

        .alert {
            padding: 20px 25px;
            border-radius: 10px;
            margin: 25px 0;
            border-left: 5px solid;
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);
        }

        .alert-info {
            background: #e3f2fd;
            border-color: var(--info-color);
            color: #0d47a1;
        }

        .alert-success {
            background: #e8f5e9;
            border-color: var(--success-color);
            color: #1b5e20;
        }

        .alert-warning {
            background: #fff3e0;
            border-color: var(--warning-color);
            color: #e65100;
        }

        .alert-danger {
            background: #ffebee;
            border-color: var(--danger-color);
            color: #b71c1c;
        }

        .alert h4 {
            margin-top: 0;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 10px;
            font-size: 1.1em;
        }

        .alert-info h4::before { content: '‚ÑπÔ∏è'; }
        .alert-success h4::before { content: '‚úÖ'; }
        .alert-warning h4::before { content: '‚ö†Ô∏è'; }
        .alert-danger h4::before { content: 'üö®'; }

        .card {
            background: var(--bg-light);
            padding: 25px;
            border-radius: 12px;
            margin: 25px 0;
            border: 1px solid var(--border-color);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
        }

        .key-points {
            background: linear-gradient(135deg, #0078d4 0%, #00bcf2 100%);
            color: white;
            padding: 35px;
            border-radius: 15px;
            margin: 40px 0;
            box-shadow: 0 10px 30px rgba(0, 120, 212, 0.3);
        }

        .key-points h3 {
            color: white;
            margin-top: 0;
            font-size: 1.6em;
            margin-bottom: 20px;
        }

        .key-points ul {
            list-style: none;
            padding-left: 0;
        }

        .key-points li {
            padding-left: 35px;
            position: relative;
            margin-bottom: 15px;
            color: white;
        }

        .key-points li::before {
            content: '‚≠ê';
            position: absolute;
            left: 0;
            font-size: 1.2em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            background: white;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.08);
            border-radius: 10px;
            overflow: hidden;
        }

        thead {
            background: linear-gradient(135deg, #0066cc 0%, #00a3e0 100%);
            color: white;
        }

        th {
            padding: 18px;
            text-align: left;
            font-weight: 600;
            font-size: 1em;
        }

        td {
            padding: 15px 18px;
            border-bottom: 1px solid var(--border-color);
        }

        tr:last-child td {
            border-bottom: none;
        }

        tbody tr:hover {
            background: var(--bg-light);
        }

        .workflow-diagram {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 12px;
            margin: 25px 0;
            border: 2px solid var(--border-color);
            font-family: 'JetBrains Mono', monospace;
            overflow-x: auto;
        }

        .workflow-diagram pre {
            background: transparent !important;
            color: var(--text-dark) !important;
            padding: 0 !important;
            margin: 0 !important;
            font-size: 1em;
            line-height: 1.8;
        }

        .image-container {
            margin: 30px 0;
            text-align: center;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.15);
        }

        .image-caption {
            margin-top: 12px;
            color: var(--text-light);
            font-style: italic;
            font-size: 0.95em;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }

        .grid-item {
            background: var(--bg-light);
            padding: 25px;
            border-radius: 12px;
            border-left: 4px solid var(--primary-color);
            transition: transform 0.3s ease;
        }

        .grid-item:hover {
            transform: translateX(5px);
        }

        .grid-item h4 {
            margin-top: 0;
            color: var(--primary-color);
        }

        footer {
            background: #1a1a1a;
            color: white;
            padding: 40px 50px;
            text-align: center;
        }

        footer a {
            color: var(--secondary-color);
            text-decoration: none;
            transition: opacity 0.3s ease;
        }

        footer a:hover {
            opacity: 0.8;
        }

        .resources {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .resource-link {
            display: block;
            padding: 20px;
            background: var(--bg-light);
            border-radius: 10px;
            text-decoration: none;
            color: var(--text-dark);
            border: 2px solid transparent;
            transition: all 0.3s ease;
        }

        .resource-link:hover {
            border-color: var(--primary-color);
            transform: translateY(-3px);
            box-shadow: 0 5px 15px rgba(0, 102, 204, 0.2);
        }

        .resource-link strong {
            color: var(--primary-color);
            display: block;
            margin-bottom: 8px;
            font-size: 1.1em;
        }

        @media (max-width: 768px) {
            body {
                padding: 20px 10px;
            }

            .content {
                padding: 30px 20px;
            }

            header {
                padding: 40px 30px;
            }

            h1 {
                font-size: 2em;
            }

            .objectives ul {
                grid-template-columns: 1fr;
            }

            pre[class*="language-"] {
                font-size: 0.85em;
                padding: 20px !important;
            }
        }

        .comparison-table td:first-child {
            font-weight: 600;
            background: #f8f9fa;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Formation DltHub</h1>
            <p class="subtitle">De Z√©ro √† Hero - Framework Python moderne pour les pipelines de donn√©es</p>
            <span class="duration">‚è±Ô∏è Dur√©e : 1 heure</span>
        </header>

        <div class="content">
            <div class="objectives">
                <h2>Objectifs de la formation</h2>
                <ul>
                    <li>Comprendre DltHub et ses avantages</li>
                    <li>Cr√©er des pipelines de donn√©es simples</li>
                    <li>Utiliser les sources et destinations</li>
                    <li>Impl√©menter des transformations</li>
                    <li>G√©rer le schema evolution</li>
                    <li>D√©ployer en production</li>
                </ul>
            </div>

            <!-- PARTIE 1: INTRODUCTION √Ä DLTHUB -->
            <section class="section">
                <h2>1. Introduction √† DltHub</h2>

                <h3>Qu'est-ce que DltHub ?</h3>

                <p>
                    <strong>DltHub (Data Load Tool Hub)</strong> est un framework Python open-source qui simplifie la cr√©ation
                    de pipelines de donn√©es (ELT). Il permet d'extraire des donn√©es de diverses sources et de les charger dans
                    des destinations (data warehouses, data lakes) avec un minimum de code.
                </p>

                <div class="alert alert-info">
                    <h4>Philosophie de DltHub</h4>
                    <p>
                        DltHub suit une approche <strong>ELT</strong> (Extract, Load, Transform) plut√¥t que ETL.
                        Les transformations lourdes sont effectu√©es dans la destination (warehouse) plut√¥t que pendant le transit.
                    </p>
                </div>

                <h3>Pourquoi DltHub ?</h3>

                <div class="grid">
                    <div class="grid-item">
                        <h4>üöÄ Simplicit√©</h4>
                        <p>Cr√©ez des pipelines en quelques lignes de Python, sans infrastructure complexe</p>
                    </div>
                    <div class="grid-item">
                        <h4>üîÑ Schema Evolution</h4>
                        <p>Gestion automatique des changements de sch√©ma, pas de migrations manuelles</p>
                    </div>
                    <div class="grid-item">
                        <h4>üéØ Python-First</h4>
                        <p>100% Python, int√©gration naturelle avec pandas, requests, etc.</p>
                    </div>
                    <div class="grid-item">
                        <h4>üîå Connecteurs</h4>
                        <p>Nombreux connecteurs vers BigQuery, Snowflake, PostgreSQL, DuckDB...</p>
                    </div>
                    <div class="grid-item">
                        <h4>üìä Data Quality</h4>
                        <p>Validation automatique, d√©tection d'anomalies, data contracts</p>
                    </div>
                    <div class="grid-item">
                        <h4>‚ö° Incr√©mental</h4>
                        <p>Support natif du loading incr√©mental pour √©conomiser ressources et temps</p>
                    </div>
                </div>

                <h3>DltHub vs Alternatives</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>DltHub</th>
                            <th>Airbyte</th>
                            <th>Fivetran</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Type</strong></td>
                            <td>Librairie Python</td>
                            <td>Platform (UI + API)</td>
                            <td>SaaS</td>
                        </tr>
                        <tr>
                            <td><strong>Code</strong></td>
                            <td>Python natif</td>
                            <td>Configuration YAML</td>
                            <td>No-code</td>
                        </tr>
                        <tr>
                            <td><strong>Hosting</strong></td>
                            <td>Votre infra</td>
                            <td>Self-hosted ou Cloud</td>
                            <td>Cloud seulement</td>
                        </tr>
                        <tr>
                            <td><strong>Complexit√©</strong></td>
                            <td>Simple (Python)</td>
                            <td>Moyenne (Docker)</td>
                            <td>Simple (UI)</td>
                        </tr>
                        <tr>
                            <td><strong>Co√ªt</strong></td>
                            <td>Gratuit (open source)</td>
                            <td>Gratuit + Paid</td>
                            <td>Payant</td>
                        </tr>
                        <tr>
                            <td><strong>Flexibilit√©</strong></td>
                            <td>Tr√®s haute (code)</td>
                            <td>Moyenne</td>
                            <td>Limit√©e</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Concepts cl√©s</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Concept</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Source</strong></td>
                            <td>Origine des donn√©es (API, base de donn√©es, fichiers)</td>
                        </tr>
                        <tr>
                            <td><strong>Resource</strong></td>
                            <td>Unit√© de donn√©es √† charger (table, endpoint API)</td>
                        </tr>
                        <tr>
                            <td><strong>Destination</strong></td>
                            <td>O√π les donn√©es sont charg√©es (BigQuery, PostgreSQL...)</td>
                        </tr>
                        <tr>
                            <td><strong>Pipeline</strong></td>
                            <td>Configuration du flux source ‚Üí destination</td>
                        </tr>
                        <tr>
                            <td><strong>Schema</strong></td>
                            <td>Structure des donn√©es, √©volutif automatiquement</td>
                        </tr>
                        <tr>
                            <td><strong>State</strong></td>
                            <td>Sauvegarde de l'√©tat pour loading incr√©mental</td>
                        </tr>
                    </tbody>
                </table>

                <div class="workflow-diagram">
                    <pre>
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   SOURCE    ‚îÇ       ‚îÇ   DLTHUB     ‚îÇ       ‚îÇ DESTINATION ‚îÇ
‚îÇ             ‚îÇ       ‚îÇ   PIPELINE   ‚îÇ       ‚îÇ             ‚îÇ
‚îÇ  API/DB/    ‚îÇ  ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ              ‚îÇ  ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ  BigQuery   ‚îÇ
‚îÇ  Files      ‚îÇ       ‚îÇ  Extract     ‚îÇ       ‚îÇ  Snowflake  ‚îÇ
‚îÇ             ‚îÇ       ‚îÇ  Validate    ‚îÇ       ‚îÇ  PostgreSQL ‚îÇ
‚îÇ             ‚îÇ       ‚îÇ  Transform   ‚îÇ       ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚Üì
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ    STATE     ‚îÇ
                      ‚îÇ (incremental)‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre>
                </div>
            </section>

            <!-- PARTIE 2: INSTALLATION ET PREMIERS PAS -->
            <section class="section">
                <h2>2. Installation et premier pipeline</h2>

                <h3>Installation</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash"># Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # macOS/Linux
# ou
venv\Scripts\activate  # Windows

# Installer dlt
pip install dlt

# Installer des destinations sp√©cifiques
pip install "dlt[duckdb]"      # Pour DuckDB
pip install "dlt[postgres]"    # Pour PostgreSQL
pip install "dlt[bigquery]"    # Pour BigQuery
pip install "dlt[snowflake]"   # Pour Snowflake

# V√©rifier l'installation
dlt --version</code></pre>
                </div>

                <h3>Premier pipeline : API ‚Üí DuckDB</h3>

                <p>Cr√©ons un pipeline simple qui charge des donn√©es depuis une API vers DuckDB :</p>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">"""
Premier pipeline DLT : API ‚Üí DuckDB
"""
import dlt
import requests


# 1. D√©finir une source (function qui g√©n√®re des donn√©es)
@dlt.resource(name="users", write_disposition="replace")
def get_users():
    """R√©cup√©rer des utilisateurs depuis JSONPlaceholder API"""
    response = requests.get("https://jsonplaceholder.typicode.com/users")
    yield response.json()


# 2. Cr√©er le pipeline
def run_pipeline():
    # Configurer le pipeline
    pipeline = dlt.pipeline(
        pipeline_name="api_to_duckdb",
        destination="duckdb",
        dataset_name="demo_data"
    )

    # Charger les donn√©es
    load_info = pipeline.run(get_users())

    # Afficher les r√©sultats
    print(f"‚úÖ Pipeline ex√©cut√© avec succ√®s!")
    print(f"üìä Lignes charg√©es: {load_info}")


if __name__ == "__main__":
    run_pipeline()</code></pre>
                </div>

                <h4>Ex√©cuter le pipeline</h4>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash">python first_pipeline.py

# R√©sultat
‚úÖ Pipeline ex√©cut√© avec succ√®s!
üìä Lignes charg√©es: LoadInfo(...)

# V√©rifier les donn√©es dans DuckDB
duckdb demo_data.duckdb

# Dans DuckDB:
SELECT * FROM users LIMIT 5;
.exit</code></pre>
                </div>

                <div class="alert alert-success">
                    <h4>F√©licitations !</h4>
                    <p>
                        Vous venez de cr√©er votre premier pipeline DLT en quelques lignes de code.
                        Les donn√©es de l'API sont maintenant stock√©es dans DuckDB et pr√™tes √† √™tre analys√©es.
                    </p>
                </div>

                <h3>Structure d'un projet DLT</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-bash">my_dlt_project/
‚îú‚îÄ‚îÄ .dlt/
‚îÇ   ‚îú‚îÄ‚îÄ config.toml          # Configuration (credentials)
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml         # Secrets (API keys, passwords)
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ api_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ database_pipeline.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md</code></pre>
                </div>

                <h4>Configuration (.dlt/config.toml)</h4>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-toml"># Configuration du pipeline
[runtime]
log_level = "INFO"

# Configuration de la destination
[destination.duckdb]
credentials = "demo_data.duckdb"

[destination.postgres]
credentials = "postgresql://user:password@localhost:5432/db"</code></pre>
                </div>

                <h4>Secrets (.dlt/secrets.toml)</h4>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-toml"># Ne JAMAIS commiter ce fichier !
[sources.api]
api_key = "your-secret-api-key"

[destination.bigquery.credentials]
project_id = "my-project"
private_key = "-----BEGIN PRIVATE KEY-----\n..."</code></pre>
                </div>

                <div class="alert alert-warning">
                    <h4>S√©curit√©</h4>
                    <p>
                        Ajoutez <code>.dlt/secrets.toml</code> dans votre <code>.gitignore</code> pour ne pas
                        commiter vos credentials par erreur !
                    </p>
                </div>
            </section>

            <!-- PARTIE 3: SOURCES ET RESOURCES -->
            <section class="section">
                <h2>3. Sources et Resources</h2>

                <h3>Qu'est-ce qu'une Resource ?</h3>

                <p>
                    Une <strong>resource</strong> est une unit√© de donn√©es que DLT peut charger.
                    Elle peut √™tre une fonction, un g√©n√©rateur ou un it√©rateur qui produit des donn√©es.
                </p>

                <h4>Resource simple</h4>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt

@dlt.resource
def my_data():
    """Resource simple qui retourne une liste"""
    return [
        {"id": 1, "name": "Alice", "age": 28},
        {"id": 2, "name": "Bob", "age": 32},
        {"id": 3, "name": "Charlie", "age": 25}
    ]

# Utilisation
pipeline = dlt.pipeline(
    pipeline_name="simple",
    destination="duckdb",
    dataset_name="demo"
)

pipeline.run(my_data())</code></pre>
                </div>

                <h4>Resource avec g√©n√©rateur (lazy loading)</h4>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">@dlt.resource
def paginated_api():
    """Resource avec pagination (lazy loading)"""
    page = 1
    while True:
        response = requests.get(f"https://api.example.com/data?page={page}")
        data = response.json()

        if not data:
            break

        yield data  # Yield permet de traiter les donn√©es par batch
        page += 1</code></pre>
                </div>

                <h3>Write Disposition</h3>

                <p>Le <strong>write_disposition</strong> d√©finit comment les donn√©es sont √©crites dans la destination :</p>

                <table>
                    <thead>
                        <tr>
                            <th>Mode</th>
                            <th>Description</th>
                            <th>Cas d'usage</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><code>replace</code></td>
                            <td>Remplace toutes les donn√©es</td>
                            <td>Snapshots complets, petites tables</td>
                        </tr>
                        <tr>
                            <td><code>append</code></td>
                            <td>Ajoute les nouvelles donn√©es</td>
                            <td>Logs, √©v√©nements, time-series</td>
                        </tr>
                        <tr>
                            <td><code>merge</code></td>
                            <td>Upsert (update ou insert)</td>
                            <td>Dimension tables, CDC</td>
                        </tr>
                    </tbody>
                </table>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python"># Replace : √©crase les donn√©es
@dlt.resource(write_disposition="replace")
def full_snapshot():
    return get_all_users()

# Append : ajoute aux donn√©es existantes
@dlt.resource(write_disposition="append")
def events_log():
    return get_new_events()

# Merge : upsert bas√© sur primary_key
@dlt.resource(
    write_disposition="merge",
    primary_key="user_id"
)
def user_updates():
    return get_updated_users()</code></pre>
                </div>

                <h3>Sources : Regrouper plusieurs Resources</h3>

                <p>Une <strong>source</strong> regroupe plusieurs resources li√©es :</p>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt

@dlt.source
def my_api_source(api_key: str):
    """Source qui regroupe plusieurs endpoints"""

    @dlt.resource(write_disposition="replace")
    def users():
        """R√©cup√©rer les utilisateurs"""
        response = requests.get(
            "https://api.example.com/users",
            headers={"Authorization": f"Bearer {api_key}"}
        )
        yield response.json()

    @dlt.resource(write_disposition="append")
    def orders():
        """R√©cup√©rer les commandes"""
        response = requests.get(
            "https://api.example.com/orders",
            headers={"Authorization": f"Bearer {api_key}"}
        )
        yield response.json()

    @dlt.resource(write_disposition="merge", primary_key="product_id")
    def products():
        """R√©cup√©rer les produits"""
        response = requests.get(
            "https://api.example.com/products",
            headers={"Authorization": f"Bearer {api_key}"}
        )
        yield response.json()

    # Retourner toutes les resources
    return users(), orders(), products()


# Utilisation
pipeline = dlt.pipeline(
    pipeline_name="ecommerce",
    destination="duckdb",
    dataset_name="ecommerce_data"
)

# Charger toutes les resources de la source
load_info = pipeline.run(my_api_source(api_key="secret-key"))

# Ou charger une seule resource
source = my_api_source(api_key="secret-key")
load_info = pipeline.run(source.users)</code></pre>
                </div>

                <h3>Loading incr√©mental</h3>

                <p>Le <strong>loading incr√©mental</strong> permet de ne charger que les nouvelles donn√©es :</p>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">from datetime import datetime, timedelta
import dlt

@dlt.resource(
    write_disposition="append",
    primary_key="order_id"
)
def orders_incremental(
    last_timestamp=dlt.sources.incremental("created_at")
):
    """
    Charger seulement les commandes r√©centes
    DLT sauvegarde automatiquement le dernier timestamp
    """

    # Si c'est la premi√®re ex√©cution, prendre les 7 derniers jours
    if last_timestamp.start_value is None:
        last_timestamp.start_value = datetime.now() - timedelta(days=7)

    # Requ√™ter l'API avec le timestamp
    response = requests.get(
        "https://api.example.com/orders",
        params={"since": last_timestamp.start_value.isoformat()}
    )

    orders = response.json()

    # DLT met √† jour automatiquement le timestamp
    # bas√© sur la colonne "created_at"
    yield orders


# Premi√®re ex√©cution : charge les 7 derniers jours
pipeline.run(orders_incremental())

# Deuxi√®me ex√©cution : charge seulement les nouvelles donn√©es
pipeline.run(orders_incremental())</code></pre>
                </div>

                <div class="alert alert-info">
                    <h4>State Management</h4>
                    <p>
                        DLT sauvegarde automatiquement l'√©tat (timestamps, cursors) dans la destination.
                        Vous n'avez pas besoin de g√©rer manuellement le tracking des donn√©es d√©j√† charg√©es.
                    </p>
                </div>

                <h3>Transformer les donn√©es</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">@dlt.resource
def users_transformed():
    """Resource avec transformation des donn√©es"""
    raw_data = requests.get("https://api.example.com/users").json()

    # Transformer chaque enregistrement
    for user in raw_data:
        yield {
            "user_id": user["id"],
            "full_name": f"{user['first_name']} {user['last_name']}",
            "email": user["email"].lower(),
            "age": user["age"],
            "is_active": user.get("status") == "active",
            "created_at": datetime.fromisoformat(user["created_at"]),
            # Ajouter des champs calcul√©s
            "age_group": "young" if user["age"] < 30 else "senior",
            "loaded_at": datetime.now()
        }</code></pre>
                </div>

                <div class="key-points">
                    <h3>Best Practices pour les Resources</h3>
                    <ul>
                        <li>Utilisez des g√©n√©rateurs (yield) pour les grandes quantit√©s de donn√©es</li>
                        <li>Ajoutez un primary_key pour les merge operations</li>
                        <li>Utilisez le loading incr√©mental pour √©conomiser API calls</li>
                        <li>Ajoutez des champs de m√©tadonn√©es (loaded_at, source, etc.)</li>
                        <li>G√©rez les erreurs avec try-except</li>
                        <li>Loggez les op√©rations importantes</li>
                    </ul>
                </div>
            </section>

            <!-- PARTIE 4: DESTINATIONS -->
            <section class="section">
                <h2>4. Destinations et configuration</h2>

                <h3>Destinations support√©es</h3>

                <div class="grid">
                    <div class="grid-item">
                        <h4>ü¶Ü DuckDB</h4>
                        <p>Base de donn√©es analytique locale, parfait pour le dev et tests</p>
                    </div>
                    <div class="grid-item">
                        <h4>üêò PostgreSQL</h4>
                        <p>Base de donn√©es relationnelle classique</p>
                    </div>
                    <div class="grid-item">
                        <h4>‚òÅÔ∏è BigQuery</h4>
                        <p>Data warehouse Google Cloud, serverless et scalable</p>
                    </div>
                    <div class="grid-item">
                        <h4>‚ùÑÔ∏è Snowflake</h4>
                        <p>Data warehouse cloud leader du march√©</p>
                    </div>
                    <div class="grid-item">
                        <h4>üî∑ Azure Synapse</h4>
                        <p>Data warehouse Microsoft Azure</p>
                    </div>
                    <div class="grid-item">
                        <h4>ü™£ Parquet/CSV</h4>
                        <p>Export vers fichiers pour data lakes</p>
                    </div>
                </div>

                <h3>Configuration DuckDB</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt

# M√©thode 1 : Configuration inline
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="duckdb",
    dataset_name="my_dataset"
)

# M√©thode 2 : Configuration via credentials
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination=dlt.destinations.duckdb("my_data.duckdb"),
    dataset_name="my_dataset"
)</code></pre>
                </div>

                <h3>Configuration PostgreSQL</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt

# Configuration PostgreSQL
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination=dlt.destinations.postgres(
        "postgresql://user:password@localhost:5432/mydb"
    ),
    dataset_name="analytics"
)

# Ou via .dlt/secrets.toml:
# [destination.postgres.credentials]
# database = "mydb"
# username = "user"
# password = "password"
# host = "localhost"
# port = 5432

pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="postgres",
    dataset_name="analytics"
)</code></pre>
                </div>

                <h3>Configuration BigQuery</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt

# Configuration BigQuery
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="bigquery",
    dataset_name="analytics"
)

# Fichier .dlt/secrets.toml :
# [destination.bigquery.credentials]
# project_id = "my-gcp-project"
# private_key = "-----BEGIN PRIVATE KEY-----\n..."
# client_email = "service-account@project.iam.gserviceaccount.com"

# Ou utiliser GOOGLE_APPLICATION_CREDENTIALS
# export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"</code></pre>
                </div>

                <h3>Configuration Snowflake</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt

# Fichier .dlt/secrets.toml :
# [destination.snowflake.credentials]
# database = "MY_DATABASE"
# username = "MY_USER"
# password = "MY_PASSWORD"
# host = "account.snowflakecomputing.com"
# warehouse = "MY_WAREHOUSE"
# role = "MY_ROLE"

pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="snowflake",
    dataset_name="analytics"
)</code></pre>
                </div>

                <h3>Export vers fichiers (Data Lake)</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt

# Export vers Parquet
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination=dlt.destinations.filesystem("/data/lake"),
    dataset_name="raw_data"
)

# Configuration dans .dlt/config.toml :
# [destination.filesystem]
# bucket_url = "s3://my-bucket/data"  # S3
# # ou
# bucket_url = "gs://my-bucket/data"  # GCS
# # ou
# bucket_url = "az://my-container/data"  # Azure Blob

# Layout des fichiers :
# /data/lake/
#   ‚îú‚îÄ‚îÄ users/
#   ‚îÇ   ‚îú‚îÄ‚îÄ 2024-01-15_001.parquet
#   ‚îÇ   ‚îî‚îÄ‚îÄ 2024-01-16_001.parquet
#   ‚îî‚îÄ‚îÄ orders/
#       ‚îî‚îÄ‚îÄ 2024-01-15_001.parquet</code></pre>
                </div>

                <div class="alert alert-info">
                    <h4>Environnements multiples</h4>
                    <p>
                        Utilisez diff√©rents fichiers de configuration pour dev, staging et production :
                    </p>
                    <ul>
                        <li><code>.dlt/config.dev.toml</code></li>
                        <li><code>.dlt/config.staging.toml</code></li>
                        <li><code>.dlt/config.prod.toml</code></li>
                    </ul>
                </div>
            </section>

            <!-- PARTIE 5: SCH√âMAS ET VALIDATION -->
            <section class="section">
                <h2>5. Sch√©mas et validation des donn√©es</h2>

                <h3>Schema Evolution automatique</h3>

                <p>
                    DLT g√®re automatiquement l'√©volution du sch√©ma. Si de nouvelles colonnes apparaissent,
                    elles sont ajout√©es automatiquement sans casser le pipeline.
                </p>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python"># Premi√®re ex√©cution : 3 colonnes
data_v1 = [
    {"id": 1, "name": "Alice", "age": 28}
]
pipeline.run(data_v1, table_name="users")

# Deuxi√®me ex√©cution : nouvelle colonne "email"
# DLT ajoute automatiquement la colonne
data_v2 = [
    {"id": 2, "name": "Bob", "age": 32, "email": "bob@example.com"}
]
pipeline.run(data_v2, table_name="users")

# R√©sultat dans la table :
# id | name  | age | email
# 1  | Alice | 28  | NULL
# 2  | Bob   | 32  | bob@example.com</code></pre>
                </div>

                <h3>D√©finir un sch√©ma explicite</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">from dlt.common.schema import TColumnSchema

@dlt.resource(
    columns={
        "user_id": {
            "data_type": "bigint",
            "nullable": False,
            "primary_key": True
        },
        "email": {
            "data_type": "text",
            "nullable": False,
            "unique": True
        },
        "age": {
            "data_type": "bigint",
            "nullable": True
        },
        "created_at": {
            "data_type": "timestamp",
            "nullable": False
        }
    }
)
def users_with_schema():
    """Resource avec sch√©ma explicite"""
    return get_users_data()</code></pre>
                </div>

                <h3>Validation des donn√©es</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">import dlt
from dlt.common.typing import TDataItem

@dlt.resource
def validated_users():
    """Resource avec validation custom"""
    raw_data = get_raw_users()

    for user in raw_data:
        # Validation manuelle
        if not user.get("email") or "@" not in user["email"]:
            print(f"‚ö†Ô∏è Email invalide pour user {user.get('id')}")
            continue  # Skip cet enregistrement

        if user.get("age") and user["age"] < 0:
            print(f"‚ö†Ô∏è Age n√©gatif pour user {user.get('id')}")
            user["age"] = None  # Corriger la donn√©e

        yield user


# Ou utiliser Pydantic pour la validation
from pydantic import BaseModel, EmailStr, validator

class UserModel(BaseModel):
    """Mod√®le Pydantic pour validation"""
    user_id: int
    email: EmailStr
    age: int

    @validator('age')
    def age_must_be_positive(cls, v):
        if v < 0:
            raise ValueError('Age must be positive')
        return v


@dlt.resource
def users_pydantic():
    """Validation avec Pydantic"""
    raw_data = get_raw_users()

    for user in raw_data:
        try:
            # Valider avec Pydantic
            validated = UserModel(**user)
            yield validated.dict()
        except Exception as e:
            print(f"‚ùå Validation error: {e}")
            # Logger ou rejeter</code></pre>
                </div>

                <h3>Data Contracts</h3>

                <p>Les <strong>data contracts</strong> permettent de d√©finir des r√®gles strictes sur les donn√©es :</p>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">@dlt.resource(
    columns={
        "user_id": {"data_type": "bigint", "nullable": False},
        "email": {"data_type": "text", "nullable": False}
    },
    schema_contract={
        "tables": "evolve",      # Nouvelles tables autoris√©es
        "columns": "freeze",     # Nouvelles colonnes interdites
        "data_type": "freeze"    # Changement de type interdit
    }
)
def strict_users():
    """
    Schema contract strict :
    - Nouvelles colonnes = erreur
    - Changement de type = erreur
    """
    return get_users()</code></pre>
                </div>

                <div class="alert alert-warning">
                    <h4>Attention</h4>
                    <p>
                        Les data contracts stricts peuvent casser vos pipelines si l'API source change.
                        Utilisez-les seulement quand vous contr√¥lez la source ou avez des SLAs stricts.
                    </p>
                </div>
            </section>

            <!-- PARTIE 6: PATTERNS AVANC√âS -->
            <section class="section">
                <h2>6. Patterns avanc√©s et production</h2>

                <h3>Pipeline complet production-ready</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">"""
Pipeline production-ready avec gestion d'erreurs et monitoring
"""
import dlt
import requests
from datetime import datetime
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dlt.source
def ecommerce_source(api_key: str, base_url: str):
    """Source e-commerce compl√®te"""

    @dlt.resource(
        write_disposition="merge",
        primary_key="user_id",
        merge_key="user_id"
    )
    def users():
        """Utilisateurs avec upsert"""
        try:
            logger.info("üì• Fetching users...")
            response = requests.get(
                f"{base_url}/users",
                headers={"Authorization": f"Bearer {api_key}"},
                timeout=30
            )
            response.raise_for_status()

            data = response.json()
            logger.info(f"‚úÖ Fetched {len(data)} users")

            # Enrichir avec m√©tadonn√©es
            for user in data:
                user["_loaded_at"] = datetime.now()
                user["_source"] = "api"
                yield user

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error fetching users: {e}")
            raise

    @dlt.resource(
        write_disposition="append",
        primary_key="order_id"
    )
    def orders(last_date=dlt.sources.incremental("created_at")):
        """Commandes avec loading incr√©mental"""
        try:
            logger.info(f"üì• Fetching orders since {last_date.start_value}...")

            params = {}
            if last_date.start_value:
                params["since"] = last_date.start_value.isoformat()

            response = requests.get(
                f"{base_url}/orders",
                headers={"Authorization": f"Bearer {api_key}"},
                params=params,
                timeout=30
            )
            response.raise_for_status()

            data = response.json()
            logger.info(f"‚úÖ Fetched {len(data)} orders")

            for order in data:
                order["_loaded_at"] = datetime.now()
                yield order

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error fetching orders: {e}")
            raise

    return users(), orders()


def run_pipeline():
    """Ex√©cuter le pipeline avec monitoring"""
    logger.info("üöÄ Starting pipeline...")

    # Cr√©er le pipeline
    pipeline = dlt.pipeline(
        pipeline_name="ecommerce_prod",
        destination="bigquery",
        dataset_name="analytics"
    )

    try:
        # R√©cup√©rer credentials depuis les secrets
        api_key = dlt.secrets.value["sources.ecommerce.api_key"]
        base_url = dlt.config.value["sources.ecommerce.base_url"]

        # Ex√©cuter le pipeline
        source = ecommerce_source(api_key=api_key, base_url=base_url)
        load_info = pipeline.run(source)

        # Logger les r√©sultats
        logger.info("‚úÖ Pipeline completed successfully!")
        logger.info(f"üìä Load info: {load_info}")

        # V√©rifier les erreurs
        if load_info.has_failed_jobs:
            logger.error("‚ùå Some jobs failed!")
            for job in load_info.load_packages[0].jobs["failed_jobs"]:
                logger.error(f"Failed job: {job}")
            raise Exception("Pipeline had failed jobs")

        return load_info

    except Exception as e:
        logger.error(f"‚ùå Pipeline failed: {e}")
        # Envoyer une alerte (Slack, email, etc.)
        send_alert(f"Pipeline failed: {e}")
        raise


def send_alert(message: str):
    """Envoyer une alerte en cas d'erreur"""
    # Slack webhook
    webhook_url = dlt.secrets.value.get("alerts.slack_webhook")
    if webhook_url:
        requests.post(webhook_url, json={"text": message})


if __name__ == "__main__":
    run_pipeline()</code></pre>
                </div>

                <h3>D√©ploiement avec Airflow</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">"""
DAG Airflow pour ex√©cuter un pipeline DLT
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import dlt


def run_dlt_pipeline():
    """Fonction appel√©e par Airflow"""
    pipeline = dlt.pipeline(
        pipeline_name="ecommerce",
        destination="bigquery",
        dataset_name="analytics"
    )

    # Importer votre source
    from pipelines.ecommerce import ecommerce_source

    api_key = dlt.secrets.value["sources.ecommerce.api_key"]
    source = ecommerce_source(api_key=api_key)

    load_info = pipeline.run(source)

    if load_info.has_failed_jobs:
        raise Exception("Pipeline had failed jobs")


# D√©finir le DAG
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email': ['alerts@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'dlt_ecommerce_pipeline',
    default_args=default_args,
    description='Load ecommerce data with DLT',
    schedule_interval='@hourly',  # Toutes les heures
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['dlt', 'ecommerce']
) as dag:

    run_pipeline_task = PythonOperator(
        task_id='run_dlt_pipeline',
        python_callable=run_dlt_pipeline
    )</code></pre>
                </div>

                <h3>Tests unitaires</h3>

                <div class="code-container">
                    <div class="code-header">
                        <span class="code-dot red"></span>
                        <span class="code-dot yellow"></span>
                        <span class="code-dot green"></span>
                    </div>
                    <pre><code class="language-python">"""
Tests pour les pipelines DLT
"""
import pytest
import dlt


def test_users_resource():
    """Tester la resource users"""
    from pipelines.ecommerce import ecommerce_source

    # Mock API
    api_key = "test-key"
    base_url = "https://api.test.com"

    source = ecommerce_source(api_key=api_key, base_url=base_url)

    # Tester avec DuckDB (destination de test)
    pipeline = dlt.pipeline(
        pipeline_name="test_pipeline",
        destination="duckdb",
        dataset_name="test_data",
        full_refresh=True  # Nettoyer entre chaque test
    )

    load_info = pipeline.run(source.users)

    assert not load_info.has_failed_jobs, "Pipeline should not have failed jobs"

    # V√©rifier les donn√©es charg√©es
    with pipeline.sql_client() as client:
        result = client.execute_sql("SELECT COUNT(*) as count FROM users")
        count = result[0][0]
        assert count > 0, "Should have loaded users"


def test_schema_validation():
    """Tester la validation du sch√©ma"""
    from pipelines.ecommerce import users_resource

    # Donn√©es de test
    test_data = [
        {"user_id": 1, "email": "test@example.com", "age": 25},
        {"user_id": 2, "email": "invalid-email", "age": -5}  # Invalide
    ]

    # V√©rifier que les donn√©es invalides sont rejet√©es
    # ... votre logique de test</code></pre>
                </div>

                <div class="key-points">
                    <h3>Checklist Production</h3>
                    <ul>
                        <li>Gestion d'erreurs compl√®te (try-except)</li>
                        <li>Logging d√©taill√© de toutes les op√©rations</li>
                        <li>Alertes en cas d'√©chec (Slack, email)</li>
                        <li>Loading incr√©mental pour √©conomiser ressources</li>
                        <li>Monitoring des m√©triques (lignes charg√©es, dur√©e)</li>
                        <li>Tests unitaires et d'int√©gration</li>
                        <li>Documentation du pipeline</li>
                        <li>Secrets s√©curis√©s (jamais en dur dans le code)</li>
                        <li>Retries automatiques sur erreurs transitoires</li>
                        <li>CI/CD pour d√©ploiement automatique</li>
                    </ul>
                </div>
            </section>

            <!-- RESSOURCES -->
            <section class="section">
                <h2>üìö Ressources et liens utiles</h2>

                <div class="resources">
                    <a href="https://dlthub.com/docs" target="_blank" class="resource-link">
                        <strong>Documentation officielle DLT</strong>
                        <p>Documentation compl√®te, guides et r√©f√©rences</p>
                    </a>

                    <a href="https://github.com/dlt-hub/dlt" target="_blank" class="resource-link">
                        <strong>DLT sur GitHub</strong>
                        <p>Code source, issues et contributions</p>
                    </a>

                    <a href="https://dlthub.com/docs/examples" target="_blank" class="resource-link">
                        <strong>Exemples de pipelines</strong>
                        <p>Exemples pr√™ts √† l'emploi pour diff√©rentes sources</p>
                    </a>

                    <a href="https://dlthub.com/docs/reference" target="_blank" class="resource-link">
                        <strong>API Reference</strong>
                        <p>Documentation d√©taill√©e de l'API DLT</p>
                    </a>

                    <a href="https://dlthub.com/docs/blog" target="_blank" class="resource-link">
                        <strong>Blog DLT</strong>
                        <p>Articles, tutorials et best practices</p>
                    </a>

                    <a href="https://discord.gg/dlthub" target="_blank" class="resource-link">
                        <strong>Discord Community</strong>
                        <p>Aide, discussions et support communautaire</p>
                    </a>
                </div>

                <div class="alert alert-info" style="margin-top: 40px;">
                    <h4>Prochaines √©tapes</h4>
                    <p>Maintenant que vous ma√Ætrisez DLT, explorez :</p>
                    <ul>
                        <li><strong>Sources v√©rifi√©es</strong> : Utilisez les sources pr√©-construites (GitHub, Stripe, etc.)</li>
                        <li><strong>dbt integration</strong> : Transformez vos donn√©es avec dbt apr√®s le load</li>
                        <li><strong>Orchestration</strong> : D√©ployez avec Airflow, Dagster ou Prefect</li>
                        <li><strong>Monitoring</strong> : Int√©grez avec DataDog, Prometheus pour le monitoring</li>
                        <li><strong>Data Quality</strong> : Ajoutez Great Expectations pour la validation</li>
                        <li><strong>Reverse ETL</strong> : Chargez des donn√©es depuis votre warehouse vers des apps</li>
                    </ul>
                </div>
            </section>
        </div>

        <footer>
            <p>Formation DltHub pour Data Engineering - 2024</p>
            <p>Pour toute question ou suggestion : <a href="mailto:formation@example.com">formation@example.com</a></p>
        </footer>
    </div>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-toml.min.js"></script>
</body>
</html>
