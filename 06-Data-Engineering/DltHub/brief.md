# âš¡ Brief Pratique DltHub - Data Engineering

**DurÃ©e estimÃ©e :** 6 heures
**Niveau :** IntermÃ©diaire
**ModalitÃ© :** Pratique individuelle

---

## ğŸ¯ Objectifs du Brief

Ã€ l'issue de ce brief, vous serez capable de :
- CrÃ©er des pipelines DLT de A Ã  Z
- Extraire des donnÃ©es de diverses sources (API, fichiers, bases de donnÃ©es)
- Charger vers diffÃ©rentes destinations (DuckDB, PostgreSQL, BigQuery)
- ImplÃ©menter le loading incrÃ©mental
- GÃ©rer le schema evolution et la validation
- DÃ©ployer un pipeline en production avec monitoring

---

## ğŸ“‹ Contexte

Vous Ãªtes Data Engineer chez **StreamData Solutions**. L'entreprise a besoin de mettre en place des pipelines de donnÃ©es modernes pour ingÃ©rer des donnÃ©es depuis plusieurs sources (APIs, bases de donnÃ©es) vers un data warehouse. Vous avez choisi **DltHub** pour sa simplicitÃ© et sa flexibilitÃ©.

---

## ğŸš€ Partie 1 : Installation et premier pipeline (1h)

### TÃ¢che 1.1 : Setup de l'environnement

**Instructions :**

1. **CrÃ©er un projet DLT :**

```bash
# CrÃ©er un dossier pour le projet
mkdir dlthub-data-pipeline
cd dlthub-data-pipeline

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # macOS/Linux
# ou venv\Scripts\activate  # Windows
```

2. **Installer les dÃ©pendances :**

CrÃ©er un fichier `requirements.txt` :

```
dlt[duckdb]==0.4.0
dlt[postgres]
pandas==2.1.0
requests==2.31.0
python-dotenv==1.0.0
faker==20.0.0
```

```bash
pip install -r requirements.txt
```

3. **Initialiser la structure DLT :**

```bash
# CrÃ©er la structure des dossiers
mkdir -p .dlt pipelines data

# CrÃ©er les fichiers de configuration
touch .dlt/config.toml .dlt/secrets.toml .gitignore
```

4. **Fichier `.gitignore` :**

```
venv/
.dlt/secrets.toml
*.duckdb
*.duckdb.wal
__pycache__/
*.pyc
.env
data/
```

5. **Configuration `.dlt/config.toml` :**

```toml
[runtime]
log_level = "INFO"

[sources.jsonplaceholder]
base_url = "https://jsonplaceholder.typicode.com"
```

**CritÃ¨res de validation :**
- âœ… Environnement virtuel crÃ©Ã© et activÃ©
- âœ… DLT installÃ© (`dlt --version`)
- âœ… Structure des dossiers crÃ©Ã©e
- âœ… Fichiers de configuration initialisÃ©s

---

### TÃ¢che 1.2 : Premier pipeline API â†’ DuckDB

**ScÃ©nario :** Extraire des utilisateurs depuis l'API JSONPlaceholder et les charger dans DuckDB.

**Instructions :**

CrÃ©er `pipelines/first_pipeline.py` :

```python
"""
Premier pipeline DLT : API â†’ DuckDB
"""
import dlt
import requests


@dlt.resource(name="users", write_disposition="replace")
def get_users():
    """RÃ©cupÃ©rer les utilisateurs depuis JSONPlaceholder"""
    print("ğŸ“¥ Fetching users from API...")

    response = requests.get("https://jsonplaceholder.typicode.com/users")
    users = response.json()

    print(f"âœ… Fetched {len(users)} users")
    yield users


@dlt.resource(name="posts", write_disposition="replace")
def get_posts():
    """RÃ©cupÃ©rer les posts"""
    print("ğŸ“¥ Fetching posts from API...")

    response = requests.get("https://jsonplaceholder.typicode.com/posts")
    posts = response.json()

    print(f"âœ… Fetched {len(posts)} posts")
    yield posts


def run():
    """ExÃ©cuter le pipeline"""
    print("ğŸš€ Starting first DLT pipeline...\n")

    # CrÃ©er le pipeline
    pipeline = dlt.pipeline(
        pipeline_name="jsonplaceholder_api",
        destination="duckdb",
        dataset_name="jsonplaceholder_data"
    )

    # Charger les donnÃ©es
    load_info = pipeline.run([get_users(), get_posts()])

    print(f"\nâœ… Pipeline completed successfully!")
    print(f"ğŸ“Š Load info: {load_info}")


if __name__ == "__main__":
    run()
```

**ExÃ©cuter le pipeline :**

```bash
python pipelines/first_pipeline.py
```

**VÃ©rifier les donnÃ©es :**

```bash
# Ouvrir DuckDB
duckdb jsonplaceholder_data.duckdb

# Dans DuckDB:
SHOW TABLES;
SELECT * FROM users LIMIT 5;
SELECT * FROM posts LIMIT 5;
.exit
```

**CritÃ¨res de validation :**
- âœ… Pipeline s'exÃ©cute sans erreur
- âœ… Fichier `jsonplaceholder_data.duckdb` crÃ©Ã©
- âœ… Tables `users` et `posts` prÃ©sentes
- âœ… DonnÃ©es correctement chargÃ©es

---

### TÃ¢che 1.3 : Enrichir les donnÃ©es

**Instructions :**

Modifier `pipelines/first_pipeline.py` pour enrichir les donnÃ©es :

```python
from datetime import datetime

@dlt.resource(name="users_enriched", write_disposition="replace")
def get_users_enriched():
    """RÃ©cupÃ©rer et enrichir les utilisateurs"""
    print("ğŸ“¥ Fetching users from API...")

    response = requests.get("https://jsonplaceholder.typicode.com/users")
    users = response.json()

    # Enrichir chaque utilisateur
    enriched_users = []
    for user in users:
        enriched = {
            "user_id": user["id"],
            "name": user["name"],
            "username": user["username"],
            "email": user["email"].lower(),
            "city": user["address"]["city"],
            "latitude": float(user["address"]["geo"]["lat"]),
            "longitude": float(user["address"]["geo"]["lng"]),
            "company_name": user["company"]["name"],
            "company_bs": user["company"]["bs"],
            # MÃ©tadonnÃ©es
            "loaded_at": datetime.now(),
            "source": "jsonplaceholder_api"
        }
        enriched_users.append(enriched)

    print(f"âœ… Enriched {len(enriched_users)} users")
    yield enriched_users
```

**CritÃ¨res de validation :**
- âœ… DonnÃ©es aplaties (nested â†’ flat)
- âœ… Champs calculÃ©s ajoutÃ©s (loaded_at, source)
- âœ… Emails en minuscules
- âœ… GÃ©olocalisation extraite

---

## ğŸ“Š Partie 2 : Sources et destinations multiples (1h30)

### TÃ¢che 2.1 : CrÃ©er une source complÃ¨te

**Instructions :**

CrÃ©er `pipelines/ecommerce_source.py` :

```python
"""
Source e-commerce complÃ¨te avec plusieurs resources
"""
import dlt
import requests
from datetime import datetime
from typing import Iterator, Dict, Any


@dlt.source
def ecommerce_api(base_url: str = "https://fakestoreapi.com"):
    """
    Source e-commerce avec plusieurs resources
    """

    @dlt.resource(
        name="products",
        write_disposition="replace",
        primary_key="id"
    )
    def get_products() -> Iterator[Dict[str, Any]]:
        """RÃ©cupÃ©rer les produits"""
        print("ğŸ“¥ Fetching products...")

        response = requests.get(f"{base_url}/products")
        products = response.json()

        for product in products:
            yield {
                "product_id": product["id"],
                "title": product["title"],
                "price": product["price"],
                "category": product["category"],
                "description": product["description"][:100],  # Tronquer
                "rating_rate": product["rating"]["rate"],
                "rating_count": product["rating"]["count"],
                "loaded_at": datetime.now()
            }

    @dlt.resource(
        name="categories",
        write_disposition="replace"
    )
    def get_categories() -> Iterator[Dict[str, Any]]:
        """RÃ©cupÃ©rer les catÃ©gories"""
        print("ğŸ“¥ Fetching categories...")

        response = requests.get(f"{base_url}/products/categories")
        categories = response.json()

        for i, category in enumerate(categories, 1):
            yield {
                "category_id": i,
                "category_name": category,
                "loaded_at": datetime.now()
            }

    @dlt.resource(
        name="users",
        write_disposition="replace",
        primary_key="user_id"
    )
    def get_users() -> Iterator[Dict[str, Any]]:
        """RÃ©cupÃ©rer les utilisateurs"""
        print("ğŸ“¥ Fetching users...")

        response = requests.get(f"{base_url}/users")
        users = response.json()

        for user in users:
            yield {
                "user_id": user["id"],
                "email": user["email"].lower(),
                "username": user["username"],
                "name_firstname": user["name"]["firstname"],
                "name_lastname": user["name"]["lastname"],
                "phone": user["phone"],
                "city": user["address"]["city"],
                "loaded_at": datetime.now()
            }

    return get_products(), get_categories(), get_users()


def run_ecommerce_pipeline():
    """ExÃ©cuter le pipeline e-commerce"""
    print("ğŸš€ Starting e-commerce pipeline...\n")

    pipeline = dlt.pipeline(
        pipeline_name="ecommerce",
        destination="duckdb",
        dataset_name="ecommerce_data"
    )

    # Charger toutes les resources de la source
    source = ecommerce_api()
    load_info = pipeline.run(source)

    print(f"\nâœ… Pipeline completed!")
    print(f"ğŸ“Š {load_info}")


if __name__ == "__main__":
    run_ecommerce_pipeline()
```

**ExÃ©cuter :**

```bash
python pipelines/ecommerce_source.py
```

**CritÃ¨res de validation :**
- âœ… 3 tables crÃ©Ã©es : products, categories, users
- âœ… DonnÃ©es enrichies et aplaties
- âœ… Primary keys dÃ©finis

---

### TÃ¢che 2.2 : Charger vers PostgreSQL

**ScÃ©nario :** Maintenant charger les mÃªmes donnÃ©es vers PostgreSQL au lieu de DuckDB.

**Instructions :**

1. **DÃ©marrer PostgreSQL avec Docker :**

```bash
docker run -d \
  --name postgres_dlt \
  -p 5432:5432 \
  -e POSTGRES_USER=dlt_user \
  -e POSTGRES_PASSWORD=dlt_password \
  -e POSTGRES_DB=dlt_data \
  postgres:15
```

2. **Configurer les credentials dans `.dlt/secrets.toml` :**

```toml
[destination.postgres.credentials]
database = "dlt_data"
username = "dlt_user"
password = "dlt_password"
host = "localhost"
port = 5432
```

3. **CrÃ©er `pipelines/ecommerce_to_postgres.py` :**

```python
"""
Pipeline e-commerce vers PostgreSQL
"""
import dlt
from ecommerce_source import ecommerce_api


def run():
    """ExÃ©cuter le pipeline vers PostgreSQL"""
    print("ğŸš€ Starting pipeline to PostgreSQL...\n")

    pipeline = dlt.pipeline(
        pipeline_name="ecommerce_postgres",
        destination="postgres",
        dataset_name="ecommerce"
    )

    source = ecommerce_api()
    load_info = pipeline.run(source)

    print(f"\nâœ… Data loaded to PostgreSQL!")
    print(f"ğŸ“Š {load_info}")


if __name__ == "__main__":
    run()
```

4. **ExÃ©cuter :**

```bash
python pipelines/ecommerce_to_postgres.py
```

5. **VÃ©rifier dans PostgreSQL :**

```bash
docker exec -it postgres_dlt psql -U dlt_user -d dlt_data

# Dans psql:
\dt ecommerce.*
SELECT * FROM ecommerce.products LIMIT 5;
\q
```

**CritÃ¨res de validation :**
- âœ… PostgreSQL running
- âœ… DonnÃ©es chargÃ©es dans PostgreSQL
- âœ… Schema `ecommerce` crÃ©Ã©
- âœ… 3 tables prÃ©sentes

---

### TÃ¢che 2.3 : GÃ©nÃ©rer des donnÃ©es fictives avec Faker

**Instructions :**

CrÃ©er `pipelines/sales_generator.py` :

```python
"""
GÃ©nÃ©rer des ventes fictives avec Faker
"""
import dlt
from faker import Faker
from datetime import datetime, timedelta
import random

fake = Faker()


@dlt.resource(
    name="sales",
    write_disposition="append",
    primary_key="sale_id"
)
def generate_sales(num_sales: int = 1000):
    """GÃ©nÃ©rer des ventes fictives"""
    print(f"ğŸ“Š Generating {num_sales} fake sales...")

    for i in range(1, num_sales + 1):
        sale_date = fake.date_time_between(
            start_date="-30d",
            end_date="now"
        )

        yield {
            "sale_id": i,
            "customer_name": fake.name(),
            "customer_email": fake.email(),
            "product": random.choice([
                "Laptop", "Mouse", "Keyboard",
                "Monitor", "Headset", "Webcam"
            ]),
            "quantity": random.randint(1, 5),
            "unit_price": round(random.uniform(20, 1500), 2),
            "total_amount": 0,  # CalculÃ© aprÃ¨s
            "sale_date": sale_date,
            "status": random.choice(["completed", "pending", "cancelled"]),
            "payment_method": random.choice([
                "Credit Card", "PayPal", "Bank Transfer"
            ]),
            "country": fake.country(),
            "city": fake.city(),
            "loaded_at": datetime.now()
        }


@dlt.transformer(
    name="sales_with_total",
    write_disposition="replace",
    primary_key="sale_id"
)
def calculate_total(sales):
    """Transformer : calculer le total"""
    for sale in sales:
        sale["total_amount"] = round(
            sale["quantity"] * sale["unit_price"],
            2
        )
        yield sale


def run():
    """ExÃ©cuter le gÃ©nÃ©rateur"""
    print("ğŸš€ Generating and loading sales data...\n")

    pipeline = dlt.pipeline(
        pipeline_name="sales_generator",
        destination="duckdb",
        dataset_name="sales_data"
    )

    # GÃ©nÃ©rer et transformer
    sales = generate_sales(num_sales=1000)
    sales_transformed = calculate_total(sales)

    load_info = pipeline.run(sales_transformed)

    print(f"\nâœ… Sales data generated and loaded!")
    print(f"ğŸ“Š {load_info}")


if __name__ == "__main__":
    run()
```

**ExÃ©cuter :**

```bash
python pipelines/sales_generator.py
```

**CritÃ¨res de validation :**
- âœ… 1000 ventes gÃ©nÃ©rÃ©es
- âœ… Total calculÃ© automatiquement
- âœ… DonnÃ©es rÃ©alistes avec Faker
- âœ… Table `sales_with_total` crÃ©Ã©e

---

## ğŸ”„ Partie 3 : Loading incrÃ©mental et state (1h30)

### TÃ¢che 3.1 : ImplÃ©menter le loading incrÃ©mental

**ScÃ©nario :** Charger seulement les nouvelles commandes depuis une API.

**Instructions :**

CrÃ©er `pipelines/incremental_orders.py` :

```python
"""
Pipeline incrÃ©mental : charger seulement les nouvelles donnÃ©es
"""
import dlt
import requests
from datetime import datetime, timedelta
from typing import Iterator, Dict, Any


@dlt.resource(
    name="orders",
    write_disposition="append",
    primary_key="order_id"
)
def get_orders_incremental(
    last_date=dlt.sources.incremental("order_date")
):
    """
    RÃ©cupÃ©rer les commandes de maniÃ¨re incrÃ©mentale
    DLT sauvegarde automatiquement le dernier order_date
    """
    print(f"ğŸ“¥ Fetching orders...")

    # Si premiÃ¨re exÃ©cution, commencer Ã  30 jours en arriÃ¨re
    if last_date.start_value is None:
        start_date = datetime.now() - timedelta(days=30)
        print(f"âš ï¸ First run, fetching from {start_date.date()}")
    else:
        start_date = last_date.start_value
        print(f"ğŸ“… Fetching orders since {start_date}")

    # Simuler des commandes (normalement vous appelleriez une vraie API)
    # Pour la dÃ©mo, on gÃ©nÃ¨re des donnÃ©es
    from faker import Faker
    import random

    fake = Faker()
    current_date = start_date

    orders = []
    while current_date <= datetime.now():
        # GÃ©nÃ©rer 5-10 commandes par jour
        for _ in range(random.randint(5, 10)):
            order = {
                "order_id": fake.uuid4(),
                "customer_name": fake.name(),
                "customer_email": fake.email(),
                "product": random.choice(["Product A", "Product B", "Product C"]),
                "amount": round(random.uniform(10, 500), 2),
                "order_date": current_date,
                "status": random.choice(["completed", "pending"]),
                "loaded_at": datetime.now()
            }
            orders.append(order)

        current_date += timedelta(days=1)

    print(f"âœ… Fetched {len(orders)} orders")
    yield orders


def run():
    """ExÃ©cuter le pipeline incrÃ©mental"""
    print("ğŸš€ Starting incremental pipeline...\n")

    pipeline = dlt.pipeline(
        pipeline_name="orders_incremental",
        destination="duckdb",
        dataset_name="orders_data"
    )

    load_info = pipeline.run(get_orders_incremental())

    print(f"\nâœ… Pipeline completed!")
    print(f"ğŸ“Š Loaded {load_info}")


if __name__ == "__main__":
    # PremiÃ¨re exÃ©cution : charge 30 jours
    print("=" * 50)
    print("PREMIÃˆRE EXÃ‰CUTION")
    print("=" * 50)
    run()

    # Attendre un peu
    import time
    time.sleep(2)

    # DeuxiÃ¨me exÃ©cution : charge seulement les nouveaux
    print("\n" + "=" * 50)
    print("DEUXIÃˆME EXÃ‰CUTION (IncrÃ©mental)")
    print("=" * 50)
    run()
```

**ExÃ©cuter :**

```bash
python pipelines/incremental_orders.py
```

**VÃ©rifier le state :**

```bash
# Le state est sauvegardÃ© dans DuckDB
duckdb orders_data.duckdb

SELECT * FROM _dlt_pipeline_state;
.exit
```

**CritÃ¨res de validation :**
- âœ… PremiÃ¨re exÃ©cution charge 30 jours de donnÃ©es
- âœ… DeuxiÃ¨me exÃ©cution charge seulement les nouvelles
- âœ… State sauvegardÃ© automatiquement
- âœ… Pas de doublons

---

### TÃ¢che 3.2 : Loading incrÃ©mental avec plusieurs cursors

**Instructions :**

CrÃ©er `pipelines/multi_cursor_incremental.py` :

```python
"""
Loading incrÃ©mental avec plusieurs cursors
"""
import dlt
from datetime import datetime, timedelta
from faker import Faker
import random

fake = Faker()


@dlt.resource(
    name="user_activities",
    write_disposition="append",
    primary_key=["user_id", "activity_date"]
)
def get_user_activities(
    last_date=dlt.sources.incremental("activity_date"),
    last_user_id=dlt.sources.incremental("user_id")
):
    """
    Loading incrÃ©mental avec deux cursors :
    - activity_date : date de l'activitÃ©
    - user_id : ID utilisateur
    """
    print(f"ğŸ“¥ Fetching user activities...")

    # Valeurs de dÃ©part
    if last_date.start_value is None:
        start_date = datetime.now() - timedelta(days=7)
    else:
        start_date = last_date.start_value

    if last_user_id.start_value is None:
        start_user_id = 1
    else:
        start_user_id = last_user_id.start_value

    print(f"ğŸ“… From date: {start_date.date()}")
    print(f"ğŸ‘¤ From user_id: {start_user_id}")

    # GÃ©nÃ©rer des activitÃ©s
    activities = []
    for day in range(7):
        activity_date = start_date + timedelta(days=day)

        for user_id in range(start_user_id, start_user_id + 10):
            activity = {
                "user_id": user_id,
                "activity_date": activity_date,
                "activity_type": random.choice(["login", "purchase", "view", "click"]),
                "duration_seconds": random.randint(10, 3600),
                "loaded_at": datetime.now()
            }
            activities.append(activity)

    print(f"âœ… Generated {len(activities)} activities")
    yield activities


def run():
    """ExÃ©cuter le pipeline"""
    pipeline = dlt.pipeline(
        pipeline_name="user_activities",
        destination="duckdb",
        dataset_name="activities_data"
    )

    load_info = pipeline.run(get_user_activities())
    print(f"ğŸ“Š {load_info}")


if __name__ == "__main__":
    run()
```

**CritÃ¨res de validation :**
- âœ… Deux cursors utilisÃ©s (date + user_id)
- âœ… State sauvegardÃ© pour les deux
- âœ… Loading incrÃ©mental fonctionne

---

## ğŸ“‹ Partie 4 : Validation et qualitÃ© des donnÃ©es (1h)

### TÃ¢che 4.1 : Validation avec Pydantic

**Instructions :**

CrÃ©er `pipelines/validated_pipeline.py` :

```python
"""
Pipeline avec validation Pydantic
"""
import dlt
import requests
from pydantic import BaseModel, EmailStr, validator, Field
from typing import Optional
from datetime import datetime


class UserModel(BaseModel):
    """ModÃ¨le de validation pour un utilisateur"""
    user_id: int = Field(..., alias="id")
    name: str
    username: str
    email: EmailStr
    phone: str
    city: str = Field(..., alias="address__city")
    latitude: float = Field(..., alias="address__geo__lat")
    longitude: float = Field(..., alias="address__geo__lng")
    company_name: str = Field(..., alias="company__name")

    @validator('name')
    def name_must_not_be_empty(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('Name cannot be empty')
        return v.strip()

    @validator('latitude')
    def latitude_must_be_valid(cls, v):
        if not -90 <= v <= 90:
            raise ValueError('Latitude must be between -90 and 90')
        return v

    @validator('longitude')
    def longitude_must_be_valid(cls, v):
        if not -180 <= v <= 180:
            raise ValueError('Longitude must be between -180 and 180')
        return v

    class Config:
        allow_population_by_field_name = True


@dlt.resource(
    name="validated_users",
    write_disposition="replace",
    primary_key="user_id"
)
def get_validated_users():
    """
    Resource avec validation Pydantic
    """
    print("ğŸ“¥ Fetching and validating users...")

    response = requests.get("https://jsonplaceholder.typicode.com/users")
    raw_users = response.json()

    validated_users = []
    errors = []

    for user in raw_users:
        try:
            # Aplatir les donnÃ©es nested pour Pydantic
            flat_user = {
                "id": user["id"],
                "name": user["name"],
                "username": user["username"],
                "email": user["email"],
                "phone": user["phone"],
                "address__city": user["address"]["city"],
                "address__geo__lat": float(user["address"]["geo"]["lat"]),
                "address__geo__lng": float(user["address"]["geo"]["lng"]),
                "company__name": user["company"]["name"]
            }

            # Valider avec Pydantic
            validated = UserModel(**flat_user)

            # Ajouter des mÃ©tadonnÃ©es
            user_dict = validated.dict()
            user_dict["validated_at"] = datetime.now()
            user_dict["validation_passed"] = True

            validated_users.append(user_dict)
            print(f"âœ… User {validated.user_id} validated")

        except Exception as e:
            print(f"âŒ Validation error for user {user.get('id')}: {e}")
            errors.append({
                "user_id": user.get("id"),
                "error": str(e),
                "timestamp": datetime.now()
            })

    print(f"\nğŸ“Š Validation summary:")
    print(f"   âœ… Valid: {len(validated_users)}")
    print(f"   âŒ Invalid: {len(errors)}")

    yield validated_users


@dlt.resource(
    name="validation_errors",
    write_disposition="append"
)
def get_validation_errors():
    """
    Sauvegarder les erreurs de validation
    """
    # Dans un vrai projet, vous stockeriez les erreurs ici
    return []


def run():
    """ExÃ©cuter le pipeline avec validation"""
    print("ğŸš€ Starting validated pipeline...\n")

    pipeline = dlt.pipeline(
        pipeline_name="validated",
        destination="duckdb",
        dataset_name="validated_data"
    )

    load_info = pipeline.run([
        get_validated_users(),
        get_validation_errors()
    ])

    print(f"\nâœ… Pipeline completed!")
    print(f"ğŸ“Š {load_info}")


if __name__ == "__main__":
    run()
```

**ExÃ©cuter :**

```bash
pip install pydantic[email]
python pipelines/validated_pipeline.py
```

**CritÃ¨res de validation :**
- âœ… Validation Pydantic implÃ©mentÃ©e
- âœ… Erreurs de validation capturÃ©es
- âœ… DonnÃ©es invalides rejetÃ©es
- âœ… MÃ©tadonnÃ©es de validation ajoutÃ©es

---

### TÃ¢che 4.2 : Schema contracts

**Instructions :**

CrÃ©er `pipelines/schema_contracts.py` :

```python
"""
Pipeline avec schema contracts stricts
"""
import dlt


@dlt.resource(
    name="strict_products",
    write_disposition="merge",
    primary_key="product_id",
    columns={
        "product_id": {"data_type": "bigint", "nullable": False},
        "name": {"data_type": "text", "nullable": False},
        "price": {"data_type": "double", "nullable": False},
        "category": {"data_type": "text", "nullable": True}
    },
    schema_contract={
        "tables": "evolve",      # Nouvelles tables OK
        "columns": "freeze",     # Nouvelles colonnes = ERREUR
        "data_type": "freeze"    # Changement de type = ERREUR
    }
)
def get_strict_products():
    """
    Products avec schema contract strict
    """
    products = [
        {
            "product_id": 1,
            "name": "Laptop",
            "price": 999.99,
            "category": "Electronics"
        },
        {
            "product_id": 2,
            "name": "Mouse",
            "price": 29.99,
            "category": "Accessories"
        }
    ]

    yield products


def run():
    """ExÃ©cuter avec schema contract"""
    pipeline = dlt.pipeline(
        pipeline_name="strict_schema",
        destination="duckdb",
        dataset_name="strict_data"
    )

    # PremiÃ¨re exÃ©cution : OK
    print("ğŸš€ First run (should succeed)...")
    load_info = pipeline.run(get_strict_products())
    print(f"âœ… {load_info}\n")

    # DeuxiÃ¨me exÃ©cution avec nouvelle colonne : ERREUR
    @dlt.resource(
        name="strict_products",
        write_disposition="merge",
        primary_key="product_id",
        schema_contract={
            "columns": "freeze"
        }
    )
    def get_products_with_new_column():
        """Nouvelle colonne = erreur avec freeze"""
        products = [
            {
                "product_id": 3,
                "name": "Keyboard",
                "price": 79.99,
                "category": "Accessories",
                "stock": 50  # âŒ Nouvelle colonne !
            }
        ]
        yield products

    print("ğŸš€ Second run with new column (should fail)...")
    try:
        load_info = pipeline.run(get_products_with_new_column())
        print(f"âœ… {load_info}")
    except Exception as e:
        print(f"âŒ Expected error: {e}")


if __name__ == "__main__":
    run()
```

**CritÃ¨res de validation :**
- âœ… Schema contract configurÃ©
- âœ… PremiÃ¨re exÃ©cution rÃ©ussit
- âœ… DeuxiÃ¨me exÃ©cution Ã©choue (comme attendu)
- âœ… Erreur explicite sur nouvelle colonne

---

## ğŸš€ Partie 5 : Pipeline production-ready (2h)

### TÃ¢che 5.1 : Pipeline complet avec monitoring

**ScÃ©nario final :** CrÃ©er un pipeline production-ready complet avec :
- Extraction depuis une API
- Validation des donnÃ©es
- Loading incrÃ©mental
- Gestion d'erreurs
- Logging dÃ©taillÃ©
- Alertes

**Instructions :**

CrÃ©er `pipelines/production_pipeline.py` :

```python
"""
Pipeline production-ready avec monitoring complet
"""
import dlt
import requests
from datetime import datetime, timedelta
import logging
from typing import Iterator, Dict, Any
from pydantic import BaseModel, EmailStr, validator
import json

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class OrderModel(BaseModel):
    """ModÃ¨le de validation pour une commande"""
    order_id: str
    customer_email: EmailStr
    amount: float
    order_date: datetime
    status: str

    @validator('amount')
    def amount_must_be_positive(cls, v):
        if v <= 0:
            raise ValueError('Amount must be positive')
        return v

    @validator('status')
    def status_must_be_valid(cls, v):
        valid_statuses = ['pending', 'completed', 'cancelled']
        if v not in valid_statuses:
            raise ValueError(f'Status must be one of {valid_statuses}')
        return v


class ProductionPipeline:
    """Classe pour encapsuler le pipeline de production"""

    def __init__(self, pipeline_name: str, destination: str, dataset: str):
        self.pipeline_name = pipeline_name
        self.destination = destination
        self.dataset = dataset
        self.stats = {
            "extracted": 0,
            "validated": 0,
            "loaded": 0,
            "errors": 0
        }

    @dlt.resource(
        name="orders",
        write_disposition="append",
        primary_key="order_id"
    )
    def extract_orders(
        self,
        last_date=dlt.sources.incremental("order_date")
    ) -> Iterator[Dict[str, Any]]:
        """
        Extract : RÃ©cupÃ©rer les commandes avec loading incrÃ©mental
        """
        logger.info("ğŸ“¥ Starting extraction of orders...")

        # DÃ©terminer la date de dÃ©part
        if last_date.start_value is None:
            start_date = datetime.now() - timedelta(days=7)
            logger.info(f"âš ï¸ First run, fetching from {start_date.date()}")
        else:
            start_date = last_date.start_value
            logger.info(f"ğŸ“… Incremental load from {start_date}")

        try:
            # Simuler extraction API (remplacer par vraie API)
            orders = self._fetch_from_api(start_date)
            self.stats["extracted"] = len(orders)
            logger.info(f"âœ… Extracted {len(orders)} orders")

            # Valider et transformer
            for order in orders:
                validated_order = self._validate_and_transform(order)
                if validated_order:
                    yield validated_order

        except Exception as e:
            logger.error(f"âŒ Extraction error: {e}")
            self.stats["errors"] += 1
            raise

    def _fetch_from_api(self, start_date: datetime) -> list:
        """Simuler fetch API (remplacer par vraie API)"""
        from faker import Faker
        import random

        fake = Faker()
        orders = []

        current_date = start_date
        while current_date <= datetime.now():
            for _ in range(random.randint(5, 15)):
                order = {
                    "order_id": fake.uuid4(),
                    "customer_email": fake.email(),
                    "amount": round(random.uniform(10, 1000), 2),
                    "order_date": current_date,
                    "status": random.choice(["pending", "completed", "cancelled"])
                }
                orders.append(order)

            current_date += timedelta(hours=1)

        return orders

    def _validate_and_transform(self, order: Dict) -> Dict[str, Any]:
        """Valider et transformer une commande"""
        try:
            # Valider avec Pydantic
            validated = OrderModel(**order)

            # Enrichir avec mÃ©tadonnÃ©es
            order_dict = validated.dict()
            order_dict["loaded_at"] = datetime.now()
            order_dict["pipeline_version"] = "1.0.0"

            self.stats["validated"] += 1
            return order_dict

        except Exception as e:
            logger.warning(f"âš ï¸ Validation failed for order: {e}")
            self.stats["errors"] += 1
            # Sauvegarder dans une table d'erreurs
            return None

    def run(self):
        """ExÃ©cuter le pipeline complet"""
        logger.info("=" * 60)
        logger.info(f"ğŸš€ STARTING PIPELINE: {self.pipeline_name}")
        logger.info("=" * 60)

        start_time = datetime.now()

        try:
            # CrÃ©er le pipeline DLT
            pipeline = dlt.pipeline(
                pipeline_name=self.pipeline_name,
                destination=self.destination,
                dataset_name=self.dataset
            )

            # ExÃ©cuter
            load_info = pipeline.run(self.extract_orders())

            # VÃ©rifier les erreurs
            if load_info.has_failed_jobs:
                logger.error("âŒ Pipeline had failed jobs!")
                self.stats["errors"] += len(load_info.load_packages[0].jobs.get("failed_jobs", []))
                raise Exception("Some jobs failed")

            self.stats["loaded"] = self.stats["validated"]

            # DurÃ©e
            duration = (datetime.now() - start_time).total_seconds()

            # Log du succÃ¨s
            logger.info("=" * 60)
            logger.info("âœ… PIPELINE COMPLETED SUCCESSFULLY")
            logger.info(f"ğŸ“Š Extracted: {self.stats['extracted']}")
            logger.info(f"âœ… Validated: {self.stats['validated']}")
            logger.info(f"ğŸ’¾ Loaded: {self.stats['loaded']}")
            logger.info(f"âŒ Errors: {self.stats['errors']}")
            logger.info(f"â±ï¸ Duration: {duration:.2f}s")
            logger.info("=" * 60)

            # Sauvegarder les mÃ©triques
            self._save_metrics(duration, load_info)

            return load_info

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()

            logger.error("=" * 60)
            logger.error("âŒ PIPELINE FAILED")
            logger.error(f"Error: {e}")
            logger.error(f"â±ï¸ Duration: {duration:.2f}s")
            logger.error("=" * 60)

            # Envoyer alerte
            self._send_alert(f"Pipeline failed: {e}")
            raise

    def _save_metrics(self, duration: float, load_info):
        """Sauvegarder les mÃ©triques du pipeline"""
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "pipeline_name": self.pipeline_name,
            "duration_seconds": duration,
            "stats": self.stats,
            "load_info": str(load_info)
        }

        with open("pipeline_metrics.json", "a") as f:
            f.write(json.dumps(metrics) + "\n")

        logger.info("ğŸ“Š Metrics saved to pipeline_metrics.json")

    def _send_alert(self, message: str):
        """Envoyer une alerte (Slack, email, etc.)"""
        logger.warning(f"ğŸš¨ ALERT: {message}")
        # Ici vous ajouteriez l'intÃ©gration Slack/email
        # requests.post(slack_webhook, json={"text": message})


def main():
    """Point d'entrÃ©e principal"""
    pipeline = ProductionPipeline(
        pipeline_name="production_orders",
        destination="duckdb",
        dataset="production_data"
    )

    pipeline.run()


if __name__ == "__main__":
    main()
```

**ExÃ©cuter :**

```bash
python pipelines/production_pipeline.py
```

**VÃ©rifier les logs :**

```bash
cat pipeline.log
cat pipeline_metrics.json
```

**CritÃ¨res de validation :**
- âœ… Pipeline production-ready complet
- âœ… Logging dÃ©taillÃ© dans fichier
- âœ… Validation avec Pydantic
- âœ… Loading incrÃ©mental
- âœ… Gestion d'erreurs robuste
- âœ… MÃ©triques sauvegardÃ©es
- âœ… Alertes en cas d'Ã©chec

---

### TÃ¢che 5.2 : Scheduler avec APScheduler

**Instructions :**

CrÃ©er `pipelines/scheduler.py` :

```python
"""
Scheduler pour exÃ©cuter le pipeline automatiquement
"""
from apscheduler.schedulers.blocking import BlockingScheduler
from datetime import datetime
import logging
from production_pipeline import ProductionPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def run_pipeline_job():
    """Job exÃ©cutÃ© par le scheduler"""
    logger.info("â° Scheduled job triggered")

    try:
        pipeline = ProductionPipeline(
            pipeline_name="scheduled_orders",
            destination="duckdb",
            dataset="scheduled_data"
        )
        pipeline.run()

    except Exception as e:
        logger.error(f"âŒ Scheduled job failed: {e}")


def main():
    """DÃ©marrer le scheduler"""
    scheduler = BlockingScheduler()

    # ExÃ©cuter toutes les heures
    scheduler.add_job(
        run_pipeline_job,
        'interval',
        hours=1,
        next_run_time=datetime.now()  # PremiÃ¨re exÃ©cution immÃ©diate
    )

    logger.info("â° Scheduler started. Pipeline will run every hour.")
    logger.info("Press Ctrl+C to stop")

    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("Scheduler stopped")


if __name__ == "__main__":
    # Installer APScheduler
    # pip install apscheduler
    main()
```

**CritÃ¨res de validation :**
- âœ… Scheduler implÃ©mentÃ©
- âœ… Pipeline s'exÃ©cute automatiquement
- âœ… Gestion du Ctrl+C

---

## ğŸ“¤ Livrables

Ã€ la fin du brief, vous devez avoir :

### 1. Pipelines DLT (10+ fichiers) :
- âœ… `first_pipeline.py` - Premier pipeline simple
- âœ… `ecommerce_source.py` - Source complÃ¨te multi-resources
- âœ… `ecommerce_to_postgres.py` - Pipeline vers PostgreSQL
- âœ… `sales_generator.py` - GÃ©nÃ©rateur de donnÃ©es
- âœ… `incremental_orders.py` - Loading incrÃ©mental
- âœ… `multi_cursor_incremental.py` - Multi-cursors
- âœ… `validated_pipeline.py` - Validation Pydantic
- âœ… `schema_contracts.py` - Schema contracts
- âœ… `production_pipeline.py` - Pipeline production-ready
- âœ… `scheduler.py` - Scheduler automatique

### 2. Bases de donnÃ©es :
- âœ… DuckDB avec plusieurs datasets
- âœ… PostgreSQL avec donnÃ©es chargÃ©es

### 3. Fichiers de configuration :
- âœ… `.dlt/config.toml`
- âœ… `.dlt/secrets.toml`
- âœ… `requirements.txt`

### 4. Logs et mÃ©triques :
- âœ… `pipeline.log`
- âœ… `pipeline_metrics.json`

### 5. Documentation :
- âœ… README.md expliquant le projet
- âœ… Commentaires dans le code

---

## âœ… CritÃ¨res d'Ã‰valuation

| CritÃ¨re | Points |
|---------|--------|
| Installation et premier pipeline | 10 |
| Sources et destinations multiples | 15 |
| Loading incrÃ©mental implÃ©mentÃ© | 20 |
| Validation des donnÃ©es | 15 |
| Pipeline production-ready complet | 30 |
| QualitÃ© du code et logs | 10 |

**Total : 100 points**

**Bonus (+10 points) :**
- DÃ©ploiement avec Airflow
- Tests unitaires complets
- Dashboard de mÃ©triques
- Documentation exceptionnelle

---

## ğŸ’¡ Conseils

### Performance
- ğŸ”„ **Loading incrÃ©mental** : Ã‰conomise API calls et temps
- ğŸ“¦ **Batch processing** : Utilisez yield pour traiter par lots
- ğŸ—œï¸ **Compression** : Activez la compression pour les grandes tables

### Production
- ğŸ” **Secrets** : Jamais en dur, toujours dans .dlt/secrets.toml
- ğŸ“ **Logging** : Loggez tout pour le debugging
- ğŸš¨ **Alertes** : Configurez des alertes sur Ã©checs
- ğŸ“Š **Monitoring** : Trackez les mÃ©triques (durÃ©e, lignes, erreurs)

### Validation
- âœ… **Pydantic** : Validation robuste et explicite
- ğŸ”’ **Schema contracts** : Utilisez-les pour des pipelines critiques
- ğŸ§ª **Tests** : Testez avec DuckDB avant PostgreSQL/BigQuery

---

## ğŸ†˜ Troubleshooting

### Erreur : "Module dlt not found"
```bash
pip install dlt[duckdb]
```

### Erreur : "Database locked"
```bash
# Fermer toutes les connexions DuckDB
# Supprimer le fichier .wal
rm *.duckdb.wal
```

### Erreur : "Cannot connect to PostgreSQL"
```bash
# VÃ©rifier que le conteneur tourne
docker ps | grep postgres

# VÃ©rifier les credentials dans .dlt/secrets.toml
```

### Erreur de validation Pydantic
```python
# Activer les logs dÃ©taillÃ©s
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## ğŸ“š Ressources

### Documentation
- [DLT Documentation](https://dlthub.com/docs)
- [DLT API Reference](https://dlthub.com/docs/reference)
- [Pydantic Documentation](https://docs.pydantic.dev/)

### Exemples
- [DLT Examples Repository](https://github.com/dlt-hub/verified-sources)
- [DLT Blog](https://dlthub.com/docs/blog)

### Community
- [DLT Discord](https://discord.gg/dlthub)
- [GitHub Issues](https://github.com/dlt-hub/dlt/issues)

---

**Bon courage ! ğŸš€**

*DltHub : Le framework Python moderne pour vos pipelines de donnÃ©es !*
