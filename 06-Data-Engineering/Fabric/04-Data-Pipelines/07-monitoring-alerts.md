# Monitoring et Alerting

## Introduction

Le **monitoring** et **l'alerting** sont cruciaux pour maintenir des pipelines en production. Fabric offre plusieurs outils pour surveiller et diagnostiquer les exÃ©cutions.

```
Monitoring Stack:
â”œâ”€â”€ Pipeline Runs (UI)
â”œâ”€â”€ Activity Runs (dÃ©tails)
â”œâ”€â”€ Monitoring Hub (Fabric)
â”œâ”€â”€ Logs et Diagnostics
â”œâ”€â”€ Metrics et KPIs
â””â”€â”€ Alerting (Teams, Email, Webhook)
```

## Monitoring UI

### Pipeline Runs View

```
Fabric UI â†’ Workspace â†’ Pipeline â†’ Monitor

Tableau:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run ID     â”‚ Status â”‚ Start    â”‚ Duration â”‚ Trigger â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ abc-123    â”‚ âœ…     â”‚ 08:00    â”‚ 5m 23s   â”‚ Scheduleâ”‚
â”‚ abc-122    â”‚ âœ…     â”‚ 07:00    â”‚ 4m 56s   â”‚ Scheduleâ”‚
â”‚ abc-121    â”‚ âŒ     â”‚ 06:00    â”‚ 2m 10s   â”‚ Manual  â”‚
â”‚ abc-120    â”‚ â¸ï¸     â”‚ 05:00    â”‚ -        â”‚ Scheduleâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Statuses:
  âœ… Succeeded
  âŒ Failed
  â¸ï¸ InProgress
  â¹ï¸ Cancelled
  â­ï¸ Queued
```

**Filtres :**
```
â€¢ Date range
â€¢ Status (Succeeded/Failed/InProgress)
â€¢ Trigger type
â€¢ Pipeline name
â€¢ Run ID
```

### Activity Runs DÃ©tails

```
Click sur Run ID â†’ Activity details:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Activity        â”‚ Status â”‚ Duration â”‚ Details â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Copy_Data       â”‚ âœ…     â”‚ 45s      â”‚ [View]  â”‚
â”‚ Transform       â”‚ âœ…     â”‚ 3m 15s   â”‚ [View]  â”‚
â”‚ Load_Warehouse  â”‚ âœ…     â”‚ 1m 23s   â”‚ [View]  â”‚
â”‚ Send_Notificationâ”‚ âœ…    â”‚ 2s       â”‚ [View]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pour chaque activity:
  â€¢ Input (parameters, source)
  â€¢ Output (rÃ©sultats, metrics)
  â€¢ Error (si failed)
  â€¢ JSON dÃ©tails
```

**Copy Activity Output Example :**
```json
{
  "dataRead": 1073741824,
  "dataWritten": 1073741824,
  "rowsCopied": 1000000,
  "rowsSkipped": 15,
  "copyDuration": 45,
  "throughput": 23814,
  "errors": [],
  "effectiveIntegrationRuntime": "AutoResolveIntegrationRuntime",
  "usedDataIntegrationUnits": 16,
  "usedParallelCopies": 8,
  "executionDetails": [
    {
      "source": {
        "type": "SqlServer",
        "status": "Succeeded"
      },
      "sink": {
        "type": "Lakehouse",
        "status": "Succeeded"
      },
      "detailedDurations": {
        "queuingDuration": 2,
        "transferDuration": 43
      }
    }
  ]
}
```

## Monitoring Hub (Fabric)

### Vue d'Ensemble

```
Fabric â†’ Monitoring Hub

Sections:
â”œâ”€â”€ Pipeline runs
â”œâ”€â”€ Dataflow refreshes
â”œâ”€â”€ Notebook runs
â”œâ”€â”€ Spark job runs
â”œâ”€â”€ Semantic model refreshes
â””â”€â”€ Real-time intelligence
```

**Metrics Globales :**
```
Overview Dashboard:
  â”œâ”€â”€ Success rate (last 24h)
  â”œâ”€â”€ Failed runs (last 7 days)
  â”œâ”€â”€ Average duration
  â”œâ”€â”€ Total CU consumption
  â””â”€â”€ Top 10 longest running pipelines
```

### Filtering et Search

```
Filters:
  â€¢ Workspace
  â€¢ Item type (Pipeline, Dataflow, Notebook)
  â€¢ Status
  â€¢ Date range
  â€¢ User/Trigger

Search:
  â€¢ By Run ID
  â€¢ By Pipeline name
  â€¢ By Error message
```

## Logging

### Activity-Level Logging

**Copy Activity Logs :**
```json
{
  "typeProperties": {
    "enableSkipIncompatibleRow": true,
    "redirectIncompatibleRowSettings": {
      "linkedServiceName": "BlobStorage",
      "path": "logs/incompatible_rows"
    },
    "logSettings": {
      "enableCopyActivityLog": true,
      "copyActivityLogSettings": {
        "logLevel": "Warning",
        "enableReliableLogging": true
      },
      "logLocationSettings": {
        "linkedServiceName": "LogStorage",
        "path": "logs/copy_activity"
      }
    }
  }
}
```

**Log Levels :**
```
None: Pas de logs
Info: Toutes opÃ©rations
Warning: Seulement warnings et errors
```

**Log Contents :**
```
logs/copy_activity/
â”œâ”€â”€ {run_id}/
â”‚   â”œâ”€â”€ summary.json
â”‚   â”œâ”€â”€ incompatible_rows.csv
â”‚   â””â”€â”€ skipped_rows.csv
```

### Custom Logging avec Web Activity

```json
{
  "name": "Log_Pipeline_Start",
  "type": "WebActivity",
  "typeProperties": {
    "url": "https://api.company.com/logs",
    "method": "POST",
    "headers": {
      "Content-Type": "application/json"
    },
    "body": {
      "value": "@json(concat('{\"pipeline\":\"', pipeline().Pipeline, '\",\"runId\":\"', pipeline().RunId, '\",\"startTime\":\"', utcnow(), '\",\"triggeredBy\":\"', pipeline().TriggerName, '\"}'))",
      "type": "Expression"
    }
  }
}
```

### Database Logging Pattern

**Log Table :**
```sql
CREATE TABLE pipeline_execution_log (
    run_id VARCHAR(100),
    pipeline_name VARCHAR(200),
    status VARCHAR(50),
    start_time DATETIME,
    end_time DATETIME,
    duration_seconds INT,
    trigger_type VARCHAR(50),
    error_message VARCHAR(MAX),
    rows_processed BIGINT,
    created_at DATETIME DEFAULT GETDATE()
);
```

**Log Success :**
```json
{
  "name": "Log_Success",
  "type": "Script",
  "typeProperties": {
    "scripts": [{
      "type": "Query",
      "text": "INSERT INTO pipeline_execution_log (run_id, pipeline_name, status, start_time, end_time, duration_seconds, trigger_type, rows_processed) VALUES ('@{pipeline().RunId}', '@{pipeline().Pipeline}', 'Succeeded', '@{pipeline().TriggerTime}', '@{utcnow()}', @{div(sub(ticks(utcnow()), ticks(pipeline().TriggerTime)), 10000000)}, '@{pipeline().TriggerType}', @{activity('Copy_Data').output.rowsCopied})"
    }]
  },
  "linkedServiceName": {
    "referenceName": "MetadataDatabase"
  },
  "dependsOn": [
    { "activity": "Copy_Data", "dependencyConditions": ["Succeeded"] }
  ]
}
```

**Log Failure :**
```json
{
  "name": "Log_Failure",
  "type": "Script",
  "typeProperties": {
    "scripts": [{
      "type": "Query",
      "text": "INSERT INTO pipeline_execution_log (run_id, pipeline_name, status, start_time, end_time, error_message) VALUES ('@{pipeline().RunId}', '@{pipeline().Pipeline}', 'Failed', '@{pipeline().TriggerTime}', '@{utcnow()}', '@{activity('Copy_Data').error.message}')"
    }]
  },
  "dependsOn": [
    { "activity": "Copy_Data", "dependencyConditions": ["Failed"] }
  ]
}
```

## Metrics et KPIs

### Pipeline Performance Metrics

```sql
-- Average duration par pipeline
SELECT
    pipeline_name,
    AVG(duration_seconds) as avg_duration_sec,
    MIN(duration_seconds) as min_duration_sec,
    MAX(duration_seconds) as max_duration_sec,
    STDEV(duration_seconds) as std_dev
FROM pipeline_execution_log
WHERE created_at >= DATEADD(DAY, -30, GETDATE())
    AND status = 'Succeeded'
GROUP BY pipeline_name
ORDER BY avg_duration_sec DESC;
```

```
Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pipeline            â”‚ Avg      â”‚ Min      â”‚ Max      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ETL_Daily_Sales     â”‚ 325s     â”‚ 280s     â”‚ 450s     â”‚
â”‚ Transform_Customers â”‚ 180s     â”‚ 150s     â”‚ 220s     â”‚
â”‚ Load_Warehouse      â”‚ 95s      â”‚ 80s      â”‚ 130s     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Success Rate

```sql
SELECT
    pipeline_name,
    COUNT(*) as total_runs,
    SUM(CASE WHEN status = 'Succeeded' THEN 1 ELSE 0 END) as successful_runs,
    SUM(CASE WHEN status = 'Failed' THEN 1 ELSE 0 END) as failed_runs,
    CAST(SUM(CASE WHEN status = 'Succeeded' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS DECIMAL(5,2)) as success_rate_pct
FROM pipeline_execution_log
WHERE created_at >= DATEADD(DAY, -7, GETDATE())
GROUP BY pipeline_name
ORDER BY success_rate_pct ASC;
```

### Throughput Metrics

```sql
SELECT
    pipeline_name,
    AVG(rows_processed) as avg_rows,
    AVG(CAST(rows_processed AS FLOAT) / NULLIF(duration_seconds, 0)) as avg_rows_per_second
FROM pipeline_execution_log
WHERE status = 'Succeeded'
    AND created_at >= DATEADD(DAY, -7, GETDATE())
GROUP BY pipeline_name
ORDER BY avg_rows_per_second DESC;
```

### SLA Compliance

```sql
-- Pipelines qui dÃ©passent SLA (ex: 10 minutes)
SELECT
    run_id,
    pipeline_name,
    duration_seconds,
    duration_seconds / 60.0 as duration_minutes,
    start_time,
    end_time
FROM pipeline_execution_log
WHERE duration_seconds > 600  -- SLA: 10 minutes
    AND created_at >= DATEADD(DAY, -1, GETDATE())
ORDER BY duration_seconds DESC;
```

## Alerting

### Teams Notification

**Webhook Setup :**
```
1. Teams â†’ Channel â†’ Connectors â†’ Incoming Webhook
2. Configure webhook
3. Copy URL
```

**Pipeline Alert :**
```json
{
  "name": "Send_Teams_Alert",
  "type": "WebActivity",
  "typeProperties": {
    "url": "https://outlook.office.com/webhook/{your-webhook-id}",
    "method": "POST",
    "headers": {
      "Content-Type": "application/json"
    },
    "body": {
      "value": "@json(concat('{\"@type\":\"MessageCard\",\"@context\":\"https://schema.org/extensions\",\"summary\":\"Pipeline Alert\",\"themeColor\":\"0078D7\",\"title\":\"ğŸš¨ Pipeline Failed\",\"sections\":[{\"activityTitle\":\"', pipeline().Pipeline, '\",\"facts\":[{\"name\":\"Run ID:\",\"value\":\"', pipeline().RunId, '\"},{\"name\":\"Error:\",\"value\":\"', activity('Copy_Data').error.message, '\"},{\"name\":\"Time:\",\"value\":\"', utcnow(), '\"}]}]}'))",
      "type": "Expression"
    }
  },
  "dependsOn": [
    { "activity": "Copy_Data", "dependencyConditions": ["Failed"] }
  ]
}
```

**Adaptive Card (Advanced) :**
```json
{
  "body": {
    "type": "AdaptiveCard",
    "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
    "version": "1.4",
    "body": [
      {
        "type": "TextBlock",
        "text": "Pipeline Execution Report",
        "weight": "Bolder",
        "size": "Large"
      },
      {
        "type": "FactSet",
        "facts": [
          {
            "title": "Pipeline:",
            "value": "@{pipeline().Pipeline}"
          },
          {
            "title": "Status:",
            "value": "Succeeded"
          },
          {
            "title": "Duration:",
            "value": "@{div(sub(ticks(utcnow()), ticks(pipeline().TriggerTime)), 10000000)}s"
          },
          {
            "title": "Rows Processed:",
            "value": "@{activity('Copy_Data').output.rowsCopied}"
          }
        ]
      }
    ],
    "actions": [
      {
        "type": "Action.OpenUrl",
        "title": "View Run",
        "url": "https://fabric.microsoft.com/..."
      }
    ]
  }
}
```

### Email Notification (via Logic App)

**Logic App Trigger :**
```
HTTP Request Trigger
â””â”€ Body Schema:
   {
     "pipeline": "string",
     "runId": "string",
     "status": "string",
     "errorMessage": "string"
   }
```

**Pipeline Call :**
```json
{
  "name": "Trigger_Email_Alert",
  "type": "WebActivity",
  "typeProperties": {
    "url": "https://{logic-app-name}.azurewebsites.net/triggers/manual/paths/invoke?...",
    "method": "POST",
    "body": {
      "value": "@json(concat('{\"pipeline\":\"', pipeline().Pipeline, '\",\"runId\":\"', pipeline().RunId, '\",\"status\":\"Failed\",\"errorMessage\":\"', activity('Copy_Data').error.message, '\"}'))",
      "type": "Expression"
    }
  }
}
```

**Logic App Actions :**
```
1. Parse JSON (input body)
2. Compose HTML Email
3. Send Email (Office 365 Outlook connector)
   â”œâ”€ To: data-team@company.com
   â”œâ”€ Subject: [ALERT] Pipeline @{body('Parse_JSON')?['pipeline']} Failed
   â””â”€ Body: HTML template avec dÃ©tails
```

### PagerDuty / Opsgenie Integration

```json
{
  "name": "Create_PagerDuty_Incident",
  "type": "WebActivity",
  "typeProperties": {
    "url": "https://api.pagerduty.com/incidents",
    "method": "POST",
    "headers": {
      "Authorization": "Token token={your-api-key}",
      "Content-Type": "application/json",
      "From": "data-pipelines@company.com"
    },
    "body": {
      "incident": {
        "type": "incident",
        "title": "Pipeline @{pipeline().Pipeline} Failed",
        "service": {
          "id": "{service-id}",
          "type": "service_reference"
        },
        "urgency": "high",
        "body": {
          "type": "incident_body",
          "details": "Run ID: @{pipeline().RunId}. Error: @{activity('Copy_Data').error.message}"
        }
      }
    }
  }
}
```

## Monitoring Patterns

### Pattern 1 : Centralized Monitoring Pipeline

```
Master_Monitor_Pipeline:

1. Lookup_All_Pipelines
   â””â”€ SELECT DISTINCT pipeline_name FROM pipeline_execution_log

2. ForEach_Pipeline
   â”œâ”€ Get_Latest_Run_Status
   â”œâ”€ Check_SLA_Compliance
   â”œâ”€ Calculate_Success_Rate
   â””â”€ If_Issues_Detected:
      â””â”€ Send_Alert

3. Generate_Daily_Report
   â””â”€ Email with metrics dashboard
```

### Pattern 2 : Self-Healing Pipeline

```
ETL_With_Auto_Retry:

1. Execute_ETL (try 1)

2. On Failure:
   â”œâ”€ Log_Failure
   â”œâ”€ Wait (5 minutes)
   â”œâ”€ Check_Dependencies (files, database, capacity)
   â””â”€ If_Dependencies_OK:
      â”œâ”€ Execute_ETL (try 2)
      â””â”€ On 2nd Failure:
         â””â”€ Alert_Team

3. On Success:
   â””â”€ Clear_Previous_Alerts
```

### Pattern 3 : Proactive Monitoring

```
Health_Check_Pipeline (runs every 15 min):

1. Check_Source_Connectivity
   â””â”€ Test connection to all sources

2. Check_Capacity_Available
   â””â”€ Query Fabric capacity metrics

3. Check_File_Availability
   â””â”€ Validate expected files present

4. Check_Downstream_Dependencies
   â””â”€ Verify warehouse/lakehouse accessible

5. If_Any_Check_Failed:
   â””â”€ Preventive_Alert (before main ETL runs)
```

## Troubleshooting

### Common Errors et Solutions

**Error: Timeout**
```
Error: Activity timeout after 02:00:00

Cause:
  â€¢ Query trop lent
  â€¢ Trop de donnÃ©es
  â€¢ Network issues

Solutions:
  1. Increase timeout dans policy
  2. Optimize query (indexes, partitions)
  3. Partition Copy Activity (parallelize)
  4. Check source database performance
```

**Error: Out of Memory**
```
Error: Out of memory exception

Cause:
  â€¢ Dataset trop large pour compute
  â€¢ Complex transformations

Solutions:
  1. Increase Dataflow compute cores
  2. Enable staging pour Copy Activity
  3. Process en batches (ForEach avec partition)
  4. Optimize transformations (reduce shuffle)
```

**Error: Permission Denied**
```
Error: Access denied to resource

Cause:
  â€¢ Permissions insuffisantes
  â€¢ Expired credentials

Solutions:
  1. Verify linked service credentials
  2. Check RBAC roles (Lakehouse, Warehouse)
  3. Refresh service principal secret
  4. Check firewall rules
```

**Error: Incompatible Schema**
```
Error: Column 'amount' type mismatch (String vs Decimal)

Cause:
  â€¢ Source schema changed
  â€¢ Type conversion error

Solutions:
  1. Enable skipIncompatibleRows (temporarily)
  2. Add column mapping avec type conversion
  3. Fix source data
  4. Update sink schema
```

### Debugging Steps

```
1. Check Pipeline Run:
   â”œâ”€ Status et duration
   â”œâ”€ Trigger type et time
   â””â”€ Input parameters

2. Check Activity Runs:
   â”œâ”€ Which activity failed?
   â”œâ”€ Error message
   â””â”€ Input/Output JSON

3. Check Logs:
   â”œâ”€ Copy activity logs
   â”œâ”€ Incompatible rows
   â””â”€ Custom logs (database, blob)

4. Reproduce Locally:
   â”œâ”€ Debug mode dans UI
   â”œâ”€ Same parameters
   â””â”€ Isolate issue

5. Fix et Re-run:
   â”œâ”€ Apply fix
   â”œâ”€ Rerun failed pipeline (UI button)
   â””â”€ Monitor fix
```

## Best Practices

### âœ… Logging Strategy

```
1. Log Levels:
   â€¢ DEV: Info (verbose)
   â€¢ STAGING: Warning
   â€¢ PROD: Warning + Success summary

2. Log Retention:
   â€¢ Detailed logs: 30 days
   â€¢ Summary logs: 1 year
   â€¢ Compliance logs: 7 years

3. What to Log:
   âœ… Run ID, Pipeline name, Start/End time
   âœ… Rows processed, Duration
   âœ… Errors avec full stack trace
   âœ… Parameters values
   âŒ Pas de PII (emails, SSN, etc.)
   âŒ Pas de secrets (passwords, keys)
```

### âœ… Alerting Strategy

```
1. Alert Tiers:
   â€¢ Critical (P1): Production pipelines failed
     â†’ PagerDuty + Teams + Email
   â€¢ High (P2): SLA breach
     â†’ Teams + Email
   â€¢ Medium (P3): Performance degradation
     â†’ Teams only
   â€¢ Low (P4): Warnings
     â†’ Email digest daily

2. Alert Frequency:
   â€¢ Immediate: Critical errors
   â€¢ Batched: Warnings (hourly digest)
   â€¢ Daily: Performance reports

3. Alert Content:
   âœ… What failed
   âœ… When it failed
   âœ… Error message
   âœ… Link to run details
   âœ… Suggested next steps
```

### âœ… Monitoring Dashboards

```
Create dashboards showing:
  1. Real-time:
     â”œâ”€ Active runs
     â”œâ”€ Failed runs (last 24h)
     â””â”€ Current capacity usage

  2. Historical:
     â”œâ”€ Success rate trends
     â”œâ”€ Duration trends
     â”œâ”€ Throughput trends
     â””â”€ SLA compliance

  3. Operational:
     â”œâ”€ Top 10 slowest pipelines
     â”œâ”€ Top 10 error-prone pipelines
     â””â”€ Capacity consumption by pipeline
```

## Points ClÃ©s

- Monitoring UI pour pipeline et activity runs
- Monitoring Hub pour vue d'ensemble Fabric
- Custom logging (database, blob, API)
- Metrics: duration, success rate, throughput, SLA
- Alerting via Teams, Email, PagerDuty
- Error handling patterns (retry, self-healing)
- Proactive monitoring (health checks)
- Debugging: logs, reproduce, isolate, fix
- Alert tiers (Critical, High, Medium, Low)
- Dashboards pour visibility continue

---

**Prochain fichier :** [08 - CI/CD et Deployment](./08-cicd-deployment.md)

[â¬…ï¸ Fichier prÃ©cÃ©dent](./06-parameters-variables.md) | [â¬…ï¸ Retour au README du module](./README.md)
