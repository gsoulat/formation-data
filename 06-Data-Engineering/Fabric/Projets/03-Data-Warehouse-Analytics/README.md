# Projet 3 : Data Warehouse Analytics

## Vue d'ensemble

Dans ce projet, vous allez construire un **Data Warehouse complet** avec star schema pour une chaÃ®ne de retail "GlobalMart". Vous crÃ©erez des modÃ¨les sÃ©mantiques Direct Lake et des rapports Power BI pour fournir des insights business actionables.

**DurÃ©e estimÃ©e :** 10-12 heures
**Niveau :** IntermÃ©diaire Ã  AvancÃ©
**Modules prÃ©requis :** 03, 07

## Contexte Business

### L'entreprise : GlobalMart

GlobalMart est une chaÃ®ne de retail internationale avec :
- PrÃ©sence dans 5 rÃ©gions mondiales
- 100,000+ transactions mensuelles
- 10,000+ clients actifs
- 500+ produits dans le catalogue
- Ã‰quipes sales, marketing et finance nÃ©cessitant des analyses

### ProblÃ©matique

L'Ã©quipe de direction a besoin de :
- **VisibilitÃ© temps rÃ©el** sur les performances de ventes
- **Analyses comparatives** (YoY, MoM, par rÃ©gion)
- **Segmentation client** pour optimiser le marketing
- **Analyse produits** pour la gestion des stocks
- **ContrÃ´le d'accÃ¨s** : chaque rÃ©gion voit uniquement ses donnÃ©es

**Objectif :** CrÃ©er un Data Warehouse analytique avec reporting self-service.

## Objectifs d'Apprentissage

Ã€ la fin de ce projet, vous serez capable de :

- âœ… Concevoir un star schema optimisÃ© (dimensions, facts)
- âœ… CrÃ©er et charger un Fabric Warehouse avec T-SQL
- âœ… Configurer un semantic model Direct Lake
- âœ… Ã‰crire des mesures DAX avancÃ©es (time intelligence, CALCULATE)
- âœ… Construire des rapports Power BI interactifs
- âœ… ImplÃ©menter Row-Level Security
- âœ… Optimiser les performances (V-Order, DAX)
- âœ… Documenter un modÃ¨le de donnÃ©es

## ğŸ“¦ DonnÃ©es Fournies

**IMPORTANT : Les donnÃ©es pour ce projet sont disponibles dans `../../Ressources/datasets/`**

| Fichier | Description | Usage dans ce projet |
|---------|-------------|---------------------|
| **`retail_sales.csv`** (15 MB, 100K lignes) | Ventes avec dates, montants, rÃ©gions, catÃ©gories | â†’ Fact_Sales |
| **`customers.csv`** (1.6 MB, 10K clients) | Clients avec pays, ville, segments | â†’ Dim_Customer |
| **`products.csv`** (63 KB, 500 produits) | Catalogue avec catÃ©gories, prix, coÃ»ts | â†’ Dim_Product |

### Chargement des DonnÃ©es

1. **Uploadez les fichiers** dans votre Lakehouse `Files/raw/`
2. **CrÃ©ez les tables Bronze** :

```python
# Dans un notebook Fabric
df_sales = spark.read.csv("Files/raw/retail_sales.csv", header=True, inferSchema=True)
df_customers = spark.read.csv("Files/raw/customers.csv", header=True, inferSchema=True)
df_products = spark.read.csv("Files/raw/products.csv", header=True, inferSchema=True)

# Sauvegarder comme tables Delta (Bronze)
df_sales.write.mode("overwrite").saveAsTable("bronze_sales")
df_customers.write.mode("overwrite").saveAsTable("bronze_customers")
df_products.write.mode("overwrite").saveAsTable("bronze_products")
```

3. **GÃ©nÃ©rer Dim_Date** : Script T-SQL fourni dans la section "Star Schema"
4. **Transformer en Star Schema** : Scripts de crÃ©ation des dimensions et facts fournis

### Structure Star Schema

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Dim_Date   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dim_Customerâ”œâ”€â”€â”€â”€â”¤  Fact_Sales   â”œâ”€â”€â”€â”€â”¤ Dim_Product â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚Dim_Geographyâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture Cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SOURCE DATA (Lakehouse)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ retail_sales â”‚ customers    â”‚ products                      â”‚
â”‚ (100K rows)  â”‚ (10K rows)   â”‚ (500 rows)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    FABRIC WAREHOUSE      â”‚
        â”‚    (Star Schema)         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚ â”‚    Dim_Date         â”‚  â”‚
        â”‚ â”‚    Dim_Customer     â”‚  â”‚
        â”‚ â”‚    Dim_Product      â”‚  â”‚
        â”‚ â”‚    Dim_Geography    â”‚  â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚           â”‚              â”‚
        â”‚           â–¼              â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚ â”‚    Fact_Sales       â”‚  â”‚
        â”‚ â”‚    (100K+ rows)     â”‚  â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   SEMANTIC MODEL         â”‚
        â”‚   (Direct Lake)          â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ â€¢ Relationships          â”‚
        â”‚ â€¢ DAX Measures           â”‚
        â”‚ â€¢ Row-Level Security     â”‚
        â”‚ â€¢ Hierarchies            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   POWER BI REPORTS       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ â€¢ Executive Dashboard    â”‚
        â”‚ â€¢ Customer Analysis      â”‚
        â”‚ â€¢ Product Performance    â”‚
        â”‚ â€¢ Financial Insights     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## DonnÃ©es Source

Utilisez les datasets gÃ©nÃ©rÃ©s :
- `retail_sales.csv` (14.6 MB, 100K transactions)
- `customers.csv` (1.5 MB, 10K clients)
- `products.csv` (500 produits)

**Emplacement :** `/Ressources/datasets/`

## Phase 1: Chargement des DonnÃ©es Sources (1h)

### 1.1 Upload vers Lakehouse

1. CrÃ©er un Lakehouse "GlobalMart_Raw"
2. Upload les fichiers CSV dans Files/raw/
3. CrÃ©er des tables Delta :

```python
# Notebook: Load Raw Data

# Load retail sales
df_sales = spark.read.csv(
    "Files/raw/retail_sales.csv",
    header=True,
    inferSchema=True
)
df_sales.write.format("delta").saveAsTable("raw_sales")
print(f"Sales: {df_sales.count()} rows")

# Load customers
df_customers = spark.read.csv(
    "Files/raw/customers.csv",
    header=True,
    inferSchema=True
)
df_customers.write.format("delta").saveAsTable("raw_customers")
print(f"Customers: {df_customers.count()} rows")

# Load products
df_products = spark.read.csv(
    "Files/raw/products.csv",
    header=True,
    inferSchema=True
)
df_products.write.format("delta").saveAsTable("raw_products")
print(f"Products: {df_products.count()} rows")

# Validate
spark.sql("SHOW TABLES").show()
```

## Phase 2: CrÃ©ation du Warehouse (2h)

### 2.1 CrÃ©er le Fabric Warehouse

1. Fabric Workspace â†’ New â†’ Warehouse
2. Nom : "GlobalMart_DW"
3. Description : "Analytical Data Warehouse for GlobalMart"

### 2.2 CrÃ©ation de la Dimension Date

```sql
-- Date dimension generator (for 2020-2025)
CREATE TABLE Dim_Date (
    DateKey INT NOT NULL,
    Date DATE NOT NULL,
    Year INT NOT NULL,
    Quarter INT NOT NULL,
    QuarterName VARCHAR(10) NOT NULL,
    Month INT NOT NULL,
    MonthName VARCHAR(20) NOT NULL,
    MonthShort VARCHAR(3) NOT NULL,
    Week INT NOT NULL,
    DayOfMonth INT NOT NULL,
    DayOfWeek INT NOT NULL,
    DayName VARCHAR(20) NOT NULL,
    DayShort VARCHAR(3) NOT NULL,
    IsWeekend BIT NOT NULL,
    IsHoliday BIT NOT NULL,
    FiscalYear INT NOT NULL,
    FiscalQuarter INT NOT NULL,

    CONSTRAINT PK_DimDate PRIMARY KEY NONCLUSTERED (DateKey) NOT ENFORCED
);

-- Populate date dimension using CTE
WITH DateRange AS (
    SELECT CAST('2020-01-01' AS DATE) AS Date
    UNION ALL
    SELECT DATEADD(DAY, 1, Date)
    FROM DateRange
    WHERE Date < '2025-12-31'
)
INSERT INTO Dim_Date
SELECT
    CAST(FORMAT(Date, 'yyyyMMdd') AS INT) AS DateKey,
    Date,
    YEAR(Date) AS Year,
    DATEPART(QUARTER, Date) AS Quarter,
    'Q' + CAST(DATEPART(QUARTER, Date) AS VARCHAR) AS QuarterName,
    MONTH(Date) AS Month,
    DATENAME(MONTH, Date) AS MonthName,
    LEFT(DATENAME(MONTH, Date), 3) AS MonthShort,
    DATEPART(WEEK, Date) AS Week,
    DAY(Date) AS DayOfMonth,
    DATEPART(WEEKDAY, Date) AS DayOfWeek,
    DATENAME(WEEKDAY, Date) AS DayName,
    LEFT(DATENAME(WEEKDAY, Date), 3) AS DayShort,
    CASE WHEN DATEPART(WEEKDAY, Date) IN (1, 7) THEN 1 ELSE 0 END AS IsWeekend,
    0 AS IsHoliday,  -- Mark manually or via lookup
    CASE WHEN MONTH(Date) >= 7 THEN YEAR(Date) + 1 ELSE YEAR(Date) END AS FiscalYear,
    CASE
        WHEN MONTH(Date) IN (7,8,9) THEN 1
        WHEN MONTH(Date) IN (10,11,12) THEN 2
        WHEN MONTH(Date) IN (1,2,3) THEN 3
        ELSE 4
    END AS FiscalQuarter
FROM DateRange
OPTION (MAXRECURSION 2500);

-- Verify
SELECT COUNT(*) AS TotalDays FROM Dim_Date;
-- Expected: ~2192 days (6 years)
```

### 2.3 CrÃ©ation de la Dimension Customer

```sql
CREATE TABLE Dim_Customer (
    CustomerKey INT IDENTITY(1,1) NOT NULL,
    CustomerID VARCHAR(20) NOT NULL,
    FirstName VARCHAR(50) NOT NULL,
    LastName VARCHAR(50) NOT NULL,
    FullName VARCHAR(100) NOT NULL,
    Email VARCHAR(100) NOT NULL,
    Segment VARCHAR(50) NOT NULL,
    Industry VARCHAR(50) NOT NULL,
    CompanySize VARCHAR(20) NOT NULL,
    RegistrationDate DATE NOT NULL,
    TotalPurchases INT NOT NULL,
    TotalSpend DECIMAL(12,2) NOT NULL,
    LifetimeValue DECIMAL(12,2) NOT NULL,
    SatisfactionScore DECIMAL(3,1) NOT NULL,
    IsChurned BIT NOT NULL,

    CONSTRAINT PK_DimCustomer PRIMARY KEY NONCLUSTERED (CustomerKey) NOT ENFORCED
);

-- Load from Lakehouse via shortcut or INSERT SELECT
INSERT INTO Dim_Customer (
    CustomerID, FirstName, LastName, FullName, Email, Segment, Industry,
    CompanySize, RegistrationDate, TotalPurchases, TotalSpend, LifetimeValue,
    SatisfactionScore, IsChurned
)
SELECT
    customer_id,
    first_name,
    last_name,
    first_name + ' ' + last_name,
    email,
    segment,
    industry,
    company_size,
    CAST(registration_date AS DATE),
    total_purchases,
    total_spend,
    lifetime_value,
    satisfaction_score,
    CAST(is_churned AS BIT)
FROM lakehouse.dbo.raw_customers;

-- Create index for lookups
CREATE INDEX IX_DimCustomer_CustomerID ON Dim_Customer(CustomerID);
```

### 2.4 CrÃ©ation de la Dimension Product

```sql
CREATE TABLE Dim_Product (
    ProductKey INT IDENTITY(1,1) NOT NULL,
    ProductID VARCHAR(20) NOT NULL,
    ProductName VARCHAR(200) NOT NULL,
    Category VARCHAR(50) NOT NULL,
    SubCategory VARCHAR(50) NOT NULL,
    Brand VARCHAR(50) NOT NULL,
    UnitCost DECIMAL(10,2) NOT NULL,
    UnitPrice DECIMAL(10,2) NOT NULL,
    MarginPercent DECIMAL(5,1) NOT NULL,
    IsActive BIT NOT NULL,

    CONSTRAINT PK_DimProduct PRIMARY KEY NONCLUSTERED (ProductKey) NOT ENFORCED
);

INSERT INTO Dim_Product (
    ProductID, ProductName, Category, SubCategory, Brand,
    UnitCost, UnitPrice, MarginPercent, IsActive
)
SELECT
    product_id,
    product_name,
    category,
    subcategory,
    brand,
    unit_cost,
    unit_price,
    margin_percent,
    CAST(is_active AS BIT)
FROM lakehouse.dbo.raw_products;

CREATE INDEX IX_DimProduct_ProductID ON Dim_Product(ProductID);
```

### 2.5 CrÃ©ation de la Dimension Geography

```sql
CREATE TABLE Dim_Geography (
    GeographyKey INT IDENTITY(1,1) NOT NULL,
    Region VARCHAR(50) NOT NULL,
    Country VARCHAR(50) NOT NULL,
    Continent VARCHAR(50) NOT NULL,

    CONSTRAINT PK_DimGeography PRIMARY KEY NONCLUSTERED (GeographyKey) NOT ENFORCED
);

-- Extract unique geographies from sales
INSERT INTO Dim_Geography (Region, Country, Continent)
SELECT DISTINCT
    region,
    country,
    CASE
        WHEN region = 'North America' THEN 'Americas'
        WHEN region = 'Latin America' THEN 'Americas'
        WHEN region = 'Europe' THEN 'Europe'
        WHEN region = 'Asia Pacific' THEN 'Asia'
        WHEN region = 'Middle East' THEN 'Asia'
        ELSE 'Other'
    END AS Continent
FROM lakehouse.dbo.raw_sales;
```

### 2.6 CrÃ©ation de la Fact Table

```sql
CREATE TABLE Fact_Sales (
    SalesKey BIGINT IDENTITY(1,1) NOT NULL,
    DateKey INT NOT NULL,
    CustomerKey INT NOT NULL,
    ProductKey INT NOT NULL,
    GeographyKey INT NOT NULL,
    TransactionID VARCHAR(20) NOT NULL,
    Quantity INT NOT NULL,
    UnitPrice DECIMAL(10,2) NOT NULL,
    GrossAmount DECIMAL(12,2) NOT NULL,
    DiscountPercent DECIMAL(5,2) NOT NULL,
    DiscountAmount DECIMAL(12,2) NOT NULL,
    NetAmount DECIMAL(12,2) NOT NULL,
    Cost DECIMAL(12,2) NOT NULL,
    Profit DECIMAL(12,2) NOT NULL,
    Channel VARCHAR(50) NOT NULL,
    PaymentMethod VARCHAR(50) NOT NULL,

    CONSTRAINT PK_FactSales PRIMARY KEY NONCLUSTERED (SalesKey) NOT ENFORCED,
    CONSTRAINT FK_FactSales_DimDate FOREIGN KEY (DateKey)
        REFERENCES Dim_Date(DateKey) NOT ENFORCED,
    CONSTRAINT FK_FactSales_DimCustomer FOREIGN KEY (CustomerKey)
        REFERENCES Dim_Customer(CustomerKey) NOT ENFORCED,
    CONSTRAINT FK_FactSales_DimProduct FOREIGN KEY (ProductKey)
        REFERENCES Dim_Product(ProductKey) NOT ENFORCED,
    CONSTRAINT FK_FactSales_DimGeography FOREIGN KEY (GeographyKey)
        REFERENCES Dim_Geography(GeographyKey) NOT ENFORCED
);

-- Load fact data with dimension lookups
INSERT INTO Fact_Sales (
    DateKey, CustomerKey, ProductKey, GeographyKey, TransactionID,
    Quantity, UnitPrice, GrossAmount, DiscountPercent, DiscountAmount,
    NetAmount, Cost, Profit, Channel, PaymentMethod
)
SELECT
    CAST(FORMAT(CAST(s.date AS DATE), 'yyyyMMdd') AS INT) AS DateKey,
    c.CustomerKey,
    p.ProductKey,
    g.GeographyKey,
    s.transaction_id,
    s.quantity,
    s.unit_price,
    s.gross_amount,
    s.discount_percent,
    s.discount_amount,
    s.net_amount,
    s.cost,
    s.profit,
    s.channel,
    s.payment_method
FROM lakehouse.dbo.raw_sales s
INNER JOIN Dim_Customer c ON s.customer_id = c.CustomerID
INNER JOIN Dim_Product p ON s.product_id = p.ProductID
INNER JOIN Dim_Geography g ON s.region = g.Region AND s.country = g.Country;

-- Verify row count
SELECT COUNT(*) AS TotalSales FROM Fact_Sales;
-- Expected: ~100,000 rows
```

## Phase 3: Semantic Model Direct Lake (2h)

### 3.1 CrÃ©er le Semantic Model

1. Fabric Workspace â†’ New â†’ Semantic Model
2. Type : Direct Lake
3. Source : GlobalMart_DW (Warehouse)
4. SÃ©lectionner toutes les tables (Dim_*, Fact_Sales)

### 3.2 DÃ©finir les Relations

```
Relationships (Star Schema):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dim_Date      â”‚     â”‚  Dim_Customer â”‚
â”‚   (DateKey)     â”‚     â”‚  (CustomerKey)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1:*                  â”‚ 1:*
         â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Fact_Sales                  â”‚
â”‚  (DateKey, CustomerKey, ProductKey,     â”‚
â”‚   GeographyKey)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                      â–²
         â”‚ *:1                  â”‚ *:1
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dim_Product    â”‚     â”‚ Dim_Geography â”‚
â”‚  (ProductKey)   â”‚     â”‚ (GeographyKey)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Configuration:
- All relationships: Single direction
- Cross-filter: Single
- Cardinality: Many-to-One (Fact to Dim)
- Active: Yes
```

### 3.3 CrÃ©er les Mesures DAX

```dax
// ===========================================
// BASE MEASURES
// ===========================================

Total Sales =
SUM(Fact_Sales[NetAmount])

Total Quantity =
SUM(Fact_Sales[Quantity])

Total Cost =
SUM(Fact_Sales[Cost])

Total Profit =
SUM(Fact_Sales[Profit])

Transaction Count =
COUNTROWS(Fact_Sales)

Average Order Value =
DIVIDE([Total Sales], [Transaction Count], 0)

Profit Margin % =
DIVIDE([Total Profit], [Total Sales], 0)

// ===========================================
// TIME INTELLIGENCE
// ===========================================

Sales MTD =
TOTALMTD([Total Sales], Dim_Date[Date])

Sales QTD =
TOTALQTD([Total Sales], Dim_Date[Date])

Sales YTD =
TOTALYTD([Total Sales], Dim_Date[Date])

Sales Previous Month =
CALCULATE([Total Sales], PREVIOUSMONTH(Dim_Date[Date]))

Sales Previous Year =
CALCULATE([Total Sales], SAMEPERIODLASTYEAR(Dim_Date[Date]))

Sales YoY Growth % =
VAR CurrentPeriod = [Total Sales]
VAR PriorPeriod = [Sales Previous Year]
RETURN
    DIVIDE(CurrentPeriod - PriorPeriod, PriorPeriod, BLANK())

Sales MoM Growth % =
VAR CurrentMonth = [Total Sales]
VAR PriorMonth = [Sales Previous Month]
RETURN
    DIVIDE(CurrentMonth - PriorMonth, PriorMonth, BLANK())

// ===========================================
// CUSTOMER ANALYTICS
// ===========================================

Active Customers =
DISTINCTCOUNT(Fact_Sales[CustomerKey])

Average Customer Spend =
DIVIDE([Total Sales], [Active Customers], 0)

New Customers =
VAR CurrentPeriodCustomers = VALUES(Fact_Sales[CustomerKey])
VAR PriorPeriodCustomers =
    CALCULATETABLE(
        VALUES(Fact_Sales[CustomerKey]),
        PREVIOUSMONTH(Dim_Date[Date])
    )
RETURN
    COUNTROWS(EXCEPT(CurrentPeriodCustomers, PriorPeriodCustomers))

Customer Retention Rate =
VAR TotalCustomers = [Active Customers]
VAR NewCust = [New Customers]
VAR RetainedCustomers = TotalCustomers - NewCust
RETURN
    DIVIDE(RetainedCustomers, TotalCustomers, 0)

// ===========================================
// PRODUCT ANALYTICS
// ===========================================

Products Sold =
DISTINCTCOUNT(Fact_Sales[ProductKey])

Top Product Sales =
VAR ProductRank =
    RANKX(ALL(Dim_Product), [Total Sales], , DESC)
RETURN
    IF(ProductRank <= 10, [Total Sales], BLANK())

Category Contribution % =
DIVIDE(
    [Total Sales],
    CALCULATE([Total Sales], ALL(Dim_Product[Category])),
    0
)

// ===========================================
// GEOGRAPHIC ANALYTICS
// ===========================================

Regional Sales Distribution % =
DIVIDE(
    [Total Sales],
    CALCULATE([Total Sales], ALL(Dim_Geography)),
    0
)

// ===========================================
// KPI STATUS
// ===========================================

Sales Target =
2000000  // $2M monthly target (customize)

Sales vs Target % =
DIVIDE([Total Sales], [Sales Target], 0)

Target Status =
VAR Achievement = [Sales vs Target %]
RETURN
    SWITCH(
        TRUE(),
        Achievement >= 1.1, "Exceeding",
        Achievement >= 1.0, "On Target",
        Achievement >= 0.9, "Slightly Below",
        "Below Target"
    )
```

### 3.4 Configurer Row-Level Security

1. Dans le semantic model â†’ Manage Roles
2. CrÃ©er un rÃ´le "Regional_Access"

```dax
// Role: Regional_Access
// Table: Dim_Geography
[Region] IN {
    LOOKUPVALUE(
        UserRegionMapping[Region],
        UserRegionMapping[UserEmail],
        USERPRINCIPALNAME()
    )
}

// Alternative avec table de mapping
VAR CurrentUser = USERPRINCIPALNAME()
RETURN
    [Region] IN
    CALCULATETABLE(
        VALUES(SecurityMapping[AllowedRegion]),
        SecurityMapping[UserEmail] = CurrentUser
    )
```

3. Assigner les utilisateurs aux rÃ´les

## Phase 4: Power BI Reports (3h)

### 4.1 Page 1: Executive Dashboard

**Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Total   â”‚ YoY     â”‚ Profit  â”‚ Trans.  â”‚
â”‚ Sales   â”‚ Growth  â”‚ Margin  â”‚ Count   â”‚
â”‚ (Card)  â”‚ (Card)  â”‚ (Card)  â”‚ (Card)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚    Sales Trend (Line Chart)            â”‚
â”‚    X: Date (Month)                     â”‚
â”‚    Y: Total Sales, Sales Previous Year â”‚
â”‚                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    â”‚                   â”‚
â”‚ Sales by Category  â”‚ Sales by Region   â”‚
â”‚ (Bar Chart)        â”‚ (Donut Chart)     â”‚
â”‚                    â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Visual Configurations:**

Card - Total Sales:
- Value: [Total Sales]
- Format: Currency, 0 decimals
- Conditional formatting: Color scale

Line Chart - Sales Trend:
```
X-Axis: Dim_Date[MonthName]
Y-Axis: [Total Sales], [Sales Previous Year]
Legend: "Current Year", "Prior Year"
Title: "Monthly Sales Trend"
```

### 4.2 Page 2: Customer Analysis

**Visuals:**

1. **Customer Segmentation (Pie Chart)**
   - Values: [Active Customers]
   - Legend: Dim_Customer[Segment]

2. **Top 10 Customers (Table)**
```
Columns:
- Dim_Customer[FullName]
- [Total Sales]
- [Transaction Count]
- [Average Order Value]

Sort: [Total Sales] DESC
Top N filter: 10
```

3. **Customer Lifetime Value Distribution (Histogram)**
   - X-Axis: Dim_Customer[LifetimeValue] (bins)
   - Y-Axis: Count of customers

4. **Geographic Sales (Map)**
   - Location: Dim_Geography[Country]
   - Size: [Total Sales]
   - Color: [Profit Margin %]

5. **New vs Returning Customers (Clustered Column)**
   - X-Axis: Dim_Date[Month]
   - Y-Axis: [New Customers], [Active Customers] - [New Customers]

### 4.3 Page 3: Product Performance

**Visuals:**

1. **Category Performance (Treemap)**
   - Group: Dim_Product[Category]
   - Values: [Total Sales]
   - Color: [Profit Margin %]

2. **Brand Analysis (Matrix)**
```
Rows: Dim_Product[Brand]
Columns: Dim_Product[Category]
Values: [Total Sales], [Profit Margin %]

Conditional formatting:
- [Total Sales]: Data bars
- [Profit Margin %]: Color scale (Red < 30%, Yellow 30-50%, Green > 50%)
```

3. **Product Ranking (Table)**
   - Dim_Product[ProductName]
   - [Total Sales]
   - [Total Quantity]
   - [Category Contribution %]
   - Sparkline: Sales over time

4. **Sales by Channel (Stacked Bar)**
   - X-Axis: Dim_Product[Category]
   - Y-Axis: [Total Sales]
   - Legend: Fact_Sales[Channel]

### 4.4 Page 4: Financial Insights

**Visuals:**

1. **Profit Waterfall**
   - Category: "Revenue", "Cost", "Gross Profit", "Operating Expenses", "Net Profit"
   - Show: Breakdown of profit components

2. **Discount Analysis (Scatter)**
   - X-Axis: [DiscountPercent] (average)
   - Y-Axis: [Profit Margin %]
   - Size: [Total Sales]
   - Details: Dim_Product[Category]

3. **Payment Method Distribution (Donut)**
   - Values: [Transaction Count]
   - Legend: Fact_Sales[PaymentMethod]

### 4.5 Slicers et Filters

Add to all pages:
- Date Range (Slicer - Slider)
- Region (Slicer - Dropdown)
- Category (Slicer - List)
- Channel (Slicer - Buttons)

Enable:
- Sync slicers across pages
- Clear all filters button

## Phase 5: Optimisation et Tests (1h)

### 5.1 Performance Optimization

**V-Order (Warehouse):**
```sql
-- Tables are automatically V-Order optimized in Fabric
-- Verify optimization status
SELECT
    table_name,
    column_name,
    is_v_ordered
FROM sys.dm_column_store_row_groups
WHERE table_name LIKE 'Fact%' OR table_name LIKE 'Dim%';
```

**DAX Optimization:**
```dax
// BEFORE (Slow)
Total Sales Slow =
SUMX(Fact_Sales, Fact_Sales[Quantity] * Fact_Sales[UnitPrice])

// AFTER (Fast - pre-calculated column)
Total Sales Fast =
SUM(Fact_Sales[NetAmount])

// Use Variables
Profit Margin Optimized =
VAR TotalProfit = [Total Profit]
VAR TotalSales = [Total Sales]
RETURN
    DIVIDE(TotalProfit, TotalSales, 0)
```

### 5.2 Testing

**Performance Tests:**
```
Use Power BI Performance Analyzer:
1. View â†’ Performance Analyzer
2. Start Recording
3. Refresh all visuals
4. Analyze results

Targets:
- DAX query < 1 second
- Visual rendering < 500ms
- Total page load < 3 seconds
```

**Data Validation:**
```sql
-- Validate referential integrity
SELECT 'Orphan Sales (No Customer)' AS Check,
       COUNT(*) AS Count
FROM Fact_Sales f
LEFT JOIN Dim_Customer c ON f.CustomerKey = c.CustomerKey
WHERE c.CustomerKey IS NULL

UNION ALL

SELECT 'Total Sales Amount Match',
       CASE WHEN ABS(SUM(f.NetAmount) - (SELECT SUM(net_amount) FROM lakehouse.dbo.raw_sales)) < 1
            THEN 0 ELSE 1 END
FROM Fact_Sales f;
```

## Livrables

### Checklist Technique
- [ ] Lakehouse avec donnÃ©es source (3 tables)
- [ ] Warehouse avec star schema (4 dimensions, 1 fact)
- [ ] DonnÃ©es chargÃ©es et validÃ©es (100K+ transactions)
- [ ] Semantic model Direct Lake configurÃ©
- [ ] 15+ mesures DAX (base, time intelligence, analytics)
- [ ] Row-Level Security par rÃ©gion
- [ ] 4 pages de rapport Power BI
- [ ] Slicers synchronisÃ©s
- [ ] Performance < 3s par page
- [ ] Documentation complÃ¨te

### Documentation Requise
- [ ] Diagramme star schema
- [ ] Dictionnaire de donnÃ©es
- [ ] Catalogue des mesures DAX
- [ ] Guide utilisateur des rapports
- [ ] Configuration RLS
- [ ] Rapport de performance

### MÃ©triques de SuccÃ¨s
```
Data Quality:
  âœ“ 100% referential integrity
  âœ“ 0 orphan records
  âœ“ Data matches source

Performance:
  âœ“ Page load < 3 seconds
  âœ“ DAX queries < 1 second
  âœ“ Direct Lake mode active

Security:
  âœ“ RLS filters data correctly
  âœ“ Users see only allowed regions
  âœ“ No data leakage

Business Value:
  âœ“ Executives can see KPIs
  âœ“ Self-service analytics enabled
  âœ“ Insights are actionable
```

## CritÃ¨res d'Ã‰valuation

- Star schema design quality (25%)
- DAX measures correctness et performance (30%)
- Report UX et business insights (25%)
- Performance optimization (10%)
- Documentation (10%)

## Extensions Possibles

1. **SCD Type 2** : Historisation des dimensions
2. **Incremental Refresh** : Mise Ã  jour incrÃ©mentale
3. **Composite Model** : MÃ©lange Direct Lake + Import
4. **AI Insights** : Q&A, Key Influencers, Smart Narratives
5. **Mobile Layout** : Rapports optimisÃ©s mobile
6. **Alertes** : Data-driven alerts sur KPIs

---

**DurÃ©e estimÃ©e: 10-12 heures**

**DonnÃ©es requises :** `retail_sales.csv`, `customers.csv`, `products.csv` dans `/Ressources/datasets/`

[â¬…ï¸ Retour aux projets](../README.md)
