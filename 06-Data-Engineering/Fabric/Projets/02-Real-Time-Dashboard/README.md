# Projet 2 : Real-Time IoT Dashboard

## Vue d'ensemble

Dans ce projet, vous allez construire un **systÃ¨me de monitoring temps rÃ©el** pour une usine de fabrication fictive "SmartFactory". Vous implÃ©menterez une solution complÃ¨te d'analytics temps rÃ©el avec alertes automatiques et dashboards interactifs.

**DurÃ©e estimÃ©e :** 8-10 heures
**Niveau :** IntermÃ©diaire
**Modules prÃ©requis :** 08 (Real-Time Analytics)

## Contexte Business

### L'entreprise : SmartFactory

SmartFactory est une usine de fabrication de composants Ã©lectroniques. Ils disposent de :
- 50 capteurs de tempÃ©rature sur les machines
- 30 capteurs d'humiditÃ© dans les zones de stockage
- 20 capteurs de pression dans les systÃ¨mes pneumatiques
- 10 capteurs de vibration sur les Ã©quipements critiques

### ProblÃ©matique

L'usine fait face Ã  plusieurs dÃ©fis :
- **Pannes imprÃ©vues** : coÃ»tant 50Kâ‚¬/heure d'arrÃªt
- **QualitÃ© variable** : dÃ©fauts liÃ©s aux conditions environnementales
- **Maintenance rÃ©active** : pas de visibilitÃ© sur l'Ã©tat des Ã©quipements
- **Temps de rÃ©ponse lent** : alertes manuelles avec dÃ©lai

**Objectif :** CrÃ©er un systÃ¨me de monitoring temps rÃ©el avec alertes automatiques pour prÃ©dire et prÃ©venir les pannes.

## Objectifs d'Apprentissage

Ã€ la fin de ce projet, vous serez capable de :

- âœ… Configurer un EventStream pour l'ingestion de donnÃ©es en streaming
- âœ… CrÃ©er et gÃ©rer une KQL Database pour l'analytics temps rÃ©el
- âœ… Ã‰crire des requÃªtes KQL avancÃ©es (agrÃ©gations, time series, anomalies)
- âœ… Construire des dashboards temps rÃ©el interactifs
- âœ… ImplÃ©menter des alertes automatiques avec Data Activator
- âœ… Analyser des patterns et dÃ©tecter des anomalies
- âœ… Optimiser les performances d'ingestion et de requÃªtes

## ğŸ“¦ DonnÃ©es Fournies

**IMPORTANT : Les donnÃ©es pour ce projet sont disponibles dans `../../Ressources/datasets/`**

| Fichier | Description | Usage dans ce projet |
|---------|-------------|---------------------|
| **`iot_telemetry.json`** (2.0 MB, 10K events) | TÃ©lÃ©mÃ©trie IoT avec temperature, pressure, humidity | â†’ Simulateur de capteurs IoT |
| **`web_logs.json`** (17 MB, 50K logs) | Logs web avec timestamps, status codes, response times | â†’ Analytics temps rÃ©el optionnel |

### Option 1 : Utiliser les DonnÃ©es JSON Fournies

```python
# Charger les donnÃ©es IoT dans un notebook Fabric
import json
from datetime import datetime, timedelta

# Lire le fichier JSON
with open("Files/raw/iot_telemetry.json", "r") as f:
    telemetry_data = json.load(f)

# Convertir en DataFrame Spark
df_telemetry = spark.createDataFrame(telemetry_data)
print(f"Events IoT: {df_telemetry.count()} lignes")
df_telemetry.show(5)
```

### Option 2 : Simulateur Temps RÃ©el (RecommandÃ©)

Le projet inclut un **simulateur Python complet** (voir section "Simulateur IoT" plus bas) qui gÃ©nÃ¨re des donnÃ©es en temps rÃ©el. Ce simulateur peut :
- Utiliser `iot_telemetry.json` comme base de donnÃ©es historiques
- GÃ©nÃ©rer de nouveaux Ã©vÃ©nements en continu
- Envoyer les donnÃ©es vers EventStream

### Chargement dans Fabric

1. **Uploadez** `iot_telemetry.json` dans `Files/raw/`
2. **Pour le streaming** : Utilisez le simulateur Python fourni dans ce README
3. **Configuration EventStream** : Le simulateur peut publier vers Azure Event Hub ou Custom Endpoint

## Architecture Cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SOURCES IoT                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Temperature  â”‚ Humidity     â”‚ Pressure     â”‚ Vibration         â”‚
â”‚ Sensors (50) â”‚ Sensors (30) â”‚ Sensors (20) â”‚ Sensors (10)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Azure IoT Hub  â”‚  (SimulÃ©)
                    â”‚   / Event Hub    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   EventStream   â”‚
                    â”‚   (Ingestion)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
              â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  KQL Database   â”‚           â”‚  Data Activator â”‚
    â”‚  (Analytics)    â”‚           â”‚  (Alerts)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                             â”‚
             â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Real-Time       â”‚           â”‚  Teams/Email    â”‚
    â”‚ Dashboard       â”‚           â”‚  Notifications  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Phase 1: GÃ©nÃ©ration de DonnÃ©es IoT (1h)

### 1.1 Simulateur de Capteurs

CrÃ©ez un script Python pour gÃ©nÃ©rer des donnÃ©es IoT rÃ©alistes :

```python
import json
import random
from datetime import datetime
import time

class IoTSimulator:
    """Simulateur de capteurs IoT pour SmartFactory"""

    def __init__(self):
        self.devices = self._initialize_devices()

    def _initialize_devices(self):
        """CrÃ©er les configurations des capteurs"""
        devices = []

        # Temperature sensors (machines)
        for i in range(1, 51):
            devices.append({
                "device_id": f"TEMP_{i:03d}",
                "type": "temperature",
                "location": f"Machine_{(i-1)//5 + 1}",
                "zone": f"Production_Zone_{(i-1)//10 + 1}",
                "normal_value": 45 + random.uniform(-5, 5),
                "noise": 2.0
            })

        # Humidity sensors (storage)
        for i in range(1, 31):
            devices.append({
                "device_id": f"HUM_{i:03d}",
                "type": "humidity",
                "location": f"Storage_Area_{(i-1)//6 + 1}",
                "zone": f"Storage_Zone_{(i-1)//10 + 1}",
                "normal_value": 55 + random.uniform(-10, 10),
                "noise": 5.0
            })

        # Pressure sensors
        for i in range(1, 21):
            devices.append({
                "device_id": f"PRESS_{i:03d}",
                "type": "pressure",
                "location": f"Pneumatic_System_{(i-1)//4 + 1}",
                "zone": f"Production_Zone_{(i-1)//5 + 1}",
                "normal_value": 100 + random.uniform(-10, 10),
                "noise": 3.0
            })

        # Vibration sensors
        for i in range(1, 11):
            devices.append({
                "device_id": f"VIB_{i:03d}",
                "type": "vibration",
                "location": f"Critical_Equipment_{i}",
                "zone": "Critical_Zone",
                "normal_value": 0.5 + random.uniform(-0.1, 0.1),
                "noise": 0.1
            })

        return devices

    def generate_reading(self, device):
        """GÃ©nÃ©rer une lecture pour un capteur"""
        # Valeur normale avec bruit
        value = device["normal_value"] + random.gauss(0, device["noise"])

        # Simuler occasionnellement des anomalies (5% chance)
        if random.random() < 0.05:
            if device["type"] == "temperature":
                value += random.uniform(15, 30)  # Surchauffe
            elif device["type"] == "vibration":
                value *= random.uniform(2, 4)    # Vibration excessive
            elif device["type"] == "pressure":
                value += random.uniform(-20, 20)  # Pression anormale

        return {
            "device_id": device["device_id"],
            "device_type": device["type"],
            "location": device["location"],
            "zone": device["zone"],
            "value": round(value, 2),
            "unit": self._get_unit(device["type"]),
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "quality": "good" if not self._is_anomaly(device, value) else "warning"
        }

    def _get_unit(self, device_type):
        units = {
            "temperature": "celsius",
            "humidity": "percent",
            "pressure": "bar",
            "vibration": "mm/s"
        }
        return units.get(device_type, "unknown")

    def _is_anomaly(self, device, value):
        threshold = 3 * device["noise"]
        return abs(value - device["normal_value"]) > threshold

    def generate_batch(self):
        """GÃ©nÃ©rer un batch de donnÃ©es pour tous les capteurs"""
        return [self.generate_reading(device) for device in self.devices]

    def stream_data(self, interval_seconds=1):
        """Streamer des donnÃ©es en continu"""
        while True:
            batch = self.generate_batch()
            for reading in batch:
                yield json.dumps(reading)
            time.sleep(interval_seconds)

# Usage
simulator = IoTSimulator()

# GÃ©nÃ©rer un fichier de test
with open("iot_sample_data.json", "w") as f:
    for _ in range(1000):
        batch = simulator.generate_batch()
        for reading in batch:
            f.write(json.dumps(reading) + "\n")

print(f"Generated {1000 * 111} readings")
```

### 1.2 Structure des DonnÃ©es

```json
{
  "device_id": "TEMP_001",
  "device_type": "temperature",
  "location": "Machine_1",
  "zone": "Production_Zone_1",
  "value": 47.32,
  "unit": "celsius",
  "timestamp": "2024-01-15T10:30:00.000Z",
  "quality": "good"
}
```

## Phase 2: Configuration EventStream (2h)

### 2.1 CrÃ©er EventStream

1. Dans Fabric Workspace â†’ New â†’ EventStream
2. Nommer : "SmartFactory_Telemetry_Stream"
3. Description : "Real-time ingestion of IoT sensor data"

### 2.2 Configurer la Source

**Option A : Sample Data (pour tests)**
```
Source Type: Sample Data
Data Format: JSON
Schema: Custom (IoT telemetry)
```

**Option B : Azure Event Hub (production)**
```
Source Type: Azure Event Hub
Connection String: <your-connection-string>
Consumer Group: $Default
Data Format: JSON
```

**Option C : Custom Endpoint**
```python
# Envoyer des donnÃ©es vers EventStream
import requests

eventstream_endpoint = "https://your-eventstream-endpoint"

simulator = IoTSimulator()
for reading in simulator.stream_data(interval_seconds=1):
    requests.post(eventstream_endpoint, data=reading, headers={"Content-Type": "application/json"})
```

### 2.3 Configurer Transformations (Optionnel)

Dans EventStream, ajoutez des transformations :

```sql
-- Filtrer les donnÃ©es de qualitÃ©
SELECT *
FROM InputStream
WHERE quality IN ('good', 'warning')

-- Enrichir avec calculs
SELECT
    device_id,
    device_type,
    location,
    zone,
    value,
    unit,
    timestamp,
    quality,
    CASE
        WHEN device_type = 'temperature' AND value > 70 THEN 'critical'
        WHEN device_type = 'vibration' AND value > 2.0 THEN 'critical'
        ELSE 'normal'
    END AS alert_level
FROM InputStream
```

### 2.4 Configurer la Destination

1. Ajouter destination : KQL Database
2. SÃ©lectionner la database (crÃ©Ã©e dans Phase 3)
3. Table : `Telemetry`
4. Mapping : Auto-detect ou manuel

## Phase 3: KQL Database Setup (2h)

### 3.1 CrÃ©er KQL Database

1. Fabric Workspace â†’ New â†’ KQL Database
2. Nom : "SmartFactory_Analytics"
3. Type : Eventhouse

### 3.2 CrÃ©er les Tables

```kql
// Table principale de tÃ©lÃ©mÃ©trie
.create table Telemetry (
    DeviceId: string,
    DeviceType: string,
    Location: string,
    Zone: string,
    Value: real,
    Unit: string,
    Timestamp: datetime,
    Quality: string
)

// Politique de rÃ©tention (garder 30 jours)
.alter-merge table Telemetry policy retention
```
{
    "SoftDeletePeriod": "30.00:00:00",
    "Recoverability": "Enabled"
}
```

// Table des seuils d'alerte
.create table AlertThresholds (
    DeviceType: string,
    MinValue: real,
    MaxValue: real,
    CriticalMin: real,
    CriticalMax: real
)

// InsÃ©rer les seuils
.ingest inline into table AlertThresholds
<| temperature,20,60,10,75
| humidity,30,70,20,80
| pressure,80,120,70,130
| vibration,0,1.5,0,3.0
```

### 3.3 Configurer l'Ingestion Mapping

```kql
// CrÃ©er le mapping JSON
.create table Telemetry ingestion json mapping 'IoTMapping'
'['
'  {"column": "DeviceId", "path": "$.device_id"},'
'  {"column": "DeviceType", "path": "$.device_type"},'
'  {"column": "Location", "path": "$.location"},'
'  {"column": "Zone", "path": "$.zone"},'
'  {"column": "Value", "path": "$.value"},'
'  {"column": "Unit", "path": "$.unit"},'
'  {"column": "Timestamp", "path": "$.timestamp"},'
'  {"column": "Quality", "path": "$.quality"}'
']'
```

### 3.4 VÃ©rifier l'Ingestion

```kql
// VÃ©rifier les donnÃ©es entrantes
Telemetry
| take 10

// Compter par type
Telemetry
| summarize Count = count() by DeviceType
| render piechart

// VÃ©rifier la latence
Telemetry
| extend IngestionLatency = now() - Timestamp
| summarize avg(IngestionLatency), max(IngestionLatency)
```

## Phase 4: Queries KQL AvancÃ©es (2h)

### 4.1 Monitoring en Temps RÃ©el

```kql
// ========================================
// DASHBOARD PRINCIPAL
// ========================================

// KPI 1: Nombre de devices actifs (derniÃ¨re minute)
Telemetry
| where Timestamp > ago(1m)
| summarize ActiveDevices = dcount(DeviceId)

// KPI 2: TempÃ©rature moyenne globale
Telemetry
| where DeviceType == "temperature" and Timestamp > ago(5m)
| summarize AvgTemperature = round(avg(Value), 1)

// KPI 3: Nombre d'alertes actives
Telemetry
| where Timestamp > ago(5m)
| join kind=inner AlertThresholds on $left.DeviceType == $right.DeviceType
| where Value < CriticalMin or Value > CriticalMax
| summarize AlertCount = dcount(DeviceId)
```

### 4.2 Analyse de Tendances

```kql
// TempÃ©rature moyenne par zone (5 min bins)
Telemetry
| where DeviceType == "temperature"
| where Timestamp > ago(1h)
| summarize AvgTemp = avg(Value) by bin(Timestamp, 5m), Zone
| render timechart

// Comparaison des zones
Telemetry
| where DeviceType == "temperature"
| where Timestamp > ago(1h)
| summarize
    AvgTemp = avg(Value),
    MaxTemp = max(Value),
    MinTemp = min(Value),
    StdDev = stdev(Value)
  by Zone
| order by AvgTemp desc
| render columnchart
```

### 4.3 DÃ©tection d'Anomalies

```kql
// DÃ©tection d'anomalies par sÃ©rie temporelle
Telemetry
| where DeviceType == "temperature"
| where Timestamp > ago(24h)
| make-series AvgTemp = avg(Value) default=0 on Timestamp step 10m by Zone
| extend anomalies = series_decompose_anomalies(AvgTemp, 1.5)
| mv-expand Timestamp, AvgTemp, anomalies
| where toint(anomalies) != 0
| project Timestamp = todatetime(Timestamp), Zone, AvgTemp = todouble(AvgTemp), AnomalyScore = toint(anomalies)

// Devices avec comportement anormal
Telemetry
| where Timestamp > ago(1h)
| join kind=inner AlertThresholds on $left.DeviceType == $right.DeviceType
| extend
    IsOutOfRange = Value < MinValue or Value > MaxValue,
    IsCritical = Value < CriticalMin or Value > CriticalMax
| summarize
    TotalReadings = count(),
    OutOfRangeCount = countif(IsOutOfRange),
    CriticalCount = countif(IsCritical)
  by DeviceId, DeviceType, Location
| where OutOfRangeCount > 0
| extend AnomalyRate = round(100.0 * OutOfRangeCount / TotalReadings, 2)
| order by CriticalCount desc, AnomalyRate desc
| take 20
```

### 4.4 Pattern Analysis

```kql
// Patterns horaires (dÃ©tection de cycles)
Telemetry
| where DeviceType == "temperature"
| where Timestamp > ago(7d)
| extend Hour = hourofday(Timestamp)
| summarize AvgValue = avg(Value) by Hour, Zone
| order by Zone, Hour
| render linechart

// CorrÃ©lation entre capteurs
Telemetry
| where Timestamp > ago(1h)
| where DeviceType in ("temperature", "humidity")
| summarize Value = avg(Value) by bin(Timestamp, 1m), DeviceType
| evaluate pivot(DeviceType, avg(Value))
| extend Correlation = temperature / humidity
| render timechart
```

### 4.5 Fonctions RÃ©utilisables

```kql
// CrÃ©er une fonction pour les alertes
.create-or-alter function GetActiveAlerts() {
    Telemetry
    | where Timestamp > ago(5m)
    | join kind=inner AlertThresholds on $left.DeviceType == $right.DeviceType
    | where Value < CriticalMin or Value > CriticalMax
    | extend AlertType = iff(Value < CriticalMin, "LOW", "HIGH")
    | project
        Timestamp,
        DeviceId,
        DeviceType,
        Location,
        Zone,
        Value,
        AlertType,
        Threshold = iff(AlertType == "LOW", CriticalMin, CriticalMax)
    | order by Timestamp desc
}

// Utilisation
GetActiveAlerts()
| take 50
```

## Phase 5: Real-Time Dashboard (2h)

### 5.1 CrÃ©er le Dashboard

1. Fabric Workspace â†’ New â†’ Real-Time Dashboard
2. Nom : "SmartFactory_Operations_Dashboard"
3. Source : KQL Database "SmartFactory_Analytics"

### 5.2 Page 1: Vue d'Ensemble

**Tile 1: Active Devices (Card)**
```kql
Telemetry
| where Timestamp > ago(1m)
| summarize ActiveDevices = dcount(DeviceId)
```

**Tile 2: Average Temperature (Gauge)**
```kql
Telemetry
| where DeviceType == "temperature" and Timestamp > ago(5m)
| summarize Value = round(avg(Value), 1)
```
- Min: 0, Max: 100, Target: 50

**Tile 3: Active Alerts (Card - with conditional color)**
```kql
Telemetry
| where Timestamp > ago(5m)
| join kind=inner AlertThresholds on $left.DeviceType == $right.DeviceType
| where Value < CriticalMin or Value > CriticalMax
| summarize AlertCount = dcount(DeviceId)
```

**Tile 4: Temperature Trend (Time Chart)**
```kql
Telemetry
| where DeviceType == "temperature"
| where Timestamp > ago(1h)
| summarize AvgTemp = avg(Value) by bin(Timestamp, 1m), Zone
| render timechart
```

**Tile 5: Alert Table (Table)**
```kql
GetActiveAlerts()
| take 20
```

### 5.3 Page 2: Zone Analysis

**Tile 1: Temperature by Zone (Heatmap)**
```kql
Telemetry
| where DeviceType == "temperature"
| where Timestamp > ago(30m)
| summarize AvgTemp = avg(Value) by Zone, bin(Timestamp, 5m)
```

**Tile 2: Device Status (Donut Chart)**
```kql
Telemetry
| where Timestamp > ago(5m)
| join kind=inner AlertThresholds on $left.DeviceType == $right.DeviceType
| extend Status = case(
    Value < CriticalMin or Value > CriticalMax, "Critical",
    Value < MinValue or Value > MaxValue, "Warning",
    "Normal"
  )
| summarize Count = dcount(DeviceId) by Status
| render piechart
```

**Tile 3: Zone Performance (Bar Chart)**
```kql
Telemetry
| where Timestamp > ago(1h)
| summarize
    AvgValue = avg(Value),
    Readings = count(),
    Anomalies = countif(Quality == "warning")
  by Zone
| extend AnomalyRate = round(100.0 * Anomalies / Readings, 2)
| order by AnomalyRate desc
| render barchart
```

### 5.4 Page 3: Device Details

**Tile 1: Device Selector (Parameter)**
- Type: Dynamic
- Query: `Telemetry | distinct DeviceId | order by DeviceId`

**Tile 2: Device History (Time Chart)**
```kql
Telemetry
| where DeviceId == "${DeviceSelector}"
| where Timestamp > ago(24h)
| project Timestamp, Value
| render timechart
```

**Tile 3: Device Statistics (Table)**
```kql
Telemetry
| where DeviceId == "${DeviceSelector}"
| where Timestamp > ago(24h)
| summarize
    MinValue = min(Value),
    MaxValue = max(Value),
    AvgValue = round(avg(Value), 2),
    StdDev = round(stdev(Value), 2),
    Readings = count()
```

### 5.5 Configuration du Refresh

1. Dashboard Settings â†’ Auto-refresh
2. Interval: 30 seconds
3. Scope: All tiles

## Phase 6: Data Activator Alerts (2h)

### 6.1 CrÃ©er un Reflex

1. Fabric Workspace â†’ New â†’ Reflex
2. Nom : "SmartFactory_Alerts"
3. Source : KQL Database

### 6.2 DÃ©finir les Objets

**Object: High Temperature Alert**
```
Property: DeviceId
Property: Temperature (Value where DeviceType == "temperature")
Property: Location
Property: Zone
```

### 6.3 Configurer les Triggers

**Trigger 1: Critical Temperature**
```
Condition: Temperature > 70
Duration: Any occurrence
Cooldown: 5 minutes
Action: Send Teams message
```

Message template:
```
ğŸš¨ CRITICAL TEMPERATURE ALERT

Device: {DeviceId}
Location: {Location}
Zone: {Zone}
Temperature: {Temperature}Â°C
Time: {Timestamp}

Immediate action required!
```

**Trigger 2: Device Offline**
```
Condition: No data for 10 minutes
Duration: 10 minutes
Action: Send email
```

**Trigger 3: Vibration Anomaly**
```
Condition: Vibration > 2.0 mm/s
Duration: 3 consecutive readings
Action: Power Automate flow
```

### 6.4 Power Automate Integration

```json
{
  "trigger": "When Reflex trigger fires",
  "actions": [
    {
      "type": "Create_Teams_Notification",
      "channel": "Operations_Alerts",
      "message": "Alert from SmartFactory"
    },
    {
      "type": "Create_ServiceNow_Incident",
      "priority": "High",
      "description": "Automated incident from IoT monitoring"
    },
    {
      "type": "Log_to_SharePoint",
      "list": "Alert_History"
    }
  ]
}
```

## Phase 7: Tests et Validation (1h)

### 7.1 Tests Fonctionnels

**Test 1: Ingestion Performance**
```kql
// VÃ©rifier le dÃ©bit d'ingestion
Telemetry
| where Timestamp > ago(1h)
| summarize EventsPerMinute = count() by bin(Timestamp, 1m)
| summarize
    AvgEventsPerMin = avg(EventsPerMinute),
    MaxEventsPerMin = max(EventsPerMinute),
    MinEventsPerMin = min(EventsPerMinute)
```

**Test 2: Latence End-to-End**
```kql
// Mesurer la latence
Telemetry
| where Timestamp > ago(10m)
| extend Latency = ingestion_time() - Timestamp
| summarize
    AvgLatencySeconds = avg(Latency) / 1s,
    P95LatencySeconds = percentile(Latency, 95) / 1s
```

**Test 3: Data Quality**
```kql
// VÃ©rifier la qualitÃ© des donnÃ©es
Telemetry
| where Timestamp > ago(1h)
| summarize
    TotalRecords = count(),
    GoodQuality = countif(Quality == "good"),
    Warnings = countif(Quality == "warning"),
    NullValues = countif(isnull(Value))
| extend QualityRate = round(100.0 * GoodQuality / TotalRecords, 2)
```

### 7.2 Tests d'Alertes

1. Injecter une valeur anormale manuellement
2. VÃ©rifier que l'alerte se dÃ©clenche en < 1 minute
3. Confirmer la rÃ©ception de la notification
4. VÃ©rifier le cooldown (pas de spam)

### 7.3 Tests de Performance

```kql
// Performance des requÃªtes
.show queries
| where StartedOn > ago(1h)
| summarize
    AvgDuration = avg(Duration),
    MaxDuration = max(Duration),
    QueryCount = count()
```

## Livrables

### Checklist Technique
- [ ] EventStream configurÃ© et fonctionnel
- [ ] KQL Database crÃ©Ã©e avec tables et mappings
- [ ] 10+ requÃªtes KQL optimisÃ©es
- [ ] Dashboard temps rÃ©el avec 10+ tiles sur 3 pages
- [ ] 3+ alertes Data Activator configurÃ©es
- [ ] Tests de validation documentÃ©s
- [ ] Documentation architecture complÃ¨te

### Documentation Requise
- [ ] SchÃ©ma d'architecture dÃ©taillÃ©
- [ ] Dictionnaire des donnÃ©es (tous les champs)
- [ ] Catalogue des requÃªtes KQL avec explications
- [ ] Guide utilisateur du dashboard
- [ ] ProcÃ©dure de gestion des alertes
- [ ] SLA et mÃ©triques de performance

### MÃ©triques de SuccÃ¨s
```
Performance:
  âœ“ Latence d'ingestion < 5 secondes
  âœ“ Dashboard refresh < 30 secondes
  âœ“ Queries KQL < 2 secondes
  âœ“ Uptime EventStream > 99.5%

Fonctionnel:
  âœ“ 100% des capteurs monitorÃ©s
  âœ“ Alertes dÃ©clenchÃ©es en < 1 minute
  âœ“ Taux de faux positifs < 5%
  âœ“ Dashboard accessible 24/7
```

## CritÃ¨res d'Ã‰valuation

- Architecture et configuration EventStream (20%)
- QualitÃ© et performance des requÃªtes KQL (25%)
- ComplÃ©tude et UX du dashboard (25%)
- Configuration et fiabilitÃ© des alertes (15%)
- Documentation et tests (15%)

## Extensions Possibles

1. **Machine Learning** : PrÃ©diction de pannes avec ML dans Fabric
2. **Historical Analysis** : Archivage long terme vers Lakehouse
3. **Multi-site** : Extension Ã  plusieurs usines
4. **Mobile Dashboard** : AccÃ¨s mobile pour Ã©quipes terrain
5. **Integration ERP** : Connexion avec SAP/Oracle pour contexte business

---

**DurÃ©e estimÃ©e: 8-10 heures**

**Note:** Ce projet utilise des donnÃ©es simulÃ©es. Pour une implÃ©mentation production, configurez Azure IoT Hub et des capteurs rÃ©els.

[â¬…ï¸ Retour aux projets](../README.md)
