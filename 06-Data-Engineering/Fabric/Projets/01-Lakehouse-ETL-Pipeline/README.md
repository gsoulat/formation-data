# Projet 1 : Lakehouse ETL Pipeline

## Vue d'ensemble

Dans ce projet, vous allez construire un **pipeline ETL complet** pour une entreprise e-commerce fictive "TechRetail". Vous impl√©menterez l'architecture **Medallion** (Bronze/Silver/Gold) pour traiter des donn√©es de ventes provenant de multiples sources.

**Dur√©e estim√©e :** 8-10 heures
**Niveau :** Interm√©diaire
**Modules pr√©requis :** 02, 04, 05, 06

## Contexte Business

### L'entreprise : TechRetail

TechRetail est un retailer en ligne vendant des produits √©lectroniques. Ils ont :
- Un site web e-commerce
- 3 magasins physiques
- Une application mobile
- Un syst√®me CRM (Salesforce)

### Probl√©matique

Les donn√©es sont actuellement **silot√©es** :
- Ventes web : fichiers CSV quotidiens dans Azure Blob Storage
- Ventes magasins : base SQL Server on-premises
- Donn√©es produits : API REST
- Donn√©es clients : Salesforce

**Objectif :** Cr√©er une plateforme analytics unifi√©e dans Fabric avec architecture Medallion.

## Objectifs d'Apprentissage

√Ä la fin de ce projet, vous serez capable de :

- ‚úÖ Cr√©er une architecture Lakehouse Medallion compl√®te
- ‚úÖ Ing√©rer des donn√©es de sources multiples
- ‚úÖ Transformer des donn√©es avec Spark
- ‚úÖ Impl√©menter un chargement incr√©mental
- ‚úÖ Optimiser les tables Delta (V-Order, partitionnement)
- ‚úÖ Cr√©er des pipelines orchestr√©s
- ‚úÖ Documenter une solution data engineering

## üì¶ Donn√©es Fournies

**IMPORTANT : Les donn√©es pour ce projet sont disponibles dans `../../Ressources/datasets/`**

| Fichier | Description | Usage dans ce projet |
|---------|-------------|---------------------|
| **`retail_sales.csv`** (15 MB, 100K lignes) | Ventes e-commerce avec dates, montants, r√©gions | ‚Üí bronze_web_sales, bronze_store_sales |
| **`customers.csv`** (1.6 MB, 10K clients) | Clients avec pays, ville, date inscription | ‚Üí bronze_customers |
| **`products.csv`** (63 KB, 500 produits) | Catalogue produits avec cat√©gories et prix | ‚Üí bronze_products |

### Chargement des Donn√©es dans Fabric

1. **T√©l√©chargez les fichiers** depuis `Ressources/datasets/`
2. **Dans votre Workspace Fabric** :
   - Ouvrez votre Lakehouse
   - Cliquez sur **"Get data" ‚Üí "Upload files"**
   - Uploadez les 3 fichiers CSV dans le dossier `Files/raw/`
3. **Structure recommand√©e** :
   ```
   Files/
   ‚îú‚îÄ‚îÄ raw/
   ‚îÇ   ‚îú‚îÄ‚îÄ retail_sales.csv
   ‚îÇ   ‚îú‚îÄ‚îÄ customers.csv
   ‚îÇ   ‚îî‚îÄ‚îÄ products.csv
   ```

### Exemple de Chargement Spark

```python
# Dans un notebook Fabric
df_sales = spark.read.csv("Files/raw/retail_sales.csv", header=True, inferSchema=True)
df_customers = spark.read.csv("Files/raw/customers.csv", header=True, inferSchema=True)
df_products = spark.read.csv("Files/raw/products.csv", header=True, inferSchema=True)

# V√©rification
print(f"Ventes: {df_sales.count()} lignes")
print(f"Clients: {df_customers.count()} lignes")
print(f"Produits: {df_products.count()} lignes")
```

## Architecture Cible

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        SOURCES                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Azure Blob   ‚îÇ SQL Server   ‚îÇ REST API     ‚îÇ Salesforce     ‚îÇ
‚îÇ (CSV files)  ‚îÇ (on-prem)    ‚îÇ (Products)   ‚îÇ (Customers)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ              ‚îÇ              ‚îÇ                ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Data Pipelines‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ              ‚îÇ              ‚îÇ
       ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   BRONZE    ‚îÇ   SILVER    ‚îÇ    GOLD     ‚îÇ
‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ
‚îÇ  Raw Data   ‚îÇ  Cleaned    ‚îÇ Aggregated  ‚îÇ
‚îÇ  As-is      ‚îÇ  Validated  ‚îÇ Business    ‚îÇ
‚îÇ  Schema-on- ‚îÇ  Dedup      ‚îÇ Metrics     ‚îÇ
‚îÇ  read       ‚îÇ  Enriched   ‚îÇ Ready for   ‚îÇ
‚îÇ             ‚îÇ             ‚îÇ reporting   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Power BI    ‚îÇ
       ‚îÇ  Dashboards  ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Exigences Fonctionnelles

### Bronze Layer (Raw Data)

**Tables √† cr√©er :**

1. **bronze_web_sales**
   - Source : CSV files (Azure Blob)
   - Sch√©ma : order_id, customer_id, product_id, quantity, amount, order_date, source
   - Fr√©quence : Quotidienne
   - Format : Delta (non-partitionn√© initialement)

2. **bronze_store_sales**
   - Source : SQL Server
   - Sch√©ma : sale_id, store_id, product_id, quantity, price, sale_datetime
   - Fr√©quence : Temps r√©el (simulation: toutes les heures)
   - Chargement : Incr√©mental (watermark)

3. **bronze_products**
   - Source : REST API
   - Sch√©ma : product_id, name, category, price, stock_level
   - Fr√©quence : Quotidienne

4. **bronze_customers**
   - Source : Salesforce (simul√© avec CSV)
   - Sch√©ma : customer_id, name, email, country, city, signup_date
   - Fr√©quence : Quotidienne

### Silver Layer (Cleaned & Validated)

**Transformations requises :**

1. **silver_sales_unified**
   - Union de web_sales + store_sales
   - Nettoyage : remove nulls, deduplicate
   - Standardisation des colonnes
   - Validation : amount > 0, valid dates
   - Enrichissement avec donn√©es produits et clients

2. **silver_products_clean**
   - Remove duplicates
   - Validation de prix (> 0)
   - Standardisation category (uppercase, trim)

3. **silver_customers_clean**
   - Validation email
   - Deduplicate by email
   - Standardisation country codes

### Gold Layer (Business-Level Aggregates)

**Tables analytiques :**

1. **gold_daily_sales_by_category**
   - Ventes quotidiennes par cat√©gorie de produit
   - M√©triques : total_sales, total_orders, avg_order_value

2. **gold_monthly_sales_by_country**
   - Ventes mensuelles par pays
   - M√©triques : total_sales, unique_customers, items_sold

3. **gold_product_performance**
   - Performance des produits
   - M√©triques : total_revenue, units_sold, avg_price, stock_turnover

4. **gold_customer_rfm**
   - Analyse RFM (Recency, Frequency, Monetary)
   - Pour segmentation clients

## Exigences Techniques

### 1. Data Pipelines

Cr√©er **3 pipelines** :

**Pipeline 1 : `pl_ingest_bronze`**
- Ingestion de toutes les sources vers Bronze
- Parall√©lisation des Copy activities
- Error handling et logging

**Pipeline 2 : `pl_transform_silver`**
- Appel de notebooks Spark pour transformations
- Bronze ‚Üí Silver
- Dependencies entre notebooks

**Pipeline 3 : `pl_aggregate_gold`**
- Agr√©gations pour Gold layer
- Peut utiliser SQL ou Spark

### 2. Notebooks Spark

Cr√©er **4 notebooks** :

**Notebook 1 : `nb_bronze_validation.ipynb`**
- Valider les donn√©es brutes
- Data quality checks
- Logging des anomalies

**Notebook 2 : `nb_transform_sales.ipynb`**
- Union et transformation des ventes
- Enrichissement

**Notebook 3 : `nb_transform_dimensions.ipynb`**
- Nettoyage produits et clients

**Notebook 4 : `nb_gold_aggregations.ipynb`**
- Calcul des m√©triques Gold

### 3. Optimisations

**Requis :**
- Partitionner `silver_sales_unified` par `order_date` (ann√©e/mois)
- Activer V-Order sur toutes les tables Silver et Gold
- Utiliser Z-ordering sur `silver_sales_unified` (customer_id, product_id)
- OPTIMIZE r√©gulier
- VACUUM avec retention 7 jours

### 4. Scheduling

**Triggers :**
- Pipeline Bronze : tous les jours √† 2h00 (schedule trigger)
- Pipeline Silver : apr√®s succ√®s Bronze (tumbling window)
- Pipeline Gold : apr√®s succ√®s Silver

### 5. Monitoring

- Logging dans table `metadata.pipeline_runs`
- Suivi des row counts (source vs destination)
- Dur√©e d'ex√©cution
- Alertes en cas d'√©chec

## Jeux de Donn√©es Fournis

Dans le dossier `data/` :

- `web_sales/2024-01-*.csv` (30 jours de ventes web)
- `store_sales_dump.csv` (snapshot SQL Server)
- `products.json` (catalogue produits)
- `customers.csv` (base clients)

**Volum√©tries :**
- Web sales : ~500 MB (1M lignes)
- Store sales : ~200 MB (400K lignes)
- Products : 10K produits
- Customers : 50K clients

## Livrables Attendus

### 1. Architecture Fabric

- [ ] 3 Lakehouses cr√©√©s (Bronze, Silver, Gold)
- [ ] Toutes les tables cr√©√©es avec bon sch√©ma
- [ ] Partitionnement et optimisations appliqu√©s

### 2. Pipelines

- [ ] 3 pipelines fonctionnels
- [ ] Param√©trage et r√©utilisabilit√©
- [ ] Error handling impl√©ment√©

### 3. Notebooks

- [ ] 4 notebooks document√©s
- [ ] Code propre et comment√©
- [ ] Logs et validations

### 4. Documentation

- [ ] Sch√©ma d'architecture (draw.io, Visio)
- [ ] Data dictionary (description des tables/colonnes)
- [ ] D√©cisions techniques (pourquoi Bronze/Silver/Gold, choix de partitionnement, etc.)
- [ ] Guide de d√©ploiement

### 5. Tests

- [ ] Tests de validation des donn√©es
- [ ] Tests de performance (temps d'ex√©cution)
- [ ] Tests d'int√©grit√© (referential integrity)

## Guide de D√©marrage

### √âtape 1 : Setup environnement (1h)
1. Cr√©er un workspace Fabric "TechRetail-Analytics"
2. Cr√©er 3 Lakehouses (Bronze, Silver, Gold)
3. Uploader les datasets dans Bronze/Files/

### √âtape 2 : Bronze Layer (2h)
1. Cr√©er les pipelines d'ingestion
2. Copier les donn√©es vers tables Bronze
3. Valider les sch√©mas

### √âtape 3 : Silver Layer (3h)
1. Cr√©er les notebooks de transformation
2. Impl√©menter la logique de nettoyage
3. Enrichissement des donn√©es
4. Tests de qualit√©

### √âtape 4 : Gold Layer (2h)
1. Cr√©er les agr√©gations
2. Calculer les m√©triques business
3. Optimiser les tables

### √âtape 5 : Orchestration (1h)
1. Cha√Æner les pipelines
2. Configurer les triggers
3. Tester end-to-end

### √âtape 6 : Optimisation & Documentation (1h)
1. Appliquer V-Order, partitionnement
2. Mesurer les gains
3. Documenter la solution

## Crit√®res de R√©ussite

| Crit√®re | Points | D√©tails |
|---------|--------|---------|
| **Architecture** | 20 | Medallion correctement impl√©ment√© |
| **Ingestion** | 15 | Toutes sources int√©gr√©es |
| **Transformations** | 20 | Silver layer propre et valid√© |
| **Agr√©gations** | 15 | Gold layer pertinent |
| **Optimisation** | 15 | V-Order, partitionnement |
| **Orchestration** | 10 | Pipelines qui fonctionnent |
| **Documentation** | 5 | Clair et complet |
| **TOTAL** | 100 | |

**Passage :** 70/100

## Bonus (Optionnel)

Pour aller plus loin :

- [ ] Impl√©menter un v√©ritable CDC depuis SQL Server
- [ ] Cr√©er un dashboard Power BI sur Gold layer
- [ ] Ajouter des tests unitaires pour les transformations
- [ ] Impl√©menter Data Activator pour alertes
- [ ] Git integration et CI/CD

## Ressources

### Documentation
- [Module 02 : Lakehouse](../../02-Lakehouse/)
- [Module 04 : Data Pipelines](../../04-Data-Pipelines/)
- [Module 06 : Notebooks Spark](../../06-Notebooks-Spark/)

### Liens externes
- [Medallion Architecture](https://learn.microsoft.com/azure/databricks/lakehouse/medallion)
- [Delta Lake best practices](https://docs.delta.io/latest/best-practices.html)

## Solution de R√©f√©rence

Une solution compl√®te est disponible dans le dossier `solution/`.

**Attention :** Essayez de r√©soudre le projet par vous-m√™me avant de consulter la solution !

---

**Pr√™t √† commencer ?** Consultez le fichier [instructions.md](./instructions.md) pour le guide pas-√†-pas !

[‚¨ÖÔ∏è Retour aux Projets](../README.md)
