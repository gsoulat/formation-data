# Workloads et Composants

Microsoft Fabric intÃ¨gre **7 workloads principaux**, chacun avec ses propres composants et cas d'usage. Ce fichier dÃ©taille en profondeur chaque workload.

## Vue d'Ensemble des 7 Workloads

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Microsoft Fabric                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Data   â”‚   Data   â”‚   Data   â”‚   Data   â”‚ Real-Time  â”‚
â”‚ Engineer â”‚ Warehouseâ”‚  Factory â”‚  Science â”‚ Analytics  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Power BI â”‚              Data Activator                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Data Engineering

### ğŸ¯ Objectif
Construire des pipelines de donnÃ©es Ã  grande Ã©chelle avec Apache Spark et Delta Lake.

### ğŸ§© Composants Principaux

#### Lakehouse
Le composant central du Data Engineering dans Fabric.

**CaractÃ©ristiques :**
- Combine Data Lake + Data Warehouse
- Stockage Delta Lake natif
- 2 zones : `Files/` (non-structurÃ©) et `Tables/` (structurÃ©)
- SQL Analytics Endpoint automatique

**Structure typique :**
```
MyLakehouse/
â”œâ”€â”€ Files/
â”‚   â”œâ”€â”€ bronze/     # DonnÃ©es brutes
â”‚   â”œâ”€â”€ silver/     # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ gold/       # DonnÃ©es agrÃ©gÃ©es
â””â”€â”€ Tables/
    â”œâ”€â”€ customers   # Tables Delta
    â”œâ”€â”€ orders
    â””â”€â”€ products
```

**Use cases :**
- Architecture Medallion (Bronze/Silver/Gold)
- Data Lake pour analytics
- Feature store pour ML

#### Notebooks
Environnement de dÃ©veloppement interactif basÃ© sur Jupyter.

**Langages supportÃ©s :**
- Python (PySpark)
- Scala
- R
- SQL
- SparkSQL

**FonctionnalitÃ©s :**
- Cellules code + markdown
- Visualisations intÃ©grÃ©es
- Collaboration en temps rÃ©el
- Git integration
- Scheduling

**Magic commands :**
```python
%%pyspark  # ExÃ©cuter PySpark
%%sql      # ExÃ©cuter SQL
%%configure # Configurer Spark session
%run       # ExÃ©cuter autre notebook
```

#### Spark Job Definitions
Jobs Spark compilÃ©s pour exÃ©cution batch.

**Types :**
- JAR (Java/Scala)
- Python files
- SparkR scripts

**DiffÃ©rence avec Notebooks :**
- Notebooks : dÃ©veloppement interactif
- Jobs : exÃ©cution batch automatisÃ©e

### ğŸ“Š Cas d'Usage Data Engineering

1. **ETL/ELT Pipelines**
   - Ingestion de sources multiples
   - Transformations Spark
   - Chargement dans Warehouse/Lakehouse

2. **Data Lake Modernization**
   - Migration depuis HDFS/ADLS
   - Architecture Delta Lake
   - Gouvernance avec Purview

3. **Real-Time Processing**
   - Structured Streaming
   - Event processing
   - IoT data pipelines

---

## 2. Data Warehouse

### ğŸ¯ Objectif
Fournir un entrepÃ´t de donnÃ©es relationnel haute performance pour analytics SQL.

### ğŸ§© Composants

#### Synapse Data Warehouse
EntrepÃ´t de donnÃ©es massivement parallÃ¨le (MPP).

**Architecture :**
```
Compute (SQL Pools)
    â†“
Storage (OneLake)
```

**CaractÃ©ristiques :**
- T-SQL complet
- Separation compute/storage
- Scaling automatique
- Caching intelligent

**Tables supportÃ©es :**
- Tables rÃ©guliÃ¨res
- External tables (via Lakehouse)
- Materialized views

#### Distributions
StratÃ©gies de distribution des donnÃ©es :

**1. Round Robin**
```sql
CREATE TABLE staging_data (...)
WITH (DISTRIBUTION = ROUND_ROBIN);
```
- Distribution alÃ©atoire
- Bon pour staging
- Pas de data movement lors JOIN

**2. Hash Distribution**
```sql
CREATE TABLE fact_sales (...)
WITH (DISTRIBUTION = HASH(customer_id));
```
- Distribution par clÃ©
- Optimal pour grandes tables de fait
- Minimise shuffle lors JOIN sur clÃ© de distribution

**3. Replicated**
```sql
CREATE TABLE dim_product (...)
WITH (DISTRIBUTION = REPLICATE);
```
- Copie complÃ¨te sur chaque nÅ“ud
- IdÃ©al pour petites dimensions (<2GB)
- Zero data movement

### ğŸ“Š Cas d'Usage Data Warehouse

1. **Enterprise Data Warehouse**
   - ModÃ©lisation dimensionnelle
   - Star/Snowflake schema
   - Historisation (SCD Type 2)

2. **Reporting & Analytics**
   - RequÃªtes complexes
   - AgrÃ©gations lourdes
   - Integration Power BI

3. **Data Marts**
   - Marts mÃ©tier spÃ©cialisÃ©s
   - Performance optimisÃ©e
   - SÃ©curitÃ© granulaire (RLS)

---

## 3. Data Factory

### ğŸ¯ Objectif
Orchestrer l'ingestion et la transformation de donnÃ©es.

### ğŸ§© Composants

#### Data Pipelines
Workflows d'orchestration visuels (comme Azure Data Factory).

**Activities disponibles :**

**Data Movement:**
- Copy Data
- Delete Data

**Data Transformation:**
- Dataflow Gen2
- Notebook
- Stored Procedure
- Script (SQL, Python)

**Control Flow:**
- If Condition
- Switch
- ForEach
- Until
- Wait
- Webhook

**Example pipeline :**
```
[Copy Activity] â†’ [Notebook Transform] â†’ [Stored Proc] â†’ [Email Notification]
     â†“ (on failure)
[Log Error] â†’ [Alert]
```

#### Dataflows Gen2
Transformations low-code avec Power Query.

**CaractÃ©ristiques :**
- Interface visuelle
- Langage M
- 100+ connecteurs
- Destinations multiples
- Refresh incrÃ©mental

**DiffÃ©rence avec Dataflow Gen1 (Power BI) :**
| Feature | Gen1 | Gen2 |
|---------|------|------|
| Destinations | Power BI only | Lakehouse, Warehouse, KQL |
| Staging | Limited | OneLake native |
| Performance | Medium | Optimized |

### ğŸ“Š Cas d'Usage Data Factory

1. **Data Integration**
   - Ingestion multi-sources
   - Orchestration complexe
   - Scheduling

2. **Hybrid Scenarios**
   - On-premises â†’ Cloud
   - Multi-cloud integration
   - Legacy system integration

---

## 4. Data Science

### ğŸ¯ Objectif
DÃ©velopper et dÃ©ployer des modÃ¨les Machine Learning.

### ğŸ§© Composants

#### ML Notebooks
Notebooks optimisÃ©s pour Data Science.

**Libraries prÃ©-installÃ©es :**
- scikit-learn
- TensorFlow
- PyTorch
- XGBoost
- LightGBM
- pandas, numpy

**GPU Support :**
- GPU pools disponibles
- AccÃ©lÃ©ration training
- Deep Learning optimisÃ©

#### MLflow Integration
Plateforme de suivi des experiments ML.

**FonctionnalitÃ©s :**
- Tracking (params, metrics, artifacts)
- Model Registry
- Model versioning
- Deployment automation

**Example :**
```python
import mlflow

with mlflow.start_run():
    mlflow.log_param("n_estimators", 100)
    mlflow.log_metric("accuracy", 0.95)
    mlflow.sklearn.log_model(model, "model")
```

#### AutoML
Machine Learning automatisÃ©.

**TÃ¢ches supportÃ©es :**
- Classification
- Regression
- Forecasting (time series)

**Process :**
1. Upload data
2. Select target column
3. Configure (time limit, metrics)
4. Run AutoML
5. Get best model + insights

### ğŸ“Š Cas d'Usage Data Science

1. **Predictive Analytics**
   - Churn prediction
   - Demand forecasting
   - Fraud detection

2. **Recommendation Systems**
   - Product recommendations
   - Content personalization

3. **NLP & Computer Vision**
   - Sentiment analysis
   - Image classification
   - Object detection

---

## 5. Real-Time Analytics

### ğŸ¯ Objectif
Analyser des flux de donnÃ©es en temps rÃ©el.

### ğŸ§© Composants

#### EventStream
Ingestion de flux de donnÃ©es.

**Sources :**
- Azure Event Hubs
- Azure IoT Hub
- Kafka
- Custom apps (API)

**Transformations :**
- Filtering
- Aggregations
- Windowing
- Enrichment

**Destinations :**
- KQL Database
- Lakehouse
- Custom endpoints

#### KQL Database
Base de donnÃ©es optimisÃ©e pour time-series.

**BasÃ©e sur Azure Data Explorer (Kusto).**

**CaractÃ©ristiques :**
- Ingestion temps rÃ©el (<1 sec latency)
- RequÃªtes ultra-rapides
- Compression efficace
- Retention policies

#### Kusto Query Language (KQL)
Langage de requÃªte optimisÃ© pour logs/telemetry.

**Example :**
```kql
Logs
| where TimeGenerated > ago(1h)
| where Level == "Error"
| summarize count() by bin(TimeGenerated, 5m)
| render timechart
```

### ğŸ“Š Cas d'Usage Real-Time Analytics

1. **IoT Monitoring**
   - Sensor data analysis
   - Predictive maintenance
   - Anomaly detection

2. **Application Monitoring**
   - Log analytics
   - Performance monitoring
   - Error tracking

3. **Business Metrics**
   - Real-time dashboards
   - KPI tracking
   - Alerting

---

## 6. Power BI

### ğŸ¯ Objectif
Business Intelligence et visualisation de donnÃ©es.

### ğŸ§© Composants

#### Semantic Models
ModÃ¨les de donnÃ©es (anciennement "Datasets").

**Types de connexion :**

**1. Import**
- DonnÃ©es en mÃ©moire
- Performance maximale
- Refresh scheduling requis

**2. DirectQuery**
- RequÃªtes temps rÃ©el
- Pas de limite de taille
- Performance dÃ©pend de la source

**3. Direct Lake (NOUVEAU dans Fabric)**
- Lecture directe depuis OneLake
- Performance Import
- FraÃ®cheur DirectQuery
- **Game changer !**

#### Reports & Dashboards
Visualisations interactives.

**FonctionnalitÃ©s :**
- 100+ visualisations
- InteractivitÃ© (cross-filtering)
- Drill-through
- Bookmarks
- Mobile layouts

#### Paginated Reports
Rapports formatÃ©s pour impression (style SSRS).

### ğŸ“Š Cas d'Usage Power BI

1. **Self-Service BI**
   - Rapports adhoc
   - Exploration interactive
   - Data-driven decisions

2. **Operational Reporting**
   - Dashboards KPI
   - Monitoring temps rÃ©el
   - Alertes automatiques

---

## 7. Data Activator

### ğŸ¯ Objectif
Automatisation basÃ©e sur les donnÃ©es (no-code).

### ğŸ§© Composants

#### Triggers
DÃ©tection de conditions sur les donnÃ©es.

**Types de conditions :**
- Seuils (> 100, < 50)
- Changements (augmentation de 20%)
- Patterns (absence de donnÃ©es)
- Anomalies

#### Actions
RÃ©actions automatiques.

**Actions disponibles :**
- Email notifications
- Teams messages
- Power Automate flows
- Webhooks

**Example :**
```
Trigger: Sales dropped > 20% vs yesterday
  â†“
Action: Send Teams alert to Sales Manager
```

### ğŸ“Š Cas d'Usage Data Activator

1. **Alerting**
   - SLA violations
   - Threshold breaches
   - Anomaly alerts

2. **Process Automation**
   - Trigger workflows
   - Update systems
   - Notify stakeholders

---

## Interactions Entre Workloads

### ScÃ©nario 1 : Analytics End-to-End

```
[Sources]
  â†“
[Data Factory] â†’ Ingestion
  â†“
[Data Engineering] â†’ Transformation (Lakehouse)
  â†“
[Data Warehouse] â†’ ModÃ©lisation (Star schema)
  â†“
[Power BI] â†’ Visualisation (Direct Lake)
  â†“
[Data Activator] â†’ Alertes
```

### ScÃ©nario 2 : ML Pipeline

```
[Data Engineering] â†’ Feature engineering
  â†“
[Data Science] â†’ Model training (MLflow)
  â†“
[Data Engineering] â†’ Batch scoring
  â†“
[Power BI] â†’ Predictions visualization
```

### ScÃ©nario 3 : Real-Time Monitoring

```
[IoT Devices]
  â†“
[Real-Time Analytics] â†’ EventStream + KQL
  â†“
[Power BI] â†’ Real-time dashboard
  â†“
[Data Activator] â†’ Alerts
```

## Choisir le Bon Workload

| Besoin | Workload RecommandÃ© |
|--------|---------------------|
| ETL large Ã©chelle | Data Engineering (Spark) |
| RequÃªtes SQL complexes | Data Warehouse |
| Ingestion orchestrÃ©e | Data Factory |
| Machine Learning | Data Science |
| Streaming analytics | Real-Time Analytics |
| Visualisation BI | Power BI |
| Alertes automatiques | Data Activator |

## Points ClÃ©s Ã  Retenir

- 7 workloads couvrent tout le cycle de vie de la donnÃ©e
- Chaque workload a ses composants spÃ©cialisÃ©s
- Integration native entre tous les workloads
- OneLake unifie le stockage
- Choisir le bon workload selon le use case

---

**Prochain fichier :** [04 - Workspaces et CapacitÃ©s](./04-workspaces-capacites.md)

[â¬…ï¸ Fichier prÃ©cÃ©dent](./02-architecture-onelake.md) | [â¬…ï¸ Retour au README du module](./README.md)
