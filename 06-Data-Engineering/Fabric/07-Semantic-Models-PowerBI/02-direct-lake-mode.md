# Direct Lake Mode

## Introduction

**Direct Lake** est l'innovation rÃ©volutionnaire de Microsoft Fabric, combinant la performance de l'Import avec la fraÃ®cheur des donnÃ©es du DirectQuery, sans compromis.

```
Evolution of Storage Modes:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Import (Legacy)                         â”‚
â”‚  âœ… Fast â”‚ âŒ Stale data â”‚ âŒ Refreshes  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DirectQuery (Legacy)                    â”‚
â”‚  âœ… Fresh â”‚ âŒ Slow â”‚ âŒ Source load     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Direct Lake (Fabric Innovation) ğŸš€      â”‚
â”‚  âœ… Fast â”‚ âœ… Fresh â”‚ âœ… No copy         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture Technique

### Comment Ã§a fonctionne

```
Traditional Import:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Copy   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Query  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  VertiPaq â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Power BI â”‚
â”‚ (Delta)  â”‚  (ETL)  â”‚  (memory) â”‚ (fast)  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Latency: Refresh schedule (hours/days)
  Duplication: Yes (source + VertiPaq)

Direct Lake:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Query  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OneLake  â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”‚  VertiPaq â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Power BI â”‚
â”‚ (Delta)  â”‚  Direct â”‚  (reads   â”‚ (fast)  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Read   â”‚  Parquet) â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Latency: Real-time (seconds)
  Duplication: No (reads directly from Delta)
```

### Technical Details

```
Direct Lake leverages:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Delta Lake Format                    â”‚
â”‚     â”œâ”€ Parquet files (columnar)          â”‚
â”‚     â”œâ”€ Transaction log                   â”‚
â”‚     â””â”€ Metadata (statistics, schema)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. VertiPaq Engine                      â”‚
â”‚     â”œâ”€ Reads Parquet directly            â”‚
â”‚     â”œâ”€ In-memory cache (hot data)        â”‚
â”‚     â””â”€ Columnar compression              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. OneLake Integration                  â”‚
â”‚     â”œâ”€ Single storage (no duplication)   â”‚
â”‚     â”œâ”€ ADLS Gen2 compatible              â”‚
â”‚     â””â”€ Unified security                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key innovation:
  VertiPaq can read Parquet files directly
  without importing/copying data first!
```

## Avantages vs Import et DirectQuery

### Performance Comparison

```
Query Performance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Import:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (Fast)      â”‚
â”‚  Direct Lake:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (Very Fast) â”‚
â”‚  DirectQuery:    â–ˆâ–ˆâ–ˆâ–ˆ        (Slow)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Freshness:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Import:         â–ˆâ–ˆ          (Hours/Days)â”‚
â”‚  Direct Lake:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (Seconds)   â”‚
â”‚  DirectQuery:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (Real-time) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Volume Limit:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Import:         â–ˆâ–ˆâ–ˆ         (GB limit)  â”‚
â”‚  Direct Lake:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (TB scale)  â”‚
â”‚  DirectQuery:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (Unlimited) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Matrix

```
Feature             â”‚ Import â”‚ DirectQuery â”‚ Direct Lake
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query Speed         â”‚   âœ…   â”‚      âŒ     â”‚     âœ…
Data Freshness      â”‚   âŒ   â”‚      âœ…     â”‚     âœ…
No Data Duplication â”‚   âŒ   â”‚      âœ…     â”‚     âœ…
Full DAX Support    â”‚   âœ…   â”‚      âš ï¸     â”‚     âœ…
Large Datasets      â”‚   âŒ   â”‚      âœ…     â”‚     âœ…
Offline Access      â”‚   âœ…   â”‚      âŒ     â”‚     âš ï¸
Incremental Refresh â”‚   âœ…   â”‚      âŒ     â”‚     âœ…
Aggregations        â”‚   âœ…   â”‚      âš ï¸     â”‚     âœ…
```

## Lecture Directe depuis OneLake

### Data Flow

```
1. User queries Power BI report
         â†“
2. DAX query sent to VertiPaq engine
         â†“
3. VertiPaq reads Delta table metadata
         â†“
4. VertiPaq identifies relevant Parquet files
         â†“
5. VertiPaq reads Parquet columnar data directly
         â†“
6. VertiPaq applies filters, aggregations
         â†“
7. Results returned to Power BI
         â†“
8. Visual updated

Speed: Milliseconds for most queries!
```

### Caching Strategy

```
Direct Lake Intelligent Caching:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hot Data (Frequently Accessed)          â”‚
â”‚  â”œâ”€ Cached in VertiPaq memory            â”‚
â”‚  â”œâ”€ Sub-second query response            â”‚
â”‚  â””â”€ Example: Current month sales         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Warm Data (Occasionally Accessed)       â”‚
â”‚  â”œâ”€ Partially cached                     â”‚
â”‚  â”œâ”€ Read from Delta on-demand            â”‚
â”‚  â””â”€ Example: Last year sales             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Cold Data (Rarely Accessed)             â”‚
â”‚  â”œâ”€ Not cached                           â”‚
â”‚  â”œâ”€ Read from OneLake when needed        â”‚
â”‚  â””â”€ Example: Historical data (5+ years)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
  âœ… Memory efficiency (cache what matters)
  âœ… Fast for common queries
  âœ… Support for large datasets
```

## PrÃ©requis et Limitations

### Requirements

```
âœ… Required:
  1. Microsoft Fabric capacity (F SKU)
  2. Data stored in OneLake
  3. Delta Lake table format
  4. Parquet file format (V-Order optimized)
  5. Compatible data types

âœ… Recommended:
  â€¢ V-Order optimization enabled
  â€¢ Delta table optimized (OPTIMIZE command)
  â€¢ Appropriate partitioning
  â€¢ Regular VACUUM for cleanup
```

### Creating Direct Lake Model

```
Step-by-step:
1. Create Lakehouse with Delta tables
   spark.sql("CREATE TABLE sales (...) USING DELTA")

2. Optimize tables for Direct Lake
   spark.sql("OPTIMIZE sales")
   spark.sql("OPTIMIZE sales ZORDER BY (customer_id)")

3. Create Semantic Model
   Workspace â†’ New â†’ Semantic model
   Source: Lakehouse (auto-detects Direct Lake)

4. Verify mode
   Model settings â†’ Storage mode â†’ "Direct Lake"

5. Create Power BI report
   Connect to semantic model
   Build visualizations
```

### Data Type Support

```
âœ… Supported Types:
  â€¢ INT, BIGINT, SMALLINT
  â€¢ DECIMAL, DOUBLE, FLOAT
  â€¢ STRING, VARCHAR
  â€¢ DATE, TIMESTAMP
  â€¢ BOOLEAN
  â€¢ BINARY (with limitations)

âŒ Unsupported Types:
  â€¢ Complex types (ARRAY, STRUCT, MAP)
  â€¢ Custom UDTs

Workaround:
  Flatten complex types in Delta table:
  df.select(col("address.city").alias("city"))
```

### Size Limitations

```
Direct Lake Limits (as of 2024):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Maximum Capacity                        â”‚
â”‚  â”œâ”€ F2:    10 GB                         â”‚
â”‚  â”œâ”€ F64:   64 GB                         â”‚
â”‚  â”œâ”€ F128:  128 GB                        â”‚
â”‚  â””â”€ F2048: 2 TB                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Per Table                               â”‚
â”‚  â””â”€ Unlimited rows                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tables per Model                        â”‚
â”‚  â””â”€ Unlimited (within capacity limit)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: Limits refer to uncompressed size
      Actual compressed size much smaller
```

## Fallback to DirectQuery

### When Fallback Occurs

```
Automatic fallback to DirectQuery when:
âŒ Unsupported data type encountered
âŒ Unsupported DAX expression used
âŒ Model size exceeds capacity limit
âŒ Complex calculated columns
âŒ Some advanced DAX functions

Example:
  Model has 100 GB data on F64 (64 GB limit)
  â†’ Automatic fallback to DirectQuery mode
  â†’ Queries still work (but slower)
```

### Detecting Fallback

```
Check storage mode:
1. Model settings â†’ Tables
2. Look for "DirectQuery" instead of "Direct Lake"

Query Performance:
  â€¢ Slow queries = likely in DirectQuery mode
  â€¢ Check Performance Analyzer

DAX Studio:
  â€¢ Connect to model
  â€¢ Run query
  â€¢ Check "Storage Engine" in query plan
  â€¢ "SE" (Storage Engine) = Direct Lake âœ…
  â€¢ "DirectQuery" = Fallback âŒ
```

### Preventing Fallback

```
âœ… Best Practices:
1. Keep model size within capacity limit
   â€¢ Use partitioning
   â€¢ Archive old data
   â€¢ Aggregate historical data

2. Avoid unsupported DAX
   â€¢ Test measures in DAX Studio
   â€¢ Check compatibility list

3. Simplify calculated columns
   â€¢ Move logic to source (Spark)
   â€¢ Use measures instead

4. Optimize Delta tables
   â€¢ OPTIMIZE regularly
   â€¢ Z-Order on filter columns
   â€¢ VACUUM old versions

5. Monitor capacity usage
   â€¢ Fabric Capacity Metrics app
   â€¢ Set up alerts
```

## Use Cases Optimaux

### âœ… Perfect for Direct Lake

```
1. Large-scale analytics
   â€¢ Sales data (millions of rows)
   â€¢ Web analytics (billions of events)
   â€¢ IoT telemetry

2. Near real-time dashboards
   â€¢ Executive dashboards
   â€¢ Operational reports
   â€¢ Live KPI monitoring

3. Medallion architecture
   â€¢ Bronze/Silver/Gold layers
   â€¢ Each layer as Direct Lake model
   â€¢ Single source of truth

4. Self-service BI
   â€¢ Business users create reports
   â€¢ Centralized semantic model
   â€¢ Consistent business logic

5. Mixed latency requirements
   â€¢ Some data needs to be fresh (Direct Lake)
   â€¢ Some can be stale (cached)
```

### âš ï¸ Consider Alternatives

```
Use Import when:
  âŒ Data sources outside OneLake
  âŒ Need offline access (Power BI Desktop)
  âŒ Complex data transformations in Power Query

Use DirectQuery when:
  âŒ Data source is not Delta Lake
  âŒ Direct connection to SQL Server/Warehouse
  âŒ Compliance requires no data copy

Use Composite when:
  âŒ Mix of Import + Direct Lake
  âŒ Aggregations for performance
  âŒ Hybrid scenarios
```

## Monitoring & Optimization

### Performance Monitoring

```
Tools:
1. Performance Analyzer (Power BI)
   â€¢ Analyze visual performance
   â€¢ Identify slow queries
   â€¢ DAX query duration

2. DAX Studio
   â€¢ Server timings
   â€¢ Query plans
   â€¢ Storage engine queries

3. Fabric Capacity Metrics
   â€¢ Capacity utilization
   â€¢ Memory usage
   â€¢ Throttling events

Metrics to watch:
  âœ… Query duration < 1 second (good)
  âš ï¸ Query duration 1-5 seconds (ok)
  âŒ Query duration > 5 seconds (investigate)
```

### Optimization Techniques

**1. Delta Table Optimization:**
```sql
-- Compact small files
OPTIMIZE sales;

-- Z-Order for faster filters
OPTIMIZE sales ZORDER BY (customer_id, order_date);

-- Enable V-Order
ALTER TABLE sales
SET TBLPROPERTIES ('delta.parquet.vorder.enabled' = 'true');

-- Clean old versions
VACUUM sales RETAIN 168 HOURS;
```

**2. Model Optimization:**
```
â€¢ Remove unused columns
â€¢ Hide technical columns
â€¢ Use appropriate data types (int vs string)
â€¢ Minimize calculated columns
â€¢ Create aggregation tables for large fact tables
```

**3. DAX Optimization:**
```dax
-- âŒ Bad: FILTER on large table
Sales Amount =
CALCULATE(
    SUM(Sales[Amount]),
    FILTER(
        ALL(Sales),
        Sales[Country] = "FR"
    )
)

-- âœ… Good: Use CALCULATE filter argument
Sales Amount =
CALCULATE(
    SUM(Sales[Amount]),
    Sales[Country] = "FR"
)
```

## Example: Create Direct Lake Model

### Step 1: Prepare Delta Table

```python
# In Fabric Notebook
from pyspark.sql.functions import *

# Load data
df = spark.read.parquet("Files/raw/sales.parquet")

# Transform
sales_clean = df.filter(col("amount") > 0) \
    .withColumn("year", year(col("order_date"))) \
    .withColumn("month", month(col("order_date")))

# Write as Delta with V-Order
sales_clean.write.format("delta") \
    .mode("overwrite") \
    .partitionBy("year", "month") \
    .option("delta.parquet.vorder.enabled", "true") \
    .saveAsTable("sales")

# Optimize
spark.sql("OPTIMIZE sales ZORDER BY (customer_id, product_id)")
```

### Step 2: Create Semantic Model

```
1. Workspace â†’ + New â†’ Semantic model
2. Name: "Sales Analytics"
3. Source: Lakehouse â†’ Select your lakehouse
4. Select tables:
   âœ… sales
   âœ… customers
   âœ… products
   âœ… date_dim
5. Create
```

### Step 3: Configure Model

```
Model View in Power BI:
1. Define relationships
   sales[customer_id] â†’ customers[customer_id]
   sales[product_id] â†’ products[product_id]
   sales[order_date] â†’ date_dim[date]

2. Hide technical columns
   sales[customer_id], sales[product_id] = Hidden

3. Create measures
   Total Sales = SUM(sales[amount])
   Order Count = COUNTROWS(sales)

4. Verify Direct Lake mode
   Settings â†’ Storage mode â†’ "Direct Lake" âœ…

5. Publish
```

### Step 4: Validate Performance

```
1. Create test report
2. Add visuals (table, chart)
3. Run Performance Analyzer
4. Check query duration < 1 second

5. Open DAX Studio
6. Connect to model
7. Run query:
   EVALUATE SUMMARIZE(sales, sales[year], "Total", SUM(sales[amount]))
8. Check query plan shows "Storage Engine" (not DirectQuery)
```

## Points ClÃ©s

- Direct Lake = rÃ©volution Fabric (Import speed + DirectQuery freshness)
- Lecture directe des fichiers Parquet Delta sans copie
- VertiPaq lit le format Delta nativement
- PrÃ©requis: OneLake, Delta Lake, V-Order
- Caching intelligent (hot/warm/cold data)
- Fallback to DirectQuery si limitations dÃ©passÃ©es
- Optimal pour large-scale analytics temps-rÃ©el
- Monitoring via Performance Analyzer et DAX Studio
- Optimization: OPTIMIZE, Z-Order, V-Order sur Delta tables
- Limite de taille selon SKU Fabric (F2-F2048)

---

**Prochain fichier :** [03 - Import vs DirectQuery vs Direct Lake](./03-import-vs-directquery.md)

[â¬…ï¸ Fichier prÃ©cÃ©dent](./01-semantic-models-overview.md) | [â¬…ï¸ Retour au README du module](./README.md)
