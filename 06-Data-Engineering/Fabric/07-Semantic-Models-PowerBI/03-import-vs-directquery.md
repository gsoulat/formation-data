# Import vs DirectQuery vs Direct Lake

## Introduction

Le choix du mode de stockage (Storage Mode) est une dÃ©cision cruciale qui impacte la performance, la fraÃ®cheur des donnÃ©es, et les coÃ»ts. Ce fichier compare les trois modes disponibles dans Fabric.

```
Storage Modes Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Import   â”‚ DirectQuery â”‚ Direct Lake  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ In-Memory  â”‚ Live Query  â”‚ Direct Read  â”‚
â”‚ Fast âœ…    â”‚ Fresh âœ…    â”‚ Both âœ…      â”‚
â”‚ Stale âŒ   â”‚ Slow âŒ     â”‚ Best ğŸš€      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Import Mode

### Architecture

```
Import Mode Data Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source  â”‚ (SQL Server, CSV, API, etc.)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ ETL/Copy
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Power BI Import             â”‚
â”‚  â”œâ”€ VertiPaq Engine          â”‚
â”‚  â”œâ”€ Compressed columnar      â”‚
â”‚  â”œâ”€ In-memory storage        â”‚
â”‚  â””â”€ Data snapshot            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Query
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Power BI  â”‚
â”‚Reports   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key characteristic:
  Data is COPIED into Power BI model
```

### Avantages âœ…

```
1. Performance maximale
   â€¢ DonnÃ©es en mÃ©moire (RAM)
   â€¢ Compression 10:1 ratio typical
   â€¢ Sub-second query response
   â€¢ Optimal pour visualizations complexes

2. Full DAX Support
   â€¢ Toutes fonctions DAX disponibles
   â€¢ Calculated columns
   â€¢ Calculated tables
   â€¢ Complex measures

3. Offline Access
   â€¢ Power BI Desktop hors ligne
   â€¢ DonnÃ©es disponibles localement
   â€¢ Pas de connexion requise

4. Data Transformations
   â€¢ Power Query (M language)
   â€¢ Riche set de transformations
   â€¢ Merge/append queries
   â€¢ Custom functions

5. Predictable Performance
   â€¢ IndÃ©pendant de la source
   â€¢ Pas de latence rÃ©seau
   â€¢ Pas de charge sur source
```

### InconvÃ©nients âŒ

```
1. Data Latency
   â€¢ Refresh schedule required
   â€¢ Data can be hours/days old
   â€¢ Not real-time

2. Size Limitations
   â€¢ Pro: 1 GB compressed
   â€¢ Premium: 10 GB compressed (per SKU)
   â€¢ Fabric: varies by capacity (F2-F2048)

3. Memory Consumption
   â€¢ Uses Capacity memory
   â€¢ Large models = high CU cost
   â€¢ Can cause throttling

4. Refresh Duration
   â€¢ Large datasets = long refresh
   â€¢ Impacts capacity availability
   â€¢ Scheduled downtime

5. Data Duplication
   â€¢ Source + Power BI copy
   â€¢ Storage cost 2x
   â€¢ Sync management
```

### Use Cases

```
âœ… Best for:
  â€¢ Small to medium datasets (< 1 GB)
  â€¢ Data changes infrequently (daily/weekly)
  â€¢ Complex DAX calculations needed
  â€¢ Need offline access
  â€¢ Source has poor query performance
  â€¢ Historical analysis (no real-time need)

Examples:
  âœ… Monthly financial reports
  âœ… HR dashboards (updated weekly)
  âœ… Customer segmentation analysis
  âœ… Historical trend analysis
```

### Configuration

```
Power BI Desktop:
1. Home â†’ Get Data â†’ [Source]
2. Transform Data (Power Query)
3. Load to model
4. Storage mode: Import (default)

Refresh Schedule (Power BI Service):
1. Dataset settings
2. Scheduled refresh
3. Frequency: Daily/Weekly
4. Time slots: Choose off-peak hours
5. Notify on failure: Yes
```

## DirectQuery Mode

### Architecture

```
DirectQuery Mode Data Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source  â”‚ (SQL Server, Synapse, Oracle)
â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”˜
     â”‚ Live Query (on each visual interaction)
     â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Power BI DirectQuery        â”‚
â”‚  â”œâ”€ No data storage          â”‚
â”‚  â”œâ”€ Query translation        â”‚
â”‚  â”œâ”€ DAX â†’ SQL                â”‚
â”‚  â””â”€ Result cache (limited)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Render
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Power BI  â”‚
â”‚Reports   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key characteristic:
  Every visual interaction = query to source
```

### Avantages âœ…

```
1. Data Freshness
   â€¢ Real-time or near real-time
   â€¢ Always current
   â€¢ No refresh schedule needed

2. No Size Limit
   â€¢ Billions of rows supported
   â€¢ No import size restrictions
   â€¢ Unlimited data volume

3. Low Memory Footprint
   â€¢ No data in Power BI model
   â€¢ Minimal capacity usage
   â€¢ Lower CU consumption

4. Source Security
   â€¢ RLS enforced at source
   â€¢ Single security model
   â€¢ Compliance (data locality)

5. No Data Duplication
   â€¢ Single source of truth
   â€¢ No sync issues
   â€¢ Lower storage cost
```

### InconvÃ©nients âŒ

```
1. Performance Variability
   â€¢ Depends on source performance
   â€¢ Network latency impact
   â€¢ Can be very slow

2. Limited DAX Support
   â€¢ Some functions not supported
   â€¢ No calculated columns
   â€¢ Limited time intelligence

3. Source Load
   â€¢ Every query hits source
   â€¢ Can overwhelm source DB
   â€¢ Concurrent users = scaling issues

4. No Offline Access
   â€¢ Requires active connection
   â€¢ Won't work offline
   â€¢ Connectivity dependent

5. Query Limitations
   â€¢ 1 million row limit per visual
   â€¢ Some complex queries fail
   â€¢ No cross-source queries
```

### Use Cases

```
âœ… Best for:
  â€¢ Very large datasets (> 100 GB)
  â€¢ Need real-time data
  â€¢ Source has excellent query performance
  â€¢ Compliance requires no data copy
  â€¢ Operational dashboards
  â€¢ Shared data source (multiple tools)

Examples:
  âœ… Real-time manufacturing dashboards
  âœ… Live sales monitoring
  âœ… Call center analytics
  âœ… Fraud detection systems
```

### Configuration

```
Power BI Desktop:
1. Home â†’ Get Data â†’ [Source]
2. Connection mode: DirectQuery âš ï¸
3. Limited Power Query transformations
4. Load to model

Optimize source:
  â€¢ Create indexes on filter columns
  â€¢ Create indexed views
  â€¢ Optimize query plans
  â€¢ Consider caching layer (Redis)
```

## Direct Lake Mode

### Architecture

```
Direct Lake Mode Data Flow (Fabric only):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OneLake         â”‚
â”‚  (Delta Tables)  â”‚
â”‚  â””â”€ Parquet filesâ”‚
â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Direct Read (columnar scan)
     â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Power BI Direct Lake        â”‚
â”‚  â”œâ”€ VertiPaq reads Parquet   â”‚
â”‚  â”œâ”€ Smart caching            â”‚
â”‚  â”œâ”€ No data copy             â”‚
â”‚  â””â”€ Delta metadata           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Query
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Power BI  â”‚
â”‚Reports   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key characteristic:
  Reads Delta Parquet files directly into VertiPaq
```

### Avantages âœ…

```
1. Import-like Performance
   â€¢ Sub-second queries
   â€¢ Columnar read from Parquet
   â€¢ In-memory caching

2. DirectQuery-like Freshness
   â€¢ Near real-time (seconds)
   â€¢ Auto-refresh on Delta update
   â€¢ No manual refresh

3. No Data Duplication
   â€¢ Single copy in OneLake
   â€¢ Reads directly from source
   â€¢ Lower storage cost

4. Large Scale
   â€¢ TBs of data supported
   â€¢ Capacity-based limits
   â€¢ Incremental refresh

5. Full DAX Support
   â€¢ Almost all DAX functions
   â€¢ Calculated columns
   â€¢ Complex measures
```

### InconvÃ©nients âŒ

```
1. Fabric Only
   â€¢ Requires Microsoft Fabric
   â€¢ Needs F-SKU capacity
   â€¢ Not available in Power BI Pro

2. OneLake Requirement
   â€¢ Must use OneLake storage
   â€¢ Delta Lake format required
   â€¢ Can't use external sources directly

3. Data Type Limitations
   â€¢ Some types unsupported
   â€¢ Complex types need flattening
   â€¢ ARRAY/STRUCT not supported

4. Fallback to DirectQuery
   â€¢ When limits exceeded
   â€¢ Performance degradation
   â€¢ Need monitoring

5. Capacity Limits
   â€¢ Size limit per capacity SKU
   â€¢ F2: 10 GB, F64: 64 GB, etc.
   â€¢ Need right-sizing
```

### Use Cases

```
âœ… Best for:
  â€¢ Large datasets in OneLake
  â€¢ Need both speed and freshness
  â€¢ Medallion architecture (Bronze/Silver/Gold)
  â€¢ Fabric-native workloads
  â€¢ Lakehouse-based analytics
  â€¢ Modern data platform

Examples:
  âœ… Executive dashboards on Lakehouse
  âœ… Self-service BI on OneLake
  âœ… Near real-time operational reports
  âœ… Large-scale customer analytics
```

### Configuration

```
Fabric Workspace:
1. Create Lakehouse with Delta tables
2. Optimize with V-Order:
   ALTER TABLE sales
   SET TBLPROPERTIES ('delta.parquet.vorder.enabled' = 'true')

3. Create Semantic Model
   â€¢ Source: Lakehouse
   â€¢ Auto-detected as Direct Lake
   â€¢ Select tables

4. Verify Direct Lake mode
   â€¢ Settings â†’ Storage mode = "Direct Lake"
```

## Comparison Matrix

### Performance

```
Scenario              â”‚ Import â”‚ DirectQuery â”‚ Direct Lake
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Simple aggregation    â”‚  âš¡âš¡âš¡ â”‚    âš¡       â”‚    âš¡âš¡âš¡
Complex calculation   â”‚  âš¡âš¡âš¡ â”‚    âŒ      â”‚    âš¡âš¡
Large scans           â”‚  âš¡âš¡âš¡ â”‚    ğŸŒ      â”‚    âš¡âš¡
Filtering             â”‚  âš¡âš¡âš¡ â”‚    âš¡       â”‚    âš¡âš¡âš¡
Drill-down            â”‚  âš¡âš¡âš¡ â”‚    âš¡       â”‚    âš¡âš¡
Cross-table joins     â”‚  âš¡âš¡âš¡ â”‚    ğŸŒ      â”‚    âš¡âš¡âš¡

Legend:
  âš¡âš¡âš¡ = Excellent (< 1 sec)
  âš¡âš¡  = Good (1-3 sec)
  âš¡   = OK (3-10 sec)
  ğŸŒ  = Slow (> 10 sec)
  âŒ  = Not supported
```

### Features

```
Feature                â”‚ Import â”‚ DirectQuery â”‚ Direct Lake
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Full DAX support       â”‚   âœ…   â”‚     âš ï¸      â”‚      âœ…
Calculated columns     â”‚   âœ…   â”‚     âŒ      â”‚      âœ…
Calculated tables      â”‚   âœ…   â”‚     âŒ      â”‚      âœ…
Power Query (M)        â”‚   âœ…   â”‚     âš ï¸      â”‚      âš ï¸
Offline access         â”‚   âœ…   â”‚     âŒ      â”‚      âš ï¸
Incremental refresh    â”‚   âœ…   â”‚     âŒ      â”‚      âœ…
Aggregations           â”‚   âœ…   â”‚     âš ï¸      â”‚      âœ…
Real-time data         â”‚   âŒ   â”‚     âœ…      â”‚      âœ…
Unlimited size         â”‚   âŒ   â”‚     âœ…      â”‚      âš ï¸
RLS (model-level)      â”‚   âœ…   â”‚     âœ…      â”‚      âœ…
```

### Cost & Resources

```
Resource Usage        â”‚ Import â”‚ DirectQuery â”‚ Direct Lake
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Memory (model)        â”‚  High  â”‚    Low      â”‚    Medium
CPU (refresh)         â”‚  High  â”‚    Low      â”‚    Low
Storage (duplication) â”‚  2x    â”‚    1x       â”‚    1x
Network bandwidth     â”‚  Low   â”‚    High     â”‚    Medium
Source DB load        â”‚  Low   â”‚    High     â”‚    Low
Capacity CU cost      â”‚  High  â”‚    Low      â”‚    Medium
```

## DÃ©cision Matrix

### Decision Tree

```
START: Choose Storage Mode
â”‚
â”œâ”€ Is data in OneLake Delta?
â”‚  â”œâ”€ Yes â†’ âœ… Direct Lake (best option)
â”‚  â””â”€ No â†’ Continue
â”‚
â”œâ”€ Need real-time data?
â”‚  â”œâ”€ Yes
â”‚  â”‚  â”œâ”€ Source has good performance?
â”‚  â”‚  â”‚  â”œâ”€ Yes â†’ DirectQuery
â”‚  â”‚  â”‚  â””â”€ No â†’ Improve source OR Import
â”‚  â”‚  â””â”€ Can data move to OneLake?
â”‚  â”‚     â”œâ”€ Yes â†’ Direct Lake
â”‚  â”‚     â””â”€ No â†’ DirectQuery
â”‚  â”‚
â”‚  â””â”€ No (daily/weekly refresh OK)
â”‚     â”œâ”€ Dataset < 1 GB?
â”‚     â”‚  â”œâ”€ Yes â†’ âœ… Import
â”‚     â”‚  â””â”€ No
â”‚     â”‚     â”œâ”€ Can move to OneLake?
â”‚     â”‚     â”‚  â”œâ”€ Yes â†’ âœ… Direct Lake
â”‚     â”‚     â”‚  â””â”€ No
â”‚     â”‚     â”‚     â”œâ”€ Source performant?
â”‚     â”‚     â”‚     â”‚  â”œâ”€ Yes â†’ DirectQuery
â”‚     â”‚     â”‚     â”‚  â””â”€ No â†’ Aggregations + Import
```

### By Scenario

```
Scenario: Executive Dashboard (updated daily)
  Data size: 500 MB
  Freshness: Daily OK
  Complexity: High DAX
  â†’ Recommendation: Import âœ…

Scenario: Real-time Sales Monitor
  Data size: 50 GB
  Freshness: Real-time required
  Complexity: Simple aggregations
  Source: SQL Server (optimized)
  â†’ Recommendation: DirectQuery âœ…

Scenario: Customer 360 Analytics
  Data size: 2 TB
  Freshness: Near real-time
  Complexity: Medium DAX
  Source: Fabric Lakehouse
  â†’ Recommendation: Direct Lake âœ… âœ…

Scenario: Financial Reporting
  Data size: 10 GB
  Freshness: Monthly
  Complexity: Very complex DAX
  Need offline: Yes
  â†’ Recommendation: Import âœ…
```

## Composite Models

### Mixing Modes

```
Composite Model = Multiple storage modes in one model

Example:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Semantic Model (Composite)        â”‚
â”‚  â”œâ”€ Fact_Sales (Direct Lake) ğŸ“Š    â”‚
â”‚  â”œâ”€ Dim_Customer (Import) ğŸ“¥       â”‚
â”‚  â”œâ”€ Dim_Product (Import) ğŸ“¥        â”‚
â”‚  â””â”€ Fact_Realtime (DirectQuery) ğŸ”— â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
  âœ… Optimize each table individually
  âœ… Import small dimensions (fast joins)
  âœ… Direct Lake/DQ large facts
  âœ… Best performance/freshness mix
```

### Aggregations

```
Aggregation Strategy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Detail Table (DirectQuery/DL)     â”‚
â”‚  â””â”€ Billions of rows               â”‚
â”‚                                    â”‚
â”‚  Aggregation Table (Import)        â”‚
â”‚  â””â”€ Pre-aggregated (thousands)     â”‚
â”‚                                    â”‚
â”‚  Automatic query routing:          â”‚
â”‚  â”œâ”€ Summary view â†’ Agg (fast âš¡)   â”‚
â”‚  â””â”€ Drill to detail â†’ Detail (OK)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example:
  Detail: Sales_DirectQuery (1B rows)
  Agg: Sales_Monthly_Import (120K rows)
  â†’ Dashboard uses Agg (instant)
  â†’ Drill-down uses Detail (acceptable)
```

## Migration Strategies

### Import â†’ Direct Lake

```
Step 1: Move data to OneLake
  â€¢ Create Lakehouse
  â€¢ Load data into Delta tables
  â€¢ Optimize with V-Order

Step 2: Create Direct Lake model
  â€¢ New semantic model
  â€¢ Source: Lakehouse
  â€¢ Select same tables

Step 3: Migrate measures
  â€¢ Copy DAX measures
  â€¢ Test compatibility
  â€¢ Adjust if needed

Step 4: Test & validate
  â€¢ Compare results
  â€¢ Test performance
  â€¢ Check fallback status

Step 5: Switch reports
  â€¢ Point reports to new model
  â€¢ Monitor performance
  â€¢ Decommission old Import model
```

### DirectQuery â†’ Direct Lake

```
Step 1: ETL to OneLake
  â€¢ Data pipeline: Source â†’ Lakehouse
  â€¢ Schedule: Match freshness needs
  â€¢ Delta table format

Step 2: Optimize Delta
  â€¢ OPTIMIZE tables
  â€¢ Z-Order on filter columns
  â€¢ Enable V-Order

Step 3: Create Direct Lake model
  â€¢ Same schema as DirectQuery
  â€¢ Test DAX compatibility

Step 4: Performance comparison
  â€¢ Direct Lake should be faster
  â€¢ Validate data freshness
  â€¢ Check query patterns

Step 5: Cutover
  â€¢ Update reports
  â€¢ Monitor
  â€¢ Optimize as needed
```

## Best Practices

### âœ… Import Mode

```
1. Incremental refresh for large tables
2. Remove unnecessary columns/rows
3. Optimize data types (int vs string)
4. Schedule refresh during off-peak hours
5. Monitor refresh duration
6. Set up failure alerts
```

### âœ… DirectQuery Mode

```
1. Optimize source database (indexes, views)
2. Limit data volume with filters
3. Use aggregations for performance
4. Test query performance before deployment
5. Monitor source load
6. Consider caching layer (Redis)
```

### âœ… Direct Lake Mode

```
1. Use V-Order on Delta tables
2. Optimize with ZORDER
3. Regular OPTIMIZE and VACUUM
4. Monitor capacity usage
5. Watch for fallback to DirectQuery
6. Partition large tables appropriately
```

## Points ClÃ©s

- Import: Fast, stale data, size limits
- DirectQuery: Fresh, slower, no limits
- Direct Lake: Fast + fresh (Fabric innovation)
- Choose based on: size, freshness, source, complexity
- Composite models mix modes for optimal balance
- Aggregations boost DirectQuery/Direct Lake performance
- Direct Lake is default choice for Fabric workloads
- Migration to Direct Lake recommended when possible
- Monitor fallback and capacity usage
- Test performance before production deployment

---

**Prochain fichier :** [04 - ModÃ©lisation Star Schema](./04-modelisation-star-schema.md)

[â¬…ï¸ Fichier prÃ©cÃ©dent](./02-direct-lake-mode.md) | [â¬…ï¸ Retour au README du module](./README.md)
