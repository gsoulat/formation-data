# Best Practices Dataflows

## Introduction

Les **best practices** pour Dataflows Gen2 couvrent la performance, la maintenabilit√©, la qualit√© des donn√©es, et la gouvernance.

```
Best Practices Categories:
‚îú‚îÄ‚îÄ Performance & Optimization
‚îú‚îÄ‚îÄ Query Design & Folding
‚îú‚îÄ‚îÄ Error Handling & Resilience
‚îú‚îÄ‚îÄ Documentation & Naming
‚îú‚îÄ‚îÄ Security & Governance
‚îî‚îÄ‚îÄ Testing & Validation
```

## Performance & Optimization

### ‚úÖ Query Folding

**Maximize Folding :**
```powerquery
// ‚úÖ GOOD: Operations fold to source
let
    Source = Sql.Database("server", "db"),
    sales = Source{[Schema="dbo",Item="sales"]}[Data],

    // These operations fold (pushed to SQL):
    Filtered = Table.SelectRows(sales, each
        [year] = 2024                    // WHERE year = 2024
        and [amount] > 100               // AND amount > 100
    ),
    Selected = Table.SelectColumns(Filtered,
        {"id", "customer_id", "amount"}), // SELECT id, customer_id, amount
    Sorted = Table.Sort(Selected,
        {{"amount", Order.Descending}})  // ORDER BY amount DESC
in
    Sorted

// Native query generated (efficient!):
-- SELECT id, customer_id, amount
-- FROM sales
-- WHERE year = 2024 AND amount > 100
-- ORDER BY amount DESC

// ‚ùå BAD: Breaks folding
let
    Source = Sql.Database("server", "db"),
    sales = Source{[Schema="dbo",Item="sales"]}[Data],

    // Custom function breaks folding
    WithUpper = Table.AddColumn(sales, "name_upper",
        each Text.Upper([customer_name])),  // ‚Üê Breaks folding

    // Now filter doesn't fold (all data loaded first!)
    Filtered = Table.SelectRows(WithUpper, each [year] = 2024)
in
    Filtered

// All 100M rows loaded, then filtered in Power Query!
```

**Check Folding :**
```
Right-click step ‚Üí View Native Query

‚úÖ Shows SQL ‚Üí Folding works
‚ùå Error message ‚Üí Folding broken
```

### ‚úÖ Filter Early, Select Columns Early

```powerquery
// ‚úÖ GOOD: Filter and select early
let
    Source = Sql.Database("server", "db"),
    sales = Source{[Schema="dbo",Item="sales"]}[Data],

    // 1. Filter first (reduce rows)
    Filtered = Table.SelectRows(sales, each [year] = 2024),
    // ‚Üí 100M rows ‚Üí 5M rows

    // 2. Select columns (reduce columns)
    Selected = Table.SelectColumns(Filtered, {"id", "amount", "date"}),
    // ‚Üí 50 columns ‚Üí 3 columns

    // 3. Now transform (on 5M rows x 3 columns)
    Transformed = Table.TransformColumns(Selected, ...)
in
    Transformed

// ‚ùå BAD: Transform all data first
let
    Source = Sql.Database("server", "db"),
    sales = Source{[Schema="dbo",Item="sales"]}[Data],

    // 1. Transform on 100M rows x 50 columns
    Transformed = Table.TransformColumns(sales, ...),

    // 2. Then filter (too late!)
    Filtered = Table.SelectRows(Transformed, each [year] = 2024)
in
    Filtered
```

### ‚úÖ Buffer Lookup Tables

```powerquery
// ‚úÖ GOOD: Buffer small lookup table
let
    Sales = /* 10M rows */,
    Products = /* 1000 rows */,

    // Load products in memory once
    BufferedProducts = Table.Buffer(Products),

    // Lookup from memory (fast!)
    Enriched = Table.AddColumn(Sales, "product_name",
        each
            let
                match = Table.SelectRows(BufferedProducts,
                    (p) => p[product_id] = [product_id])
            in
                if Table.RowCount(match) > 0 then
                    match{0}[product_name]
                else
                    "Unknown"
    )
in
    Enriched

// ‚ùå BAD: No buffering
let
    Sales = /* 10M rows */,
    Products = /* 1000 rows */,

    // Lookup scans Products table 10M times!
    Enriched = Table.AddColumn(Sales, "product_name",
        each
            let
                match = Table.SelectRows(Products,  // ‚Üê Re-scans every time
                    (p) => p[product_id] = [product_id])
            in
                ...
    )
in
    Enriched
```

### ‚úÖ Use Native Joins (Merge) When Possible

```powerquery
// ‚úÖ GOOD: Native merge (can fold)
let
    Sales = Sql.Database("server", "db"){[Item="sales"]}[Data],
    Customers = Sql.Database("server", "db"){[Item="customers"]}[Data],

    // Merge folds to SQL JOIN
    Merged = Table.NestedJoin(
        Sales, {"customer_id"},
        Customers, {"customer_id"},
        "customer",
        JoinKind.LeftOuter
    ),

    Expanded = Table.ExpandTableColumn(Merged, "customer",
        {"name", "country"})
in
    Expanded

// Native query:
-- SELECT s.*, c.name, c.country
-- FROM sales s
-- LEFT JOIN customers c ON s.customer_id = c.customer_id

// ‚ùå BAD: Manual lookup (doesn't fold)
WithCustomer = Table.AddColumn(Sales, "customer_name",
    each
        let
            match = Table.SelectRows(Customers,
                (c) => c[customer_id] = [customer_id])
        in
            match{0}[name]
)
```

## Query Design

### ‚úÖ Descriptive Naming

```powerquery
// ‚úÖ GOOD: Clear names
let
    Source_SQL_SalesDB = Sql.Database("sql-prod.company.com", "SalesDB"),
    RawSalesTable = Source_SQL_SalesDB{[Schema="dbo",Item="sales"]}[Data],
    FilterCurrentYear = Table.SelectRows(RawSalesTable, each [year] = 2024),
    ActiveCustomersOnly = Table.SelectRows(FilterCurrentYear, each [is_active] = true),
    SelectedColumns = Table.SelectColumns(ActiveCustomersOnly, {"id", "amount"}),
    FinalCleanedData = Table.RenameColumns(SelectedColumns, {{"id", "sale_id"}})
in
    FinalCleanedData

// ‚ùå BAD: Generic names
let
    Source = Sql.Database("server", "db"),
    Table1 = Source{[Item="sales"]}[Data],
    Step1 = Table.SelectRows(Table1, each [year] = 2024),
    Step2 = Table.SelectRows(Step1, each [is_active] = true),
    Result = Step2
in
    Result
```

### ‚úÖ One Responsibility Per Query

```powerquery
// ‚úÖ GOOD: Separate concerns

// Query 1: Source_Customers
let
    Source = Sql.Database("server", "db"),
    customers = Source{[Item="customers"]}[Data]
in
    customers

// Query 2: Cleaned_Customers (references Query 1)
let
    Source = Source_Customers,
    Filtered = Table.SelectRows(Source, each [is_active] = true),
    Cleaned = Table.TransformColumns(Filtered, ...)
in
    Cleaned

// Query 3: Final_Customers (references Query 2)
let
    Source = Cleaned_Customers,
    Enriched = Table.AddColumn(Source, ...),
    Final = Table.SelectColumns(Enriched, ...)
in
    Final

// ‚ùå BAD: Everything in one query (monolithic)
let
    Source = Sql.Database("server", "db"),
    customers = Source{[Item="customers"]}[Data],
    Filtered = Table.SelectRows(customers, each [is_active] = true),
    Cleaned = Table.TransformColumns(Filtered, ...),
    Enriched = Table.AddColumn(Cleaned, ...),
    Final = Table.SelectColumns(Enriched, ...)
in
    Final
```

### ‚úÖ Use Parameters for Flexibility

```powerquery
// Create parameters
Environment = "prod" meta [IsParameterQuery=true],
BatchSize = 1000 meta [IsParameterQuery=true],
StartDate = #date(2024, 1, 1) meta [IsParameterQuery=true]

// Use in queries
let
    Server = if Environment = "prod" then "sql-prod.com" else "sql-dev.com",
    Source = Sql.Database(Server, "SalesDB"),
    sales = Source{[Item="sales"]}[Data],

    Filtered = Table.SelectRows(sales, each
        [sale_date] >= StartDate
    ),

    Batched = Table.FirstN(Filtered, BatchSize)
in
    Batched
```

## Error Handling

### ‚úÖ Robust Error Handling

```powerquery
let
    // Safe source connection
    Source = try
        Sql.Database("server", "db")
    otherwise
        #table({"Error"}, {{"Cannot connect to database"}}),

    // Safe table access
    customers = try
        Source{[Schema="dbo",Item="customers"]}[Data]
    otherwise
        #table({"Error"}, {{"Table not found"}}),

    // Safe type conversions
    WithAmount = Table.TransformColumns(customers, {
        {"amount", each try Number.From(_) otherwise 0, type number}
    }),

    // Safe date parsing
    WithDate = Table.TransformColumns(WithAmount, {
        {"order_date", each
            try Date.From(_)
            otherwise #date(1900, 1, 1),
            type date}
    }),

    // Remove errors
    ValidRows = Table.SelectRows(WithDate, each
        [customer_id] <> null
        and [amount] <> null
    )
in
    ValidRows
```

### ‚úÖ Null Handling

```powerquery
// ‚úÖ GOOD: Explicit null handling
let
    Source = /* data */,

    // Replace nulls
    WithDefaults = Table.ReplaceValue(
        Source,
        null,
        "Unknown",
        Replacer.ReplaceValue,
        {"country"}
    ),

    // Coalesce columns
    WithFallback = Table.AddColumn(Source, "final_value",
        each [primary_value] ?? [secondary_value] ?? [default_value] ?? 0
    ),

    // Filter out nulls
    NoNulls = Table.SelectRows(Source, each
        [customer_id] <> null
        and [amount] <> null
    )
in
    NoNulls

// ‚ùå BAD: Assume no nulls
Calculated = Table.AddColumn(Source, "ratio",
    each [value1] / [value2]  // ‚Üê Crash si null!
)
```

## Data Quality

### ‚úÖ Validation Columns

```powerquery
let
    Source = /* raw data */,

    // Add validation flags
    WithValidation = Table.AddColumn(Source, "is_valid",
        each
            [customer_id] <> null
            and [email] <> null
            and Text.Contains([email], "@")
            and [amount] > 0
            and [sale_date] <= Date.From(DateTime.LocalNow())
    ),

    // Add error descriptions
    WithErrors = Table.AddColumn(WithValidation, "validation_errors",
        each
            if [customer_id] = null then "Missing customer ID"
            else if [email] = null then "Missing email"
            else if not Text.Contains([email], "@") then "Invalid email format"
            else if [amount] <= 0 then "Invalid amount"
            else if [sale_date] > Date.From(DateTime.LocalNow()) then "Future date"
            else "OK"
    ),

    // Split valid/invalid
    ValidData = Table.SelectRows(WithErrors, each [is_valid] = true),
    InvalidData = Table.SelectRows(WithErrors, each [is_valid] = false)
in
    ValidData  // Load valid to destination
    // InvalidData can be sent to error log
```

### ‚úÖ Deduplication Strategy

```powerquery
// ‚úÖ GOOD: Keep latest based on timestamp
let
    Source = /* data with duplicates */,

    // Sort by timestamp descending
    Sorted = Table.Sort(Source, {
        {"customer_id", Order.Ascending},
        {"updated_at", Order.Descending}
    }),

    // Keep first occurrence (latest due to sort)
    Deduplicated = Table.Distinct(Sorted, {"customer_id"}),

    // Remove sort column if temporary
    Final = Table.RemoveColumns(Deduplicated, {"updated_at"})
in
    Final

// ‚ùå BAD: Arbitrary deduplication
Deduplicated = Table.Distinct(Source, {"customer_id"})
// Which row kept? Unknown!
```

## Documentation

### ‚úÖ Comments and Metadata

```powerquery
let
    /*
     * Source: SQL Server Production Database
     * Purpose: Extract customer data for analytics
     * Owner: Data Team
     * Last modified: 2024-01-15
     */

    // Connection to production SQL Server
    Source = Sql.Database("sql-prod.company.com", "SalesDB"),

    // Get customers table
    customers_table = Source{[Schema="dbo",Item="customers"]}[Data],

    /*
     * Filter to active customers only to reduce data volume.
     * Active = customer with activity in last 12 months.
     */
    ActiveCustomers = Table.SelectRows(customers_table, each
        [is_active] = true
        and [last_activity_date] >= Date.AddMonths(Date.From(DateTime.LocalNow()), -12)
    ),

    // Transform columns to standard format
    StandardizedData = Table.TransformColumns(ActiveCustomers, {
        {"first_name", Text.Proper, type text},
        {"last_name", Text.Proper, type text},
        {"email", Text.Lower, type text}
    }),

    /*
     * Add calculated age column based on birth_date.
     * Age = (Today - birth_date) / 365.25 to account for leap years.
     */
    WithAge = Table.AddColumn(StandardizedData, "age",
        each Duration.Days(DateTime.LocalNow() - DateTime.From([birth_date])) / 365.25,
        Int64.Type
    )
in
    WithAge
```

### ‚úÖ Query Organization

```
Query Groups (folders):
‚îú‚îÄ‚îÄ üìÅ Parameters
‚îÇ   ‚îú‚îÄ‚îÄ Environment
‚îÇ   ‚îú‚îÄ‚îÄ RangeStart
‚îÇ   ‚îî‚îÄ‚îÄ RangeEnd
‚îú‚îÄ‚îÄ üìÅ Sources
‚îÇ   ‚îú‚îÄ‚îÄ Source_SQL_Customers
‚îÇ   ‚îú‚îÄ‚îÄ Source_SQL_Orders
‚îÇ   ‚îî‚îÄ‚îÄ Source_API_Products
‚îú‚îÄ‚îÄ üìÅ Transformations
‚îÇ   ‚îú‚îÄ‚îÄ Cleaned_Customers
‚îÇ   ‚îú‚îÄ‚îÄ Enriched_Orders
‚îÇ   ‚îî‚îÄ‚îÄ Joined_Customer_Orders
‚îú‚îÄ‚îÄ üìÅ Functions
‚îÇ   ‚îú‚îÄ‚îÄ fn_CalculateTax
‚îÇ   ‚îî‚îÄ‚îÄ fn_CleanPhone
‚îî‚îÄ‚îÄ üìÅ Outputs (Destinations)
    ‚îú‚îÄ‚îÄ Final_Customers
    ‚îî‚îÄ‚îÄ Final_Orders
```

## Security & Governance

### ‚úÖ Credentials Management

```
‚ùå DON'T hardcode credentials:
  Source = Sql.Database("server", "db", [User="admin", Password="pass123"])

‚úÖ DO use stored credentials:
  Source = Sql.Database("server", "db")
  // Credentials stored securely in workspace

‚úÖ DO use service principals:
  // Configured at workspace level
  // Automatic authentication
```

### ‚úÖ Row-Level Security

```powerquery
// Apply RLS in dataflow (if source doesn't support)
let
    Source = /* all data */,

    // Get current user
    CurrentUser = "user@company.com",  // Or from parameter

    // Filter based on user
    FilteredByUser = Table.SelectRows(Source, each
        [sales_rep_email] = CurrentUser
        or List.Contains({"admin@company.com", "manager@company.com"}, CurrentUser)
    )
in
    FilteredByUser

// Better: Use source RLS if available (query folding)
```

### ‚úÖ PII Handling

```powerquery
// Mask sensitive data
let
    Source = /* customer data with PII */,

    // Hash email
    MaskedEmail = Table.TransformColumns(Source, {
        {"email", each Text.Start(_, 3) & "***@" & Text.AfterDelimiter(_, "@"), type text}
    }),

    // Mask phone
    MaskedPhone = Table.TransformColumns(MaskedEmail, {
        {"phone", each "***-***-" & Text.End(_, 4), type text}
    }),

    // Remove SSN column entirely
    Final = Table.RemoveColumns(MaskedPhone, {"ssn"})
in
    Final
```

## Testing & Validation

### ‚úÖ Test Queries

```powerquery
// Create test query with sample data
TestData = #table(
    {"customer_id", "name", "email", "amount"},
    {
        {1, "John Doe", "john@example.com", 100},
        {2, "Jane Smith", null, 200},  // Test null handling
        {3, "Bob Invalid", "not-an-email", -50},  // Test validation
        {1, "John Duplicate", "john2@example.com", 150}  // Test deduplication
    }
)

// Apply transformations
TestResult = /* your transformation logic on TestData */

// Verify:
// - Null handling works
// - Validation catches invalid email
// - Deduplication removes duplicate customer_id
```

### ‚úÖ Assertions

```powerquery
let
    Source = /* data */,
    Transformed = /* transformations */,

    // Assertions
    RowCount = Table.RowCount(Transformed),
    AssertNotEmpty = if RowCount = 0 then
        error "No rows returned! Check filters."
        else Transformed,

    // Check required columns exist
    ColumnNames = Table.ColumnNames(AssertNotEmpty),
    RequiredColumns = {"customer_id", "name", "email"},
    AssertColumns = if List.ContainsAll(ColumnNames, RequiredColumns) then
        AssertNotEmpty
        else error "Missing required columns",

    // Check no nulls in key column
    NullCount = Table.RowCount(Table.SelectRows(AssertColumns,
        each [customer_id] = null)),
    AssertNoNulls = if NullCount > 0 then
        error "Found " & Text.From(NullCount) & " null customer IDs"
        else AssertColumns
in
    AssertNoNulls
```

## Performance Monitoring

### ‚úÖ Query Diagnostics

```
Tools ‚Üí Query Diagnostics ‚Üí Start Diagnostics
... perform operations ...
Tools ‚Üí Query Diagnostics ‚Üí Stop Diagnostics

Review:
  ‚îú‚îÄ Which steps took longest?
  ‚îú‚îÄ Which steps folded to source?
  ‚îú‚îÄ Memory usage per step
  ‚îî‚îÄ Data source queries generated

Optimize based on findings.
```

### ‚úÖ Profiling

```
View ‚Üí Column quality
View ‚Üí Column distribution
View ‚Üí Column profile

Check:
  ‚Ä¢ Errors per column
  ‚Ä¢ Null count
  ‚Ä¢ Distinct values (cardinality)
  ‚Ä¢ Min/Max/Average

Optimize:
  ‚Ä¢ Remove error-prone columns
  ‚Ä¢ Handle nulls early
  ‚Ä¢ Use high-cardinality columns for joins
```

## Deployment

### ‚úÖ Environment Strategy

```
Development Dataflow:
  ‚Ä¢ Parameters: Environment = "dev"
  ‚Ä¢ Source: dev databases
  ‚Ä¢ Destination: dev lakehouse
  ‚Ä¢ Refresh: Manual

Production Dataflow:
  ‚Ä¢ Parameters: Environment = "prod"
  ‚Ä¢ Source: prod databases
  ‚Ä¢ Destination: prod lakehouse
  ‚Ä¢ Refresh: Scheduled (daily 8 AM)

Use parameters to switch between environments!
```

### ‚úÖ Change Management

```
1. Version control (if using Git integration)
2. Test in DEV workspace first
3. Document changes in comments
4. Deploy to PROD during maintenance window
5. Monitor first refresh closely
6. Validate row counts match expectations
```

## Checklist

### Before Publishing

```
‚úÖ Query folding verified (View Native Query)
‚úÖ No hardcoded credentials
‚úÖ Parameters used for environments
‚úÖ Error handling on all risky operations
‚úÖ Null handling explicit
‚úÖ Comments added to complex logic
‚úÖ Query names descriptive
‚úÖ Test with sample data
‚úÖ Destination configured correctly
‚úÖ Refresh schedule set (if needed)
‚úÖ Failure alerts configured
‚úÖ Documentation updated
```

### After Publishing

```
‚úÖ First refresh successful
‚úÖ Row counts validated
‚úÖ Data quality checks pass
‚úÖ Downstream consumers notified
‚úÖ Performance monitored
‚úÖ Error logs reviewed
‚úÖ Incremental refresh working (if enabled)
```

## Points Cl√©s

- Query folding = priorit√© #1 pour performance
- Filter early, select columns early
- Buffer lookup tables
- Descriptive naming pour maintenabilit√©
- Robust error handling (try-otherwise)
- Explicit null handling
- Data validation columns
- Comments et documentation
- Parameters pour flexibility
- Test avant d√©ploiement
- Monitor performance avec diagnostics
- Security: pas de credentials hardcod√©s
- Environment-specific parameters

---

**Module 05 COMPLET** ‚úÖ

[‚¨ÖÔ∏è Fichier pr√©c√©dent](./05-incremental-refresh.md) | [‚û°Ô∏è Module suivant : Notebooks & Spark](../../06-Notebooks-Spark/) | [‚¨ÖÔ∏è Retour au README du module](./README.md)
