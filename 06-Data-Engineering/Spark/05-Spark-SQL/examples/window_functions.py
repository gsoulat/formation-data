"""
Window Functions - Calculs sur fenÃªtres de lignes

Ce script montre :
- Ranking functions (ROW_NUMBER, RANK, DENSE_RANK)
- Analytical functions (LEAD, LAG, FIRST_VALUE, LAST_VALUE)
- Aggregate window functions (SUM, AVG avec OVER)
- Window frame specifications
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.window import Window

# CrÃ©er SparkSession
spark = SparkSession.builder \
    .appName("Window Functions") \
    .master("local[*]") \
    .getOrCreate()

print("=" * 80)
print("WINDOW FUNCTIONS")
print("=" * 80)

# ========================================
# DONNÃ‰ES DE TEST
# ========================================

# EmployÃ©s avec salaires
employees = spark.createDataFrame([
    ("Alice", "Sales", 50000),
    ("Bob", "Sales", 60000),
    ("Charlie", "Engineering", 80000),
    ("David", "Engineering", 70000),
    ("Eve", "Sales", 55000),
    ("Frank", "Engineering", 75000),
    ("Grace", "HR", 45000),
    ("Henry", "HR", 48000)
], ["name", "department", "salary"])

employees.createOrReplaceTempView("employees")

# Ventes quotidiennes
daily_sales = spark.createDataFrame([
    ("2024-01-01", 1000),
    ("2024-01-02", 1500),
    ("2024-01-03", 1200),
    ("2024-01-04", 1800),
    ("2024-01-05", 1400),
    ("2024-01-06", 1600),
    ("2024-01-07", 2000)
], ["date", "sales"])

daily_sales.createOrReplaceTempView("daily_sales")

print("\nðŸ“‹ DonnÃ©es employÃ©s:")
employees.show()

print("\nðŸ“‹ Ventes quotidiennes:")
daily_sales.show()

# ========================================
# 1. RANKING FUNCTIONS
# ========================================
print("\n" + "=" * 80)
print("1. RANKING FUNCTIONS")
print("=" * 80)

# ROW_NUMBER
print("\nâœ… ROW_NUMBER - NumÃ©ro sÃ©quentiel par dÃ©partement:")
spark.sql("""
    SELECT
        name,
        department,
        salary,
        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as row_num
    FROM employees
    ORDER BY department, row_num
""").show()

# RANK (avec Ã©galitÃ©s, saute des numÃ©ros)
print("\nâœ… RANK - Avec Ã©galitÃ©s (saute):")
spark.sql("""
    SELECT
        name,
        department,
        salary,
        RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank
    FROM employees
    ORDER BY department, rank
""").show()

# DENSE_RANK (avec Ã©galitÃ©s, ne saute pas)
print("\nâœ… DENSE_RANK - Avec Ã©galitÃ©s (ne saute pas):")
spark.sql("""
    SELECT
        name,
        department,
        salary,
        DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank
    FROM employees
    ORDER BY department, dense_rank
""").show()

# Comparaison des 3 rankings
print("\nâœ… Comparaison ROW_NUMBER vs RANK vs DENSE_RANK:")
spark.sql("""
    SELECT
        name,
        department,
        salary,
        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as row_num,
        RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank,
        DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank
    FROM employees
    ORDER BY department, salary DESC
""").show()

# NTILE - Diviser en N groupes
print("\nâœ… NTILE - Diviser en quartiles (4 groupes):")
spark.sql("""
    SELECT
        name,
        salary,
        NTILE(4) OVER (ORDER BY salary DESC) as quartile
    FROM employees
    ORDER BY quartile, salary DESC
""").show()

# ========================================
# 2. TOP N PAR GROUPE
# ========================================
print("\n" + "=" * 80)
print("2. TOP N PAR GROUPE")
print("=" * 80)

# Top 2 salaires par dÃ©partement
print("\nâœ… Top 2 salaires par dÃ©partement:")
spark.sql("""
    SELECT * FROM (
        SELECT
            name,
            department,
            salary,
            ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank
        FROM employees
    ) ranked
    WHERE rank <= 2
    ORDER BY department, rank
""").show()

# ========================================
# 3. ANALYTICAL FUNCTIONS (LEAD, LAG)
# ========================================
print("\n" + "=" * 80)
print("3. ANALYTICAL FUNCTIONS")
print("=" * 80)

# LEAD - Valeur suivante
print("\nâœ… LEAD - Ventes du jour suivant:")
spark.sql("""
    SELECT
        date,
        sales,
        LEAD(sales, 1) OVER (ORDER BY date) as next_day_sales,
        LEAD(sales, 1) OVER (ORDER BY date) - sales as diff_next_day
    FROM daily_sales
""").show()

# LAG - Valeur prÃ©cÃ©dente
print("\nâœ… LAG - Ventes du jour prÃ©cÃ©dent:")
spark.sql("""
    SELECT
        date,
        sales,
        LAG(sales, 1) OVER (ORDER BY date) as prev_day_sales,
        sales - LAG(sales, 1) OVER (ORDER BY date) as diff_prev_day
    FROM daily_sales
""").show()

# Comparer avec jour prÃ©cÃ©dent ET suivant
print("\nâœ… Comparer avec jour prÃ©cÃ©dent et suivant:")
spark.sql("""
    SELECT
        date,
        sales,
        LAG(sales) OVER (ORDER BY date) as prev_day,
        LEAD(sales) OVER (ORDER BY date) as next_day,
        sales - LAG(sales) OVER (ORDER BY date) as growth
    FROM daily_sales
""").show()

# ========================================
# 4. FIRST_VALUE et LAST_VALUE
# ========================================
print("\n" + "=" * 80)
print("4. FIRST_VALUE et LAST_VALUE")
print("=" * 80)

print("\nâœ… Comparer avec le salaire le plus haut/bas du dÃ©partement:")
spark.sql("""
    SELECT
        name,
        department,
        salary,
        FIRST_VALUE(salary) OVER (
            PARTITION BY department
            ORDER BY salary DESC
        ) as highest_in_dept,
        LAST_VALUE(salary) OVER (
            PARTITION BY department
            ORDER BY salary DESC
            ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
        ) as lowest_in_dept
    FROM employees
    ORDER BY department, salary DESC
""").show()

# ========================================
# 5. AGGREGATE WINDOW FUNCTIONS
# ========================================
print("\n" + "=" * 80)
print("5. AGGREGATE WINDOW FUNCTIONS")
print("=" * 80)

# Somme cumulative
print("\nâœ… Somme cumulative des ventes:")
spark.sql("""
    SELECT
        date,
        sales,
        SUM(sales) OVER (
            ORDER BY date
            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
        ) as cumulative_sales
    FROM daily_sales
""").show()

# Moyenne mobile (3 jours)
print("\nâœ… Moyenne mobile sur 3 jours:")
spark.sql("""
    SELECT
        date,
        sales,
        AVG(sales) OVER (
            ORDER BY date
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) as moving_avg_3days
    FROM daily_sales
""").show()

# Comparer avec la moyenne du dÃ©partement
print("\nâœ… Comparer salaire avec moyenne du dÃ©partement:")
spark.sql("""
    SELECT
        name,
        department,
        salary,
        AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,
        salary - AVG(salary) OVER (PARTITION BY department) as diff_from_avg
    FROM employees
    ORDER BY department, salary DESC
""").show()

# ========================================
# 6. WINDOW FRAME SPECIFICATIONS
# ========================================
print("\n" + "=" * 80)
print("6. WINDOW FRAME SPECIFICATIONS")
print("=" * 80)

print("\nâœ… DiffÃ©rentes fenÃªtres:")
spark.sql("""
    SELECT
        date,
        sales,
        -- Somme depuis le dÃ©but jusqu'Ã  maintenant
        SUM(sales) OVER (
            ORDER BY date
            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
        ) as cumsum_start_to_current,

        -- Somme des 2 jours prÃ©cÃ©dents + aujourd'hui
        SUM(sales) OVER (
            ORDER BY date
            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
        ) as sum_3days,

        -- Somme de 1 avant Ã  1 aprÃ¨s (fenÃªtre de 3)
        SUM(sales) OVER (
            ORDER BY date
            ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
        ) as sum_window_3
    FROM daily_sales
""").show()

# ========================================
# 7. EXEMPLE COMPLET: ANALYSE RH
# ========================================
print("\n" + "=" * 80)
print("7. EXEMPLE COMPLET: ANALYSE RH")
print("=" * 80)

print("\nâœ… Analyse complÃ¨te des salaires:")
result = spark.sql("""
    SELECT
        name,
        department,
        salary,

        -- Ranking dans le dÃ©partement
        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank,

        -- Salaire moyen du dÃ©partement
        ROUND(AVG(salary) OVER (PARTITION BY department), 2) as dept_avg_salary,

        -- DiffÃ©rence avec la moyenne
        ROUND(salary - AVG(salary) OVER (PARTITION BY department), 2) as diff_from_avg,

        -- Pourcentage vs moyenne
        ROUND((salary / AVG(salary) OVER (PARTITION BY department) - 1) * 100, 2) as pct_vs_avg,

        -- Salaire le plus haut du dÃ©partement
        FIRST_VALUE(salary) OVER (
            PARTITION BY department
            ORDER BY salary DESC
        ) as highest_in_dept,

        -- Quartile
        NTILE(4) OVER (ORDER BY salary DESC) as salary_quartile

    FROM employees
    ORDER BY department, salary DESC
""")

result.show(truncate=False)

# ========================================
# 8. UTILISER WINDOW FUNCTIONS EN DATAFRAME API
# ========================================
print("\n" + "=" * 80)
print("8. WINDOW FUNCTIONS EN DATAFRAME API")
print("=" * 80)

from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, avg, sum as spark_sum
from pyspark.sql.window import Window

# DÃ©finir une window spec
window_spec = Window.partitionBy("department").orderBy(col("salary").desc())

# Appliquer des window functions
print("\nâœ… Window functions avec DataFrame API:")
result_df = employees \
    .withColumn("row_num", row_number().over(window_spec)) \
    .withColumn("rank", rank().over(window_spec)) \
    .withColumn("dept_avg_salary", avg("salary").over(Window.partitionBy("department")))

result_df.orderBy("department", "salary").show()

# ========================================
# 9. RUNNING TOTALS ET PERCENTAGES
# ========================================
print("\n" + "=" * 80)
print("9. RUNNING TOTALS ET PERCENTAGES")
print("=" * 80)

print("\nâœ… Pourcentage du total et cumul:")
spark.sql("""
    SELECT
        date,
        sales,

        -- Pourcentage du total
        ROUND(sales * 100.0 / SUM(sales) OVER (), 2) as pct_of_total,

        -- Somme cumulative
        SUM(sales) OVER (ORDER BY date) as cumulative_sales,

        -- Pourcentage cumulatif
        ROUND(
            SUM(sales) OVER (ORDER BY date) * 100.0 / SUM(sales) OVER (),
            2
        ) as cumulative_pct

    FROM daily_sales
""").show()

# ========================================
# RÃ‰CAPITULATIF
# ========================================
print("\n" + "=" * 80)
print("RÃ‰CAPITULATIF WINDOW FUNCTIONS")
print("=" * 80)

print("""
Ranking Functions:
  ROW_NUMBER()      - NumÃ©ro sÃ©quentiel (pas d'Ã©galitÃ©s)
  RANK()            - Ranking avec Ã©galitÃ©s (saute des numÃ©ros)
  DENSE_RANK()      - Ranking avec Ã©galitÃ©s (ne saute pas)
  NTILE(n)          - Divise en n groupes Ã©gaux

Analytical Functions:
  LEAD(col, n)      - Valeur n lignes aprÃ¨s
  LAG(col, n)       - Valeur n lignes avant
  FIRST_VALUE(col)  - PremiÃ¨re valeur de la fenÃªtre
  LAST_VALUE(col)   - DerniÃ¨re valeur de la fenÃªtre

Aggregate Functions:
  SUM() OVER (...)  - Somme sur fenÃªtre
  AVG() OVER (...)  - Moyenne sur fenÃªtre
  MIN/MAX/COUNT     - Autres agrÃ©gations

Window Specification:
  PARTITION BY      - Diviser en groupes
  ORDER BY          - Trier dans chaque groupe
  ROWS BETWEEN      - DÃ©finir la fenÃªtre (frame)

Frame Specifications:
  UNBOUNDED PRECEDING       - Depuis le dÃ©but
  CURRENT ROW               - Ligne actuelle
  UNBOUNDED FOLLOWING       - Jusqu'Ã  la fin
  N PRECEDING/FOLLOWING     - N lignes avant/aprÃ¨s
""")

# ArrÃªter SparkSession
spark.stop()
print("\nâœ… SparkSession arrÃªtÃ©e")
