"""
Pipeline ETL complet - Exemple e-commerce

Ce script montre un pipeline ETL complet :
1. Extract : Lire depuis CSV, JSON, Parquet
2. Transform : Nettoyer, valider, enrichir, agrÃ©ger
3. Load : Ã‰crire en Parquet partitionnÃ©
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os

# CrÃ©er SparkSession
spark = SparkSession.builder \
    .appName("Complete ETL Pipeline") \
    .master("local[*]") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

print("=" * 80)
print("PIPELINE ETL COMPLET - E-COMMERCE")
print("=" * 80)

# ========================================
# PRÃ‰PARER LES DONNÃ‰ES DE TEST
# ========================================
print("\nğŸ“¦ PrÃ©paration des donnÃ©es de test...")

# CrÃ©er des dossiers
os.makedirs("data", exist_ok=True)
os.makedirs("output", exist_ok=True)

# CrÃ©er des donnÃ©es de commandes (CSV)
orders_data = spark.createDataFrame([
    (1, 101, 1001, 2, 50.0, "2024-01-15"),
    (2, 102, 1002, 1, 100.0, "2024-01-15"),
    (3, 101, 1003, 3, 25.0, "2024-01-16"),
    (4, 103, 1001, 1, 50.0, "2024-01-16"),
    (5, 102, 1004, 2, 75.0, "2024-01-17"),
    (6, None, 1001, 1, 50.0, "2024-01-17"),  # customer_id NULL (invalide)
    (7, 104, None, 1, 100.0, "2024-01-18"),  # product_id NULL (invalide)
], ["order_id", "customer_id", "product_id", "quantity", "price", "order_date"])

orders_data.write.mode("overwrite").option("header", "true").csv("data/orders.csv")

# CrÃ©er des donnÃ©es clients (JSON)
customers_data = spark.createDataFrame([
    (101, "Alice", "alice@example.com", "Paris"),
    (102, "Bob", "bob@example.com", "Lyon"),
    (103, "Charlie", "charlie@example.com", "Marseille"),
    (104, "David", "david@example.com", "Paris"),
], ["customer_id", "name", "email", "city"])

customers_data.write.mode("overwrite").json("data/customers.json")

# CrÃ©er des donnÃ©es produits (Parquet)
products_data = spark.createDataFrame([
    (1001, "Laptop", "Electronics", 50.0),
    (1002, "Python Book", "Books", 100.0),
    (1003, "USB Cable", "Electronics", 25.0),
    (1004, "Notebook", "Office", 75.0),
], ["product_id", "product_name", "category", "base_price"])

products_data.write.mode("overwrite").parquet("data/products.parquet")

print("âœ… DonnÃ©es de test crÃ©Ã©es")

# ========================================
# 1. EXTRACT - LECTURE DES DONNÃ‰ES
# ========================================
print("\n" + "=" * 80)
print("1. EXTRACT - Lecture des donnÃ©es sources")
print("=" * 80)

# DÃ©finir le schema pour orders
orders_schema = StructType([
    StructField("order_id", IntegerType(), False),
    StructField("customer_id", IntegerType(), True),
    StructField("product_id", IntegerType(), True),
    StructField("quantity", IntegerType(), True),
    StructField("price", DoubleType(), True),
    StructField("order_date", StringType(), True)
])

# Lire orders (CSV avec schema explicite)
print("\nğŸ“„ Lecture orders.csv...")
orders = spark.read.csv("data/orders.csv", schema=orders_schema, header=True)
print(f"  Lignes lues: {orders.count()}")
orders.show()

# Lire customers (JSON)
print("\nğŸ“„ Lecture customers.json...")
customers = spark.read.json("data/customers.json")
print(f"  Lignes lues: {customers.count()}")
customers.show()

# Lire products (Parquet)
print("\nğŸ“„ Lecture products.parquet...")
products = spark.read.parquet("data/products.parquet")
print(f"  Lignes lues: {products.count()}")
products.show()

# ========================================
# 2. TRANSFORM - TRANSFORMATION
# ========================================
print("\n" + "=" * 80)
print("2. TRANSFORM - Nettoyage et enrichissement")
print("=" * 80)

# 2.1 Nettoyage des donnÃ©es
print("\nğŸ§¹ Ã‰tape 2.1 : Nettoyage des donnÃ©es")

# Supprimer les lignes avec nulls critiques
orders_valid = orders.dropna(subset=["order_id", "customer_id", "product_id"])
print(f"  Lignes avant nettoyage: {orders.count()}")
print(f"  Lignes aprÃ¨s nettoyage: {orders_valid.count()}")
print(f"  Lignes supprimÃ©es: {orders.count() - orders_valid.count()}")

# Valider les valeurs
orders_clean = orders_valid.filter(
    (col("quantity") > 0) &
    (col("price") > 0)
)

# Convertir la date
orders_clean = orders_clean.withColumn("order_date", to_date(col("order_date")))

# Calculer le montant total
orders_clean = orders_clean.withColumn("total_amount", col("quantity") * col("price"))

print("\nâœ… Orders aprÃ¨s nettoyage:")
orders_clean.show()

# 2.2 Enrichissement avec les clients
print("\nğŸ”— Ã‰tape 2.2 : Enrichissement avec clients")

orders_with_customers = orders_clean.join(
    customers,
    "customer_id",
    "left"
).select(
    orders_clean["*"],
    customers["name"].alias("customer_name"),
    customers["email"].alias("customer_email"),
    customers["city"].alias("customer_city")
)

print("âœ… Orders enrichi avec customers:")
orders_with_customers.show()

# 2.3 Enrichissement avec les produits
print("\nğŸ”— Ã‰tape 2.3 : Enrichissement avec produits")

orders_enriched = orders_with_customers.join(
    products,
    "product_id",
    "left"
).select(
    orders_with_customers["*"],
    products["product_name"],
    products["category"]
)

print("âœ… Orders enrichi avec products:")
orders_enriched.show(truncate=False)

# 2.4 Ajouter des mÃ©tadonnÃ©es
print("\nğŸ“ Ã‰tape 2.4 : Ajout de mÃ©tadonnÃ©es")

orders_final = orders_enriched \
    .withColumn("processed_at", current_timestamp()) \
    .withColumn("year", year(col("order_date"))) \
    .withColumn("month", month(col("order_date"))) \
    .withColumn("day", dayofmonth(col("order_date")))

print("âœ… Orders final avec mÃ©tadonnÃ©es:")
orders_final.select("order_id", "product_name", "total_amount", "year", "month", "processed_at").show()

# 2.5 CrÃ©er des agrÃ©gations
print("\nğŸ“Š Ã‰tape 2.5 : CrÃ©er des agrÃ©gations")

# AgrÃ©gation par jour et catÃ©gorie
daily_category_sales = orders_final.groupBy("order_date", "category").agg(
    count("*").alias("num_orders"),
    sum("total_amount").alias("total_revenue"),
    avg("total_amount").alias("avg_order_value"),
    sum("quantity").alias("total_quantity")
).orderBy("order_date", "category")

print("âœ… Ventes quotidiennes par catÃ©gorie:")
daily_category_sales.show()

# AgrÃ©gation par client
customer_stats = orders_final.groupBy("customer_id", "customer_name", "customer_city").agg(
    count("*").alias("num_orders"),
    sum("total_amount").alias("total_spent"),
    avg("total_amount").alias("avg_order_value")
).orderBy(col("total_spent").desc())

print("âœ… Statistiques par client:")
customer_stats.show()

# AgrÃ©gation par produit
product_stats = orders_final.groupBy("product_id", "product_name", "category").agg(
    count("*").alias("num_orders"),
    sum("quantity").alias("total_quantity_sold"),
    sum("total_amount").alias("total_revenue")
).orderBy(col("total_revenue").desc())

print("âœ… Statistiques par produit:")
product_stats.show()

# ========================================
# 3. LOAD - Ã‰CRITURE DES RÃ‰SULTATS
# ========================================
print("\n" + "=" * 80)
print("3. LOAD - Ã‰criture des rÃ©sultats")
print("=" * 80)

# 3.1 Sauvegarder orders enrichis (partitionnÃ© par date)
print("\nğŸ’¾ Ã‰tape 3.1 : Sauvegarde orders enrichis (partitionnÃ©)")

output_path_orders = "output/orders_enriched"
orders_final.write \
    .mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet(output_path_orders)

print(f"âœ… Orders enrichis sauvegardÃ©s dans: {output_path_orders}")

# VÃ©rifier les partitions crÃ©Ã©es
import subprocess
result = subprocess.run(["find", output_path_orders, "-name", "*.parquet"], capture_output=True, text=True)
print(f"  Fichiers crÃ©Ã©s: {len(result.stdout.strip().split())}")

# 3.2 Sauvegarder les agrÃ©gations
print("\nğŸ’¾ Ã‰tape 3.2 : Sauvegarde des agrÃ©gations")

# Daily sales
daily_category_sales.write \
    .mode("overwrite") \
    .parquet("output/daily_category_sales")
print("âœ… daily_category_sales sauvegardÃ©")

# Customer stats
customer_stats.write \
    .mode("overwrite") \
    .parquet("output/customer_stats")
print("âœ… customer_stats sauvegardÃ©")

# Product stats
product_stats.write \
    .mode("overwrite") \
    .parquet("output/product_stats")
print("âœ… product_stats sauvegardÃ©")

# 3.3 Sauvegarder une version CSV pour export
print("\nğŸ’¾ Ã‰tape 3.3 : Export CSV pour BI tools")

daily_category_sales.coalesce(1).write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("output/daily_sales_export.csv")

print("âœ… CSV export crÃ©Ã©")

# ========================================
# 4. VALIDATION ET RAPPORTS
# ========================================
print("\n" + "=" * 80)
print("4. VALIDATION ET RAPPORTS")
print("=" * 80)

# Relire les donnÃ©es sauvegardÃ©es pour validation
print("\nğŸ” Validation des donnÃ©es sauvegardÃ©es:")

orders_reloaded = spark.read.parquet(output_path_orders)
print(f"  Orders enrichis: {orders_reloaded.count()} lignes")

daily_reloaded = spark.read.parquet("output/daily_category_sales")
print(f"  Daily sales: {daily_reloaded.count()} lignes")

# Rapport final
print("\nğŸ“Š RAPPORT FINAL:")
print("-" * 50)
print(f"ğŸ“¥ DonnÃ©es source:")
print(f"  - Orders: {orders.count()} lignes")
print(f"  - Customers: {customers.count()} lignes")
print(f"  - Products: {products.count()} lignes")

print(f"\nğŸ§¹ AprÃ¨s nettoyage:")
print(f"  - Orders valides: {orders_clean.count()} lignes")
print(f"  - Lignes rejetÃ©es: {orders.count() - orders_clean.count()}")

print(f"\nğŸ“Š AgrÃ©gations:")
print(f"  - Daily sales: {daily_category_sales.count()} lignes")
print(f"  - Customers: {customer_stats.count()} lignes")
print(f"  - Products: {product_stats.count()} lignes")

print(f"\nğŸ’° Revenus totaux: ${orders_final.select(sum('total_amount')).collect()[0][0]:.2f}")

# ========================================
# RÃ‰CAPITULATIF
# ========================================
print("\n" + "=" * 80)
print("RÃ‰CAPITULATIF DU PIPELINE ETL")
print("=" * 80)

print("""
âœ… Pipeline ETL terminÃ© avec succÃ¨s !

Ã‰tapes exÃ©cutÃ©es:
  1. EXTRACT
     âœ“ Lecture de 3 sources (CSV, JSON, Parquet)
     âœ“ Schema explicite pour validation

  2. TRANSFORM
     âœ“ Nettoyage (suppression nulls, validation)
     âœ“ Enrichissement (2 joins)
     âœ“ Calculs (montants, mÃ©tadonnÃ©es)
     âœ“ AgrÃ©gations (daily, customer, product)

  3. LOAD
     âœ“ Parquet partitionnÃ© (year/month)
     âœ“ AgrÃ©gations en Parquet
     âœ“ Export CSV pour BI

  4. VALIDATION
     âœ“ Relecture et vÃ©rification
     âœ“ Rapport de statistiques

Outputs:
  ğŸ“ output/orders_enriched/          (partitionnÃ©)
  ğŸ“ output/daily_category_sales/
  ğŸ“ output/customer_stats/
  ğŸ“ output/product_stats/
  ğŸ“ output/daily_sales_export.csv/
""")

# ArrÃªter SparkSession
spark.stop()
print("\nâœ… SparkSession arrÃªtÃ©e")
