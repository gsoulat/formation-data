# Formation Apache Spark

Bienvenue dans le cours complet sur Apache Spark, le moteur de traitement de donnÃ©es distribuÃ© le plus populaire pour le Big Data.

## Vue d'ensemble

Apache Spark est un framework de traitement de donnÃ©es distribuÃ© open-source, conÃ§u pour la rapiditÃ©, la facilitÃ© d'utilisation et l'analyse sophistiquÃ©e. Ce cours couvre tous les aspects de Spark, de l'installation aux dÃ©ploiements en production.

## Qu'est-ce qu'Apache Spark ?

Spark est un moteur d'analyse unifiÃ© pour le traitement de donnÃ©es Ã  grande Ã©chelle, offrant :
- **Vitesse** : Jusqu'Ã  100x plus rapide que Hadoop MapReduce
- **SimplicitÃ©** : APIs en Python, Scala, Java, R, SQL
- **GÃ©nÃ©ralitÃ©** : Batch, streaming, SQL, ML, graph processing
- **Ã‰volutivitÃ©** : De votre laptop Ã  des clusters de milliers de machines

## Structure du cours

### [01-Introduction](./01-Introduction/README.md) ğŸ¯
**DÃ©couvrir Apache Spark**

- Qu'est-ce que Spark ?
- Architecture de Spark (Driver, Executors, Cluster Manager)
- Comparaison avec Hadoop MapReduce
- Cas d'usage et Ã©cosystÃ¨me
- Concepts clÃ©s (RDD, DataFrame, Dataset)

**DurÃ©e estimÃ©e :** 1 jour

### [02-Installation-Setup](./02-Installation-Setup/README.md) âš™ï¸
**Installer et configurer Spark**

- Installation locale (standalone)
- Docker et Docker Compose
- Configuration Spark
- Databricks Community Edition
- Jupyter Notebook avec PySpark

**DurÃ©e estimÃ©e :** 1 jour

### [03-RDD-Basics](./03-RDD-Basics/README.md) ğŸ“¦
**Resilient Distributed Datasets**

- CrÃ©ation de RDDs
- Transformations (map, filter, flatMap, etc.)
- Actions (collect, count, reduce, etc.)
- Lazy evaluation
- Partitionnement

**Exemples inclus :**
- `rdd_creation.py` - CrÃ©er des RDDs
- `rdd_transformations.py` - Transformations courantes
- `rdd_actions.py` - Actions et collect

**DurÃ©e estimÃ©e :** 2 jours

### [04-DataFrames-API](./04-DataFrames-API/README.md) ğŸ“Š
**DataFrame API - Abstraction haut niveau**

- CrÃ©ation de DataFrames
- OpÃ©rations sur colonnes
- Filtrage et sÃ©lection
- AgrÃ©gations et groupBy
- Joins
- Schema et types

**Exemples inclus :**
- `dataframe_basics.py` - OpÃ©rations de base
- `dataframe_operations.py` - Transformations avancÃ©es
- `spark_sql.py` - RequÃªtes SQL

**DurÃ©e estimÃ©e :** 3 jours

### [05-Spark-SQL](./05-Spark-SQL/README.md) ğŸ”
**SQL sur donnÃ©es distribuÃ©es**

- Spark SQL et Catalog
- Temporary Views et Tables
- Window Functions
- User Defined Functions (UDF)
- Optimisation de requÃªtes (Catalyst)

**Exemples inclus :**
- `sql_queries.py` - RequÃªtes SQL avancÃ©es
- `window_functions.py` - Fonctions de fenÃªtrage
- `udf_functions.py` - UDFs personnalisÃ©es

**DurÃ©e estimÃ©e :** 2 jours

### [06-ETL-Pipelines](./06-ETL-Pipelines/README.md) ğŸ”„
**Extract, Transform, Load avec Spark**

- Lecture de donnÃ©es (CSV, JSON, Parquet, Avro)
- Transformations de donnÃ©es
- Ã‰criture de donnÃ©es (formats et modes)
- Gestion de schÃ©mas
- Pipeline ETL complet

**Exemples inclus :**
- `read_csv_parquet.py` - Lecture de diffÃ©rents formats
- `data_transformation.py` - Nettoyage et transformation
- `write_formats.py` - Ã‰criture dans diffÃ©rents formats
- `complete_etl_pipeline.py` - Pipeline ETL de bout en bout

**DurÃ©e estimÃ©e :** 3 jours

### [07-Performance-Optimization](./07-Performance-Optimization/README.md) âš¡
**Optimiser les performances Spark**

- Partitionnement et repartitionnement
- Caching et persistence
- Broadcast variables
- Accumulateurs
- Tuning de la configuration Spark
- Ã‰viter les shuffles coÃ»teux

**Exemples inclus :**
- `partitioning.py` - StratÃ©gies de partitionnement
- `caching_persistence.py` - Cache et persistence
- `broadcast_joins.py` - Optimisation des joins

**DurÃ©e estimÃ©e :** 2 jours

### [08-Spark-Streaming](./08-Spark-Streaming/README.md) ğŸ“¡
**Traitement de donnÃ©es en temps rÃ©el**

- Structured Streaming
- Sources et Sinks
- Windowing et watermarks
- Stateful operations
- IntÃ©gration Kafka

**Exemples inclus :**
- `streaming_basics.py` - Premiers pas en streaming
- `kafka_integration.py` - Consommer depuis Kafka
- `windowing.py` - FenÃªtres temporelles

**DurÃ©e estimÃ©e :** 3 jours

### [09-Advanced-Topics](./09-Advanced-Topics/README.md) ğŸš€
**Sujets avancÃ©s**

- Spark MLlib (Machine Learning)
- Delta Lake (ACID transactions)
- GraphX (Graph processing)
- Arrow et Pandas UDF
- Performance monitoring

**Exemples inclus :**
- `spark_ml.py` - Machine Learning avec MLlib
- `delta_lake.py` - Utilisation de Delta Lake
- `graph_processing.py` - Traitement de graphes

**DurÃ©e estimÃ©e :** 3 jours

### [10-Production-Deployment](./10-Production-Deployment/README.md) ğŸ­
**DÃ©ployer Spark en production**

- Modes de dÃ©ploiement (Standalone, YARN, Kubernetes, Mesos)
- Databricks
- EMR (AWS) et Dataproc (GCP)
- Monitoring et logging
- Best practices production

**Configurations incluses :**
- `kubernetes/spark-on-k8s.yaml` - DÃ©ploiement Kubernetes
- `databricks/job-config.json` - Configuration Databricks jobs

**DurÃ©e estimÃ©e :** 2 jours

### [Projets](./Projets/)
**Projets pratiques de bout en bout**

- **01-Analyse-Logs** : Analyser des logs web avec Spark
- **02-ETL-E-commerce** : Pipeline ETL pour donnÃ©es e-commerce
- **03-Streaming-IoT** : Traitement temps rÃ©el de donnÃ©es IoT

**DurÃ©e estimÃ©e :** 1 semaine

---

## PrÃ©requis

### Connaissances requises

- **Python** : Niveau intermÃ©diaire (voir module Python-Basics)
- **SQL** : RequÃªtes de base
- **Linux** : Ligne de commande
- **Big Data** : Concepts de base (distribuÃ©, partitionnement)

### Connaissances recommandÃ©es

- **Scala** : Optionnel mais utile
- **Docker** : Pour l'environnement de dÃ©veloppement
- **Cloud** : AWS/GCP/Azure pour le dÃ©ploiement

### Logiciels Ã  installer

```bash
# Python 3.8+
python --version

# Java 8 ou 11
java -version

# Docker (recommandÃ©)
docker --version

# (Optionnel) Scala
scala -version
```

---

## Installation rapide

### Option 1 : Docker (RecommandÃ© pour dÃ©buter)

```bash
cd 02-Installation-Setup
docker-compose up -d
```

AccÃ©dez Ã  Jupyter : `http://localhost:8888`

### Option 2 : Installation locale

```bash
# Installer PySpark via pip
pip install pyspark

# Tester l'installation
python -c "import pyspark; print(pyspark.__version__)"
```

### Option 3 : Databricks Community Edition

1. S'inscrire sur [Databricks Community Edition](https://community.cloud.databricks.com/)
2. CrÃ©er un cluster
3. Importer les notebooks du cours

---

## Parcours d'apprentissage

### DÃ©butant (2 semaines)
1. 01-Introduction
2. 02-Installation-Setup
3. 04-DataFrames-API (focus sur DataFrame, pas RDD)
4. 05-Spark-SQL (bases)
5. 06-ETL-Pipelines (lecture/Ã©criture simple)

### IntermÃ©diaire (4 semaines)
1. Parcours DÃ©butant
2. 03-RDD-Basics
3. 05-Spark-SQL (complet)
4. 06-ETL-Pipelines (complet)
5. 07-Performance-Optimization
6. Projet 01 ou 02

### AvancÃ© (6 semaines)
1. Parcours IntermÃ©diaire
2. 08-Spark-Streaming
3. 09-Advanced-Topics
4. 10-Production-Deployment
5. Les 3 projets pratiques

---

## Concepts clÃ©s Ã  maÃ®triser

### 1. Abstractions Spark

```
RDD (Low-level)
  â†“
DataFrame (High-level, optimisÃ©)
  â†“
Dataset (Type-safe, Scala/Java)
```

**Recommandation** : Utilisez DataFrame API en prioritÃ©.

### 2. Architecture Spark

```
Driver Program
  â”œâ”€ SparkContext / SparkSession
  â””â”€ Cluster Manager (YARN, K8s, etc.)
      â”œâ”€ Executor 1
      â”‚   â”œâ”€ Task 1
      â”‚   â””â”€ Task 2
      â”œâ”€ Executor 2
      â”‚   â”œâ”€ Task 3
      â”‚   â””â”€ Task 4
      â””â”€ Executor N
```

### 3. Lazy Evaluation

```python
# Transformations (lazy)
df = spark.read.csv("data.csv")    # Pas d'exÃ©cution
df = df.filter(col("age") > 18)    # Pas d'exÃ©cution
df = df.select("name", "age")      # Pas d'exÃ©cution

# Action (dÃ©clenche l'exÃ©cution)
df.show()  # ExÃ©cution de toute la chaÃ®ne
```

### 4. Transformations vs Actions

**Transformations** (retournent un RDD/DataFrame) :
- `map`, `filter`, `select`, `groupBy`, `join`, etc.

**Actions** (retournent une valeur ou Ã©crivent des donnÃ©es) :
- `collect`, `count`, `show`, `write`, `reduce`, etc.

---

## Comparaison des technologies

| Technologie | Cas d'usage | Vitesse | ComplexitÃ© |
|-------------|-------------|---------|------------|
| **Hadoop MapReduce** | Batch lourd | â­ | â­â­â­ |
| **Apache Spark** | Batch + Streaming | â­â­â­â­â­ | â­â­ |
| **Apache Flink** | Streaming temps rÃ©el | â­â­â­â­â­ | â­â­â­ |
| **Pandas** | DonnÃ©es en mÃ©moire | â­â­â­ | â­ |
| **Dask** | Pandas distribuÃ© | â­â­â­ | â­â­ |

### Quand utiliser Spark ?

**âœ… Utilisez Spark pour :**
- Traiter des volumes de donnÃ©es > 10 GB
- ETL/ELT sur donnÃ©es distribuÃ©es
- Analyse de donnÃ©es Ã  grande Ã©chelle
- Machine Learning distribuÃ©
- Streaming avec Ã©tat (stateful)

**âŒ N'utilisez PAS Spark pour :**
- Petits datasets (< 1 GB) â†’ Utilisez Pandas
- Streaming ultra low-latency (< 100ms) â†’ Utilisez Flink
- OLTP (transactions) â†’ Utilisez une base de donnÃ©es
- Traitement simple de fichiers â†’ Utilisez des scripts

---

## Ã‰cosystÃ¨me Spark

```
Apache Spark
â”œâ”€â”€ Spark Core (RDD)
â”œâ”€â”€ Spark SQL (DataFrames, SQL)
â”œâ”€â”€ Spark Streaming (Structured Streaming)
â”œâ”€â”€ MLlib (Machine Learning)
â””â”€â”€ GraphX (Graph processing)

IntÃ©grations
â”œâ”€â”€ Sources de donnÃ©es
â”‚   â”œâ”€â”€ HDFS, S3, Azure Blob
â”‚   â”œâ”€â”€ Kafka, Kinesis
â”‚   â”œâ”€â”€ JDBC (MySQL, PostgreSQL)
â”‚   â””â”€â”€ NoSQL (Cassandra, MongoDB)
â”‚
â”œâ”€â”€ Formats
â”‚   â”œâ”€â”€ Parquet, ORC
â”‚   â”œâ”€â”€ Avro, JSON
â”‚   â””â”€â”€ Delta Lake, Iceberg
â”‚
â””â”€â”€ Clusters
    â”œâ”€â”€ YARN (Hadoop)
    â”œâ”€â”€ Kubernetes
    â”œâ”€â”€ Mesos
    â””â”€â”€ Databricks, EMR, Dataproc
```

---

## Ressources

### Documentation officielle
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Databricks Documentation](https://docs.databricks.com/)

### Livres recommandÃ©s
- **"Learning Spark"** (2nd Edition) - Jules S. Damji et al.
- **"Spark: The Definitive Guide"** - Bill Chambers & Matei Zaharia
- **"High Performance Spark"** - Holden Karau & Rachel Warren

### Cours en ligne
- [Databricks Academy](https://academy.databricks.com/)
- [Spark on Coursera](https://www.coursera.org/learn/scala-spark-big-data)
- [Udemy Spark Courses](https://www.udemy.com/topic/apache-spark/)

### CommunautÃ©s
- [Spark Users Mailing List](https://spark.apache.org/community.html)
- [Stack Overflow - apache-spark](https://stackoverflow.com/questions/tagged/apache-spark)
- [Reddit - r/apachespark](https://reddit.com/r/apachespark)

---

## Certifications

- **Databricks Certified Associate Developer for Apache Spark**
- **Databricks Certified Data Engineer Associate**
- **Cloudera Spark and Hadoop Developer**

---

## FAQ

**Q: Spark ou Pandas ?**
A: Pandas pour < 10 GB en mÃ©moire, Spark pour > 10 GB distribuÃ©.

**Q: PySpark ou Scala Spark ?**
A: PySpark pour la simplicitÃ© et l'Ã©cosystÃ¨me Python. Scala pour les meilleures performances.

**Q: Quelle version de Spark ?**
A: Utilisez la derniÃ¨re version stable (3.5+ en 2024).

**Q: Combien de mÃ©moire pour Spark ?**
A: Minimum 4 GB pour le dÃ©veloppement, 8+ GB recommandÃ©.

**Q: Spark est-il gratuit ?**
A: Oui, Spark est open-source (Apache 2.0). Databricks/EMR sont payants.

---

## Contribution

Pour amÃ©liorer ce cours :
1. Ouvrir une issue pour signaler des erreurs
2. Proposer des pull requests
3. Partager vos retours d'expÃ©rience

---

## Roadmap du cours

```
Semaine 1 : Fondamentaux
  â”œâ”€ Introduction Ã  Spark
  â”œâ”€ Installation
  â””â”€ DataFrames API

Semaine 2 : SQL et ETL
  â”œâ”€ Spark SQL
  â”œâ”€ ETL Pipelines
  â””â”€ Mini-projet ETL

Semaine 3 : Performance
  â”œâ”€ Optimisation
  â”œâ”€ Partitionnement
  â””â”€ Caching

Semaine 4 : Streaming
  â”œâ”€ Structured Streaming
  â”œâ”€ Kafka integration
  â””â”€ Projet streaming

Semaine 5-6 : Production
  â”œâ”€ Advanced topics
  â”œâ”€ DÃ©ploiement
  â””â”€ Projet final complet
```

---

**Bon apprentissage avec Apache Spark ! ğŸš€**

"Spark is the Swiss Army knife of Big Data" - Anonymous Data Engineer
