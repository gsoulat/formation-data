# Formation Apache Spark

Bienvenue dans le cours complet sur Apache Spark, le moteur de traitement de donn√©es distribu√© le plus populaire pour le Big Data.

## Vue d'ensemble

Apache Spark est un framework de traitement de donn√©es distribu√© open-source, con√ßu pour la rapidit√©, la facilit√© d'utilisation et l'analyse sophistiqu√©e. Ce cours couvre tous les aspects de Spark, de l'installation aux d√©ploiements en production.

## Qu'est-ce qu'Apache Spark ?

Spark est un moteur d'analyse unifi√© pour le traitement de donn√©es √† grande √©chelle, offrant :
- **Vitesse** : Jusqu'√† 100x plus rapide que Hadoop MapReduce
- **Simplicit√©** : APIs en Python, Scala, Java, R, SQL
- **G√©n√©ralit√©** : Batch, streaming, SQL, ML, graph processing
- **√âvolutivit√©** : De votre laptop √† des clusters de milliers de machines

## Structure du cours

### [01-Introduction](./01-Introduction/README.md) üéØ
**D√©couvrir Apache Spark**

- Qu'est-ce que Spark ?
- Architecture de Spark (Driver, Executors, Cluster Manager)
- Comparaison avec Hadoop MapReduce
- Cas d'usage et √©cosyst√®me
- Concepts cl√©s (RDD, DataFrame, Dataset)

**Dur√©e estim√©e :** 1 jour

### [02-Installation-Setup](./02-Installation-Setup/README.md) ‚öôÔ∏è
**Installer et configurer Spark**

- Installation locale (standalone)
- Docker et Docker Compose
- Configuration Spark
- Databricks Community Edition
- Jupyter Notebook avec PySpark

**Dur√©e estim√©e :** 1 jour

### [03-RDD-Basics](./03-RDD-Basics/README.md) üì¶
**Resilient Distributed Datasets**

- Cr√©ation de RDDs
- Transformations (map, filter, flatMap, etc.)
- Actions (collect, count, reduce, etc.)
- Lazy evaluation
- Partitionnement

**Exemples inclus :**
- `rdd_creation.py` - Cr√©er des RDDs
- `rdd_transformations.py` - Transformations courantes
- `rdd_actions.py` - Actions et collect

**Dur√©e estim√©e :** 2 jours

### [04-DataFrames-API](./04-DataFrames-API/README.md) üìä
**DataFrame API - Abstraction haut niveau**

- Cr√©ation de DataFrames
- Op√©rations sur colonnes
- Filtrage et s√©lection
- Agr√©gations et groupBy
- Joins
- Schema et types

**Exemples inclus :**
- `dataframe_basics.py` - Op√©rations de base
- `dataframe_operations.py` - Transformations avanc√©es
- `spark_sql.py` - Requ√™tes SQL

**Dur√©e estim√©e :** 3 jours

### [05-Spark-SQL](./05-Spark-SQL/README.md) üîç
**SQL sur donn√©es distribu√©es**

- Spark SQL et Catalog
- Temporary Views et Tables
- Window Functions
- User Defined Functions (UDF)
- Optimisation de requ√™tes (Catalyst)

**Exemples inclus :**
- `sql_queries.py` - Requ√™tes SQL avanc√©es
- `window_functions.py` - Fonctions de fen√™trage
- `udf_functions.py` - UDFs personnalis√©es

**Dur√©e estim√©e :** 2 jours

### [06-ETL-Pipelines](./06-ETL-Pipelines/README.md) üîÑ
**Extract, Transform, Load avec Spark**

- Lecture de donn√©es (CSV, JSON, Parquet, Avro)
- Transformations de donn√©es
- √âcriture de donn√©es (formats et modes)
- Gestion de sch√©mas
- Pipeline ETL complet

**Exemples inclus :**
- `read_csv_parquet.py` - Lecture de diff√©rents formats
- `data_transformation.py` - Nettoyage et transformation
- `write_formats.py` - √âcriture dans diff√©rents formats
- `complete_etl_pipeline.py` - Pipeline ETL de bout en bout

**Dur√©e estim√©e :** 3 jours

### [07-Performance-Optimization](./07-Performance-Optimization/README.md) ‚ö°
**Optimiser les performances Spark**

- Partitionnement et repartitionnement
- Caching et persistence
- Broadcast variables
- Accumulateurs
- Tuning de la configuration Spark
- √âviter les shuffles co√ªteux

**Exemples inclus :**
- `partitioning.py` - Strat√©gies de partitionnement
- `caching_persistence.py` - Cache et persistence
- `broadcast_joins.py` - Optimisation des joins

**Dur√©e estim√©e :** 2 jours

### [08-Spark-Streaming](./08-Spark-Streaming/README.md) üì°
**Traitement de donn√©es en temps r√©el**

- Structured Streaming
- Sources et Sinks
- Windowing et watermarks
- Stateful operations
- Int√©gration Kafka

**Exemples inclus :**
- `streaming_basics.py` - Premiers pas en streaming
- `kafka_integration.py` - Consommer depuis Kafka
- `windowing.py` - Fen√™tres temporelles

**Dur√©e estim√©e :** 3 jours

### [09-Advanced-Topics](./09-Advanced-Topics/README.md) üöÄ
**Sujets avanc√©s**

- Spark MLlib (Machine Learning)
- Delta Lake (ACID transactions)
- GraphX (Graph processing)
- Arrow et Pandas UDF
- Performance monitoring

**Exemples inclus :**
- `spark_ml.py` - Machine Learning avec MLlib
- `delta_lake.py` - Utilisation de Delta Lake
- `graph_processing.py` - Traitement de graphes

**Dur√©e estim√©e :** 3 jours

### [10-Production-Deployment](./10-Production-Deployment/README.md) üè≠
**D√©ployer Spark en production**

- Modes de d√©ploiement (Standalone, YARN, Kubernetes, Mesos)
- Databricks
- EMR (AWS) et Dataproc (GCP)
- Monitoring et logging
- Best practices production

**Configurations incluses :**
- `kubernetes/spark-on-k8s.yaml` - D√©ploiement Kubernetes
- `databricks/job-config.json` - Configuration Databricks jobs

**Dur√©e estim√©e :** 2 jours

### [Projets](./Projets/)
**Projets pratiques de bout en bout**

- **01-Analyse-Logs** : Analyser des logs web avec Spark
- **02-ETL-E-commerce** : Pipeline ETL pour donn√©es e-commerce
- **03-Streaming-IoT** : Traitement temps r√©el de donn√©es IoT

**Dur√©e estim√©e :** 1 semaine

---

## Pr√©requis

### Connaissances requises

- **Python** : Niveau interm√©diaire (voir module Python-Basics)
- **SQL** : Requ√™tes de base
- **Linux** : Ligne de commande
- **Big Data** : Concepts de base (distribu√©, partitionnement)

### Connaissances recommand√©es

- **Scala** : Optionnel mais utile
- **Docker** : Pour l'environnement de d√©veloppement
- **Cloud** : AWS/GCP/Azure pour le d√©ploiement

### Logiciels √† installer

```bash
# Python 3.8+
python --version

# Java 8 ou 11
java -version

# Docker (recommand√©)
docker --version

# (Optionnel) Scala
scala -version
```

---

## Installation rapide

### Option 1 : Docker (Recommand√© pour d√©buter)

```bash
cd 02-Installation-Setup
docker-compose up -d
```

Acc√©dez √† Jupyter : `http://localhost:8888`

### Option 2 : Installation locale

```bash
# Installer PySpark via pip
pip install pyspark

# Tester l'installation
python -c "import pyspark; print(pyspark.__version__)"
```

### Option 3 : Databricks Community Edition

1. S'inscrire sur [Databricks Community Edition](https://community.cloud.databricks.com/)
2. Cr√©er un cluster
3. Importer les notebooks du cours

---

## Parcours d'apprentissage

### D√©butant (2 semaines)
1. 01-Introduction
2. 02-Installation-Setup
3. 04-DataFrames-API (focus sur DataFrame, pas RDD)
4. 05-Spark-SQL (bases)
5. 06-ETL-Pipelines (lecture/√©criture simple)

### Interm√©diaire (4 semaines)
1. Parcours D√©butant
2. 03-RDD-Basics
3. 05-Spark-SQL (complet)
4. 06-ETL-Pipelines (complet)
5. 07-Performance-Optimization
6. Projet 01 ou 02

### Avanc√© (6 semaines)
1. Parcours Interm√©diaire
2. 08-Spark-Streaming
3. 09-Advanced-Topics
4. 10-Production-Deployment
5. Les 3 projets pratiques

---

## Concepts cl√©s √† ma√Ætriser

### 1. Abstractions Spark

```
RDD (Low-level)
  ‚Üì
DataFrame (High-level, optimis√©)
  ‚Üì
Dataset (Type-safe, Scala/Java)
```

**Recommandation** : Utilisez DataFrame API en priorit√©.

### 2. Architecture Spark

```
Driver Program
  ‚îú‚îÄ SparkContext / SparkSession
  ‚îî‚îÄ Cluster Manager (YARN, K8s, etc.)
      ‚îú‚îÄ Executor 1
      ‚îÇ   ‚îú‚îÄ Task 1
      ‚îÇ   ‚îî‚îÄ Task 2
      ‚îú‚îÄ Executor 2
      ‚îÇ   ‚îú‚îÄ Task 3
      ‚îÇ   ‚îî‚îÄ Task 4
      ‚îî‚îÄ Executor N
```

### 3. Lazy Evaluation

```python
# Transformations (lazy)
df = spark.read.csv("data.csv")    # Pas d'ex√©cution
df = df.filter(col("age") > 18)    # Pas d'ex√©cution
df = df.select("name", "age")      # Pas d'ex√©cution

# Action (d√©clenche l'ex√©cution)
df.show()  # Ex√©cution de toute la cha√Æne
```

### 4. Transformations vs Actions

**Transformations** (retournent un RDD/DataFrame) :
- `map`, `filter`, `select`, `groupBy`, `join`, etc.

**Actions** (retournent une valeur ou √©crivent des donn√©es) :
- `collect`, `count`, `show`, `write`, `reduce`, etc.

---

## Comparaison des technologies

| Technologie | Cas d'usage | Vitesse | Complexit√© |
|-------------|-------------|---------|------------|
| **Hadoop MapReduce** | Batch lourd | ‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Apache Spark** | Batch + Streaming | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Apache Flink** | Streaming temps r√©el | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Pandas** | Donn√©es en m√©moire | ‚≠ê‚≠ê‚≠ê | ‚≠ê |
| **Dask** | Pandas distribu√© | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

### Quand utiliser Spark ?

**‚úÖ Utilisez Spark pour :**
- Traiter des volumes de donn√©es > 10 GB
- ETL/ELT sur donn√©es distribu√©es
- Analyse de donn√©es √† grande √©chelle
- Machine Learning distribu√©
- Streaming avec √©tat (stateful)

**‚ùå N'utilisez PAS Spark pour :**
- Petits datasets (< 1 GB) ‚Üí Utilisez Pandas
- Streaming ultra low-latency (< 100ms) ‚Üí Utilisez Flink
- OLTP (transactions) ‚Üí Utilisez une base de donn√©es
- Traitement simple de fichiers ‚Üí Utilisez des scripts

---

## √âcosyst√®me Spark

```
Apache Spark
‚îú‚îÄ‚îÄ Spark Core (RDD)
‚îú‚îÄ‚îÄ Spark SQL (DataFrames, SQL)
‚îú‚îÄ‚îÄ Spark Streaming (Structured Streaming)
‚îú‚îÄ‚îÄ MLlib (Machine Learning)
‚îî‚îÄ‚îÄ GraphX (Graph processing)

Int√©grations
‚îú‚îÄ‚îÄ Sources de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ HDFS, S3, Azure Blob
‚îÇ   ‚îú‚îÄ‚îÄ Kafka, Kinesis
‚îÇ   ‚îú‚îÄ‚îÄ JDBC (MySQL, PostgreSQL)
‚îÇ   ‚îî‚îÄ‚îÄ NoSQL (Cassandra, MongoDB)
‚îÇ
‚îú‚îÄ‚îÄ Formats
‚îÇ   ‚îú‚îÄ‚îÄ Parquet, ORC
‚îÇ   ‚îú‚îÄ‚îÄ Avro, JSON
‚îÇ   ‚îî‚îÄ‚îÄ Delta Lake, Iceberg
‚îÇ
‚îî‚îÄ‚îÄ Clusters
    ‚îú‚îÄ‚îÄ YARN (Hadoop)
    ‚îú‚îÄ‚îÄ Kubernetes
    ‚îú‚îÄ‚îÄ Mesos
    ‚îî‚îÄ‚îÄ Databricks, EMR, Dataproc
```

---

## Ressources

### Documentation officielle
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Databricks Documentation](https://docs.databricks.com/)

### Livres recommand√©s
- **"Learning Spark"** (2nd Edition) - Jules S. Damji et al.
- **"Spark: The Definitive Guide"** - Bill Chambers & Matei Zaharia
- **"High Performance Spark"** - Holden Karau & Rachel Warren

### Cours en ligne
- [Databricks Academy](https://academy.databricks.com/)
- [Spark on Coursera](https://www.coursera.org/learn/scala-spark-big-data)
- [Udemy Spark Courses](https://www.udemy.com/topic/apache-spark/)

### Communaut√©s
- [Spark Users Mailing List](https://spark.apache.org/community.html)
- [Stack Overflow - apache-spark](https://stackoverflow.com/questions/tagged/apache-spark)
- [Reddit - r/apachespark](https://reddit.com/r/apachespark)

---

## Certifications

- **Databricks Certified Associate Developer for Apache Spark**
- **Databricks Certified Data Engineer Associate**
- **Cloudera Spark and Hadoop Developer**

---

## FAQ

**Q: Spark ou Pandas ?**
A: Pandas pour < 10 GB en m√©moire, Spark pour > 10 GB distribu√©.

**Q: PySpark ou Scala Spark ?**
A: PySpark pour la simplicit√© et l'√©cosyst√®me Python. Scala pour les meilleures performances.

**Q: Quelle version de Spark ?**
A: Utilisez la derni√®re version stable (3.5+ en 2024).

**Q: Combien de m√©moire pour Spark ?**
A: Minimum 4 GB pour le d√©veloppement, 8+ GB recommand√©.

**Q: Spark est-il gratuit ?**
A: Oui, Spark est open-source (Apache 2.0). Databricks/EMR sont payants.

---

## Contribution

Pour am√©liorer ce cours :
1. Ouvrir une issue pour signaler des erreurs
2. Proposer des pull requests
3. Partager vos retours d'exp√©rience

---

## Roadmap du cours

```
Semaine 1 : Fondamentaux
  ‚îú‚îÄ Introduction √† Spark
  ‚îú‚îÄ Installation
  ‚îî‚îÄ DataFrames API

Semaine 2 : SQL et ETL
  ‚îú‚îÄ Spark SQL
  ‚îú‚îÄ ETL Pipelines
  ‚îî‚îÄ Mini-projet ETL

Semaine 3 : Performance
  ‚îú‚îÄ Optimisation
  ‚îú‚îÄ Partitionnement
  ‚îî‚îÄ Caching

Semaine 4 : Streaming
  ‚îú‚îÄ Structured Streaming
  ‚îú‚îÄ Kafka integration
  ‚îî‚îÄ Projet streaming

Semaine 5-6 : Production
  ‚îú‚îÄ Advanced topics
  ‚îú‚îÄ D√©ploiement
  ‚îî‚îÄ Projet final complet
```

---

**Bon apprentissage avec Apache Spark ! üöÄ**

"Spark is the Swiss Army knife of Big Data" - Anonymous Data Engineer
