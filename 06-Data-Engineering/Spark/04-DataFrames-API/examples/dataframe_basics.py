"""
DataFrame Basics - Op√©rations de base sur les DataFrames

Ce script couvre :
- Cr√©ation de DataFrames
- Affichage et inspection
- S√©lection et filtrage
- Op√©rations sur colonnes
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, upper, lower, concat, when
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Cr√©er SparkSession
spark = SparkSession.builder \
    .appName("DataFrame Basics") \
    .master("local[*]") \
    .getOrCreate()

print("=" * 80)
print("DATAFRAME BASICS")
print("=" * 80)

# ========================================
# 1. CR√âATION DE DATAFRAMES
# ========================================
print("\n1. Cr√©ation de DataFrames")
print("-" * 50)

# Depuis une liste de tuples
data = [
    (1, "Alice", 25, "Paris", 50000),
    (2, "Bob", 30, "Lyon", 60000),
    (3, "Charlie", 35, "Marseille", 55000),
    (4, "David", 28, "Paris", 52000),
    (5, "Eve", 32, "Lyon", 58000)
]

df = spark.createDataFrame(data, ["id", "name", "age", "city", "salary"])
print("\n‚úÖ DataFrame cr√©√© depuis une liste de tuples:")
df.show()

# ========================================
# 2. AFFICHAGE ET INSPECTION
# ========================================
print("\n2. Affichage et inspection")
print("-" * 50)

print("\nüìã printSchema() - Structure du DataFrame:")
df.printSchema()

print(f"\nüìä count() - Nombre de lignes: {df.count()}")
print(f"\nüìä columns - Noms des colonnes: {df.columns}")
print(f"\nüìä dtypes - Types des colonnes: {df.dtypes}")

print("\nüîç head(2) - Les 2 premi√®res lignes:")
for row in df.head(2):
    print(row)

print("\nüìà describe() - Statistiques descriptives:")
df.describe().show()

# ========================================
# 3. S√âLECTION DE COLONNES
# ========================================
print("\n3. S√©lection de colonnes (select)")
print("-" * 50)

# Une colonne
print("\n‚úÖ S√©lectionner une colonne:")
df.select("name").show(3)

# Plusieurs colonnes
print("\n‚úÖ S√©lectionner plusieurs colonnes:")
df.select("name", "age", "city").show(3)

# Avec col()
print("\n‚úÖ Avec col():")
df.select(col("name"), col("age") + 5).show(3)

# Toutes les colonnes
print("\n‚úÖ Toutes les colonnes avec *:")
df.select("*").show(2)

# ========================================
# 4. FILTRAGE DE LIGNES
# ========================================
print("\n4. Filtrage de lignes (filter/where)")
print("-" * 50)

# Filtrer par √¢ge
print("\n‚úÖ Filtrer age > 28:")
df.filter(col("age") > 28).show()

# Multiples conditions (AND)
print("\n‚úÖ Filtrer age > 28 AND city = Paris:")
df.filter((col("age") > 28) & (col("city") == "Paris")).show()

# Condition OR
print("\n‚úÖ Filtrer age < 27 OR salary > 57000:")
df.filter((col("age") < 27) | (col("salary") > 57000)).show()

# Avec SQL string
print("\n‚úÖ Filtrer avec expression SQL:")
df.filter("age > 28 AND city = 'Lyon'").show()

# ========================================
# 5. AJOUTER/MODIFIER DES COLONNES
# ========================================
print("\n5. Ajouter/Modifier des colonnes (withColumn)")
print("-" * 50)

# Ajouter une colonne calcul√©e
df_with_bonus = df.withColumn("salary_with_bonus", col("salary") * 1.1)
print("\n‚úÖ Ajouter colonne salary_with_bonus:")
df_with_bonus.select("name", "salary", "salary_with_bonus").show()

# Ajouter une colonne constante
df_with_country = df.withColumn("country", lit("France"))
print("\n‚úÖ Ajouter colonne constante 'country':")
df_with_country.select("name", "city", "country").show(3)

# Modifier une colonne existante
df_upper = df.withColumn("name", upper(col("name")))
print("\n‚úÖ Modifier name en majuscules:")
df_upper.select("name", "city").show(3)

# ========================================
# 6. RENOMMER DES COLONNES
# ========================================
print("\n6. Renommer des colonnes")
print("-" * 50)

# M√©thode 1: alias()
print("\n‚úÖ Avec alias():")
df.select(
    col("name").alias("full_name"),
    col("age").alias("years_old")
).show(3)

# M√©thode 2: withColumnRenamed()
df_renamed = df.withColumnRenamed("name", "employee_name")
print("\n‚úÖ Avec withColumnRenamed():")
df_renamed.select("employee_name", "age").show(3)

# ========================================
# 7. SUPPRIMER DES COLONNES
# ========================================
print("\n7. Supprimer des colonnes (drop)")
print("-" * 50)

df_dropped = df.drop("salary")
print("\n‚úÖ Supprimer la colonne 'salary':")
print(f"Colonnes avant: {df.columns}")
print(f"Colonnes apr√®s: {df_dropped.columns}")
df_dropped.show(3)

# ========================================
# 8. TRIER LES DONN√âES
# ========================================
print("\n8. Trier les donn√©es (orderBy/sort)")
print("-" * 50)

# Tri croissant
print("\n‚úÖ Trier par age (croissant):")
df.orderBy("age").show()

# Tri d√©croissant
print("\n‚úÖ Trier par salary (d√©croissant):")
df.orderBy(col("salary").desc()).show()

# Tri multiple
print("\n‚úÖ Trier par city puis age:")
df.orderBy("city", "age").show()

# ========================================
# 9. √âLIMINER LES DOUBLONS
# ========================================
print("\n9. √âliminer les doublons (distinct/dropDuplicates)")
print("-" * 50)

# Cr√©er un DataFrame avec doublons
data_dup = [
    ("Paris", "France"),
    ("Lyon", "France"),
    ("Paris", "France"),  # Doublon
    ("Marseille", "France"),
    ("Lyon", "France")  # Doublon
]

df_dup = spark.createDataFrame(data_dup, ["city", "country"])
print("\nüìã DataFrame avec doublons:")
df_dup.show()

print("\n‚úÖ Apr√®s distinct():")
df_dup.distinct().show()

print("\n‚úÖ dropDuplicates sur colonne 'city':")
df_dup.dropDuplicates(["city"]).show()

# ========================================
# 10. CONDITIONS (WHEN/OTHERWISE)
# ========================================
print("\n10. Conditions (when/otherwise)")
print("-" * 50)

# Cat√©goriser les √¢ges
df_categorized = df.withColumn("age_category",
    when(col("age") < 30, "Young")
    .when((col("age") >= 30) & (col("age") < 35), "Mid")
    .otherwise("Senior")
)

print("\n‚úÖ Cat√©goriser les √¢ges:")
df_categorized.select("name", "age", "age_category").show()

# Cat√©goriser les salaires
df_salary_grade = df.withColumn("salary_grade",
    when(col("salary") >= 58000, "High")
    .when(col("salary") >= 52000, "Medium")
    .otherwise("Low")
)

print("\n‚úÖ Cat√©goriser les salaires:")
df_salary_grade.select("name", "salary", "salary_grade").show()

# ========================================
# 11. LIMITER LE NOMBRE DE LIGNES
# ========================================
print("\n11. Limiter le nombre de lignes (limit)")
print("-" * 50)

print("\n‚úÖ Les 3 premi√®res lignes:")
df.limit(3).show()

# ========================================
# 12. VALEURS UNIQUES
# ========================================
print("\n12. Valeurs uniques")
print("-" * 50)

print("\n‚úÖ Villes uniques:")
df.select("city").distinct().show()

print("\n‚úÖ Compter les valeurs uniques par ville:")
df.groupBy("city").count().show()

# ========================================
# 13. EXEMPLE COMPLET: NETTOYAGE
# ========================================
print("\n13. Exemple complet: Nettoyage de donn√©es")
print("-" * 50)

# Donn√©es sales
messy_data = [
    (1, " alice ", 25, "PARIS", 50000),
    (2, "BOB", 30, "lyon", 60000),
    (3, " Charlie ", None, "Marseille", 55000),
    (4, "david", 28, "PARIS", None)
]

messy_df = spark.createDataFrame(messy_data, ["id", "name", "age", "city", "salary"])

print("\n‚ùå Donn√©es avant nettoyage:")
messy_df.show()

# Nettoyage
from pyspark.sql.functions import trim, initcap

clean_df = messy_df \
    .withColumn("name", trim(col("name"))) \
    .withColumn("name", initcap(col("name"))) \
    .withColumn("city", initcap(lower(col("city")))) \
    .fillna({"age": 0, "salary": 0})

print("\n‚úÖ Donn√©es apr√®s nettoyage:")
clean_df.show()

# ========================================
# R√âCAPITULATIF
# ========================================
print("\n" + "=" * 80)
print("R√âCAPITULATIF DES OP√âRATIONS DE BASE")
print("=" * 80)

print("""
Affichage:
  df.show()              - Afficher les donn√©es
  df.printSchema()       - Afficher le schema
  df.describe()          - Statistiques descriptives
  df.count()             - Nombre de lignes

S√©lection:
  df.select("col1", "col2")  - S√©lectionner colonnes
  df.filter(condition)       - Filtrer lignes
  df.orderBy("col")          - Trier
  df.limit(n)                - Limiter le nombre de lignes
  df.distinct()              - Valeurs uniques

Transformation:
  df.withColumn(name, col)   - Ajouter/modifier colonne
  df.withColumnRenamed()     - Renommer colonne
  df.drop("col")             - Supprimer colonne
  df.fillna(value)           - Remplir les valeurs nulles
  df.dropDuplicates()        - Supprimer doublons
""")

# Arr√™ter SparkSession
spark.stop()
print("\n‚úÖ SparkSession arr√™t√©e")
